{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"code/","title":"Modules","text":""},{"location":"code/#overview","title":"Overview","text":"<p>There are 12 modules used in this package, which covers 41 functions</p>"},{"location":"code/#module-descriptions","title":"Module Descriptions","text":"Module Description <code>io</code> The <code>io</code> module is used for reading and writing tables to/from directories. <code>checks</code> The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe. <code>types</code> The <code>types</code> module is used to get, check, and change a datafames column data types. <code>keys</code> The <code>keys</code> module is used for creating new columns to act as keys (primary and foreign), to be used for joins with other tables, or to create relationships within downstream applications, like PowerBI. <code>scale</code> The <code>scale</code> module is used for rounding a column (or columns) to a given rounding accuracy. <code>dimensions</code> The <code>dimensions</code> module is used for checking the dimensions of <code>pyspark</code> <code>dataframe</code>'s."},{"location":"code/#functions-by-module","title":"Functions by Module","text":"Module Function <code>io</code> <code>read_from_path()</code> <code>write_to_path()</code> <code>transfer_table()</code> <code>checks</code> <code>column_exists()</code> <code>columns_exists()</code> <code>is_vaid_spark_type()</code> <code>table_exists()</code> <code>types</code> <code>get_column_types()</code> <code>cast_column_to_type()</code> <code>cast_columns_to_type()</code> <code>map_cast_columns_to_type()</code> <code>keys</code> <code>add_keys_from_columns()</code> <code>add_key_from_columns()</code> <code>scale</code> <code>round_column()</code> <code>round_columns()</code> <code>dimensions</code> <code>get_dims()</code> <code>get_dims_of_tables()</code> <code>columns</code> <code>get_columns()</code> <code>get_columns_by_likeness()</code> <code>rename_columns()</code> <code>reorder_columns()</code> <code>delete_columns()</code>"},{"location":"code/#testing","title":"Testing","text":"<p>This package is fully tested against:</p> <ol> <li>Unit tests</li> <li>Lint tests</li> <li>MyPy tests</li> <li>Build tests</li> </ol>"},{"location":"code/#latest-code-coverage","title":"Latest Code Coverage","text":""},{"location":"code/checks/","title":"Checks","text":""},{"location":"code/checks/#toolbox_pyspark.checks","title":"toolbox_pyspark.checks","text":"<p>Summary</p> <p>The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe.</p>"},{"location":"code/checks/#toolbox_pyspark.checks.column_exists","title":"column_exists","text":"<pre><code>column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>column.upper()</code>. Default: <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if exists or <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import column_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example1: Column Exists<pre><code>&gt;&gt;&gt; result = column_exists(df, \"a\")\n&gt;&gt;&gt; print(result)\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Column exists.</p> <p>Example 2: Column Missing<pre><code>&gt;&gt;&gt; result = column_exists(df, \"c\")\n&gt;&gt;&gt; print(result)\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py column.upper()`.&lt;br&gt;\n            Default: `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            `#!py True` if exists or `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import column_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example1: Column Exists\"}\n        &gt;&gt;&gt; result = column_exists(df, \"a\")\n        &gt;&gt;&gt; print(result)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Column exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Column Missing\"}\n        &gt;&gt;&gt; result = column_exists(df, \"c\")\n        &gt;&gt;&gt; print(result)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_exists(dataframe, [column], match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.columns_exists","title":"columns_exists","text":"<pre><code>columns_exists(\n    dataframe: psDataFrame,\n    columns: str_collection,\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check</p> required <code>columns</code> <code>Union[str_list, str_tuple, str_set]</code> <p>The columns to check</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>[col.upper() for col in columns]</code>. Default: <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all columns exist or <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import columns_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: Columns exist<pre><code>&gt;&gt;&gt; columns_exists(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: All columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; columns_exists(df, [\"b\", \"d\"])\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: One column is missing.</p> <p>Example 3: All columns missing<pre><code>&gt;&gt;&gt; columns_exists(df, [\"c\", \"d\"])\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: All columns are missing.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef columns_exists(\n    dataframe: psDataFrame,\n    columns: str_collection,\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check\n        columns (Union[str_list, str_tuple, str_set]):\n            The columns to check\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py [col.upper() for col in columns]`.&lt;br&gt;\n            Default: `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            `#!py True` if all columns exist or `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import columns_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Columns exist\"}\n        &gt;&gt;&gt; columns_exists(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: All columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; columns_exists(df, [\"b\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: One column is missing.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: All columns missing\"}\n        &gt;&gt;&gt; columns_exists(df, [\"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: All columns are missing.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_exists(dataframe, columns, match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_column_exists","title":"assert_column_exists","text":"<pre><code>assert_column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>column.upper()</code>. Default: <code>True</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1,2,3,4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_column_exists(df, \"a\")\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Column exists.</p> <p>Example 2: Error raised<pre><code>&gt;&gt;&gt; assert_column_exists(df, \"c\")\n</code></pre> Terminal<pre><code>Attribute Error: Column \"c\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py column.upper()`.&lt;br&gt;\n            Default: `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If `#!py column` does not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1,2,3,4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_column_exists(df, \"a\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Column exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Error raised\"}\n        &gt;&gt;&gt; assert_column_exists(df, \"c\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Column \"c\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    if not column_exists(dataframe, column, match_case):\n        raise AttributeError(\n            f\"Column '{column}' does not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}.\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_columns_exists","title":"assert_columns_exists","text":"<pre><code>assert_columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check</p> required <code>columns</code> <code>Union[str_list, str_tuple, str_set]</code> <p>The columns to check</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>[col.upper() for col in columns]</code>. Default: <code>True</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"c\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column \"c\" does not exist.</p> <p>Example 3: Multiple columns missing<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\", \"d\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"c\", \"d\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Columns \"c\" and \"d\" does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check\n        columns (Union[str_list, str_tuple, str_set]):\n            The columns to check\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py [col.upper() for col in columns]`.&lt;br&gt;\n            Default: `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"c\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column \"c\" does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Multiple columns missing\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"c\", \"d\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Columns \"c\" and \"d\" does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    columns = [columns] if isinstance(columns, str) else columns\n    (exist, missing_cols) = _columns_exists(dataframe, columns, match_case)\n    if not exist:\n        raise AttributeError(\n            f\"Columns {missing_cols} do not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.warn_column_missing","title":"warn_column_missing","text":"<pre><code>warn_column_missing(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeWarning</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import warn_column_missing\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; warn_column_missing(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns exist.</p> <p>Example 2: Warning raised<pre><code>&gt;&gt;&gt; warn_column_missing(df, \"c\")\n</code></pre> Terminal<pre><code>Attribute Warning: Column \"c\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef warn_column_missing(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! summary \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            Defaults to `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeWarning` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import warn_column_missing\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; warn_column_missing(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Warning raised\"}\n        &gt;&gt;&gt; warn_column_missing(df, \"c\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Warning: Column \"c\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    if not column_exists(dataframe, column, match_case):\n        warn(\n            f\"Column '{column}' does not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}.\",\n            AttributeWarning,\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.warn_columns_missing","title":"warn_columns_missing","text":"<pre><code>warn_columns_missing(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The columns to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeWarning</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import warn_columns_missing\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; warn_columns_missing(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; warn_columns_missing(df, [\"b\", \"c\"])\n</code></pre> Terminal<pre><code>Attribute Warning: Columns [\"c\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column \"c\" does not exist.</p> <p>Example 3: Multiple columns missing<pre><code>&gt;&gt;&gt; warn_columns_missing(df, [\"b\", \"c\", \"d\"])\n</code></pre> Terminal<pre><code>Attribute Warning: Columns [\"c\", \"d\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Columns \"c\" and \"d\" does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef warn_columns_missing(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! summary \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        columns (Union[str, str_collection]):\n            The columns to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            Defaults to `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeWarning` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import warn_columns_missing\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; warn_columns_missing(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; warn_columns_missing(df, [\"b\", \"c\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Warning: Columns [\"c\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column \"c\" does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Multiple columns missing\"}\n        &gt;&gt;&gt; warn_columns_missing(df, [\"b\", \"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Warning: Columns [\"c\", \"d\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Columns \"c\" and \"d\" does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    columns = [columns] if isinstance(columns, str) else columns\n    (exist, missing_cols) = _columns_exists(dataframe, columns, match_case)\n    if not exist:\n        warn(\n            f\"Columns {missing_cols} do not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}\",\n            AttributeWarning,\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.is_vaid_spark_type","title":"is_vaid_spark_type","text":"<pre><code>is_vaid_spark_type(datatype: str) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>datatype</code> is a correct and valid <code>pyspark</code> data type.</p> <p>Parameters:</p> Name Type Description Default <code>datatype</code> <code>str</code> <p>The name of the data type to check.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If the given <code>datatype</code> is not a valid <code>pyspark</code> data type.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; from toolbox_pyspark.checks import is_vaid_spark_type\n</code></pre> <p>Loop through all valid types<pre><code>&gt;&gt;&gt; type_names = [\"string\", \"char\", \"varchar\", \"binary\", \"boolean\", \"decimal\", \"float\", \"double\", \"byte\", \"short\", \"integer\", \"long\", \"date\", \"timestamp\", \"timestamp_ntz\", \"void\"]\n&gt;&gt;&gt; for type_name in type_names:\n...     is_vaid_spark_type(type_name)\n</code></pre>  Nothing is returned each time. Because they're all valid. <p>Conclusion: They're all valid.</p> <p>Check some invalid types<pre><code>&gt;&gt;&gt; type_names = [\"np.ndarray\", \"pd.DataFrame\", \"dict\"]\n&gt;&gt;&gt; for type_name in type_names:\n...     is_vaid_spark_type(type_name)\n</code></pre> Terminal<pre><code>AttributeError: DataType 'np.ndarray' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> Terminal<pre><code>AttributeError: DataType 'pd.DataFrame' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> Terminal<pre><code>AttributeError: DataType 'dict' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> <p>Conclusion: All of these types are invalid.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef is_vaid_spark_type(datatype: str) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py datatype` is a correct and valid `#!py pyspark` data type.\n\n    Params:\n        datatype (str):\n            The name of the data type to check.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If the given `#!py datatype` is not a valid `#!py pyspark` data type.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; from toolbox_pyspark.checks import is_vaid_spark_type\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Loop through all valid types\"}\n        &gt;&gt;&gt; type_names = [\"string\", \"char\", \"varchar\", \"binary\", \"boolean\", \"decimal\", \"float\", \"double\", \"byte\", \"short\", \"integer\", \"long\", \"date\", \"timestamp\", \"timestamp_ntz\", \"void\"]\n        &gt;&gt;&gt; for type_name in type_names:\n        ...     is_vaid_spark_type(type_name)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        Nothing is returned each time. Because they're all valid.\n        !!! success \"Conclusion: They're all valid.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Check some invalid types\"}\n        &gt;&gt;&gt; type_names = [\"np.ndarray\", \"pd.DataFrame\", \"dict\"]\n        &gt;&gt;&gt; for type_name in type_names:\n        ...     is_vaid_spark_type(type_name)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'np.ndarray' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'pd.DataFrame' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'dict' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        !!! failure \"Conclusion: All of these types are invalid.\"\n        &lt;/div&gt;\n    \"\"\"\n    if datatype not in VALID_PYSPARK_TYPE_NAMES:\n        raise AttributeError(\n            f\"DataType '{datatype}' is not valid.\\n\"\n            f\"Must be one of: {VALID_PYSPARK_TYPE_NAMES}\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.table_exists","title":"table_exists","text":"<pre><code>table_exists(\n    name: str,\n    path: str,\n    data_format: str,\n    spark_session: SparkSession,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Will try to read <code>table</code> from <code>path</code> using <code>format</code>, and if successful will return <code>True</code> otherwise <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to check exists.</p> required <code>path</code> <code>str</code> <p>The directory where the table should be existing.</p> required <code>data_format</code> <code>str</code> <p>The format of the table to try checking.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The <code>spark</code> session to use for the importing.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Returns <code>True</code> if the table exists, <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Constants\n&gt;&gt;&gt; write_name = \"test_df\"\n&gt;&gt;&gt; write_path = f\"./test\"\n&gt;&gt;&gt; write_format = \"parquet\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write data\n&gt;&gt;&gt; write_to_path(df, f\"{write_name}.{write_format}\", write_path)\n</code></pre> <p>Example 1: Table exists<pre><code>&gt;&gt;&gt; table_exists(\"test_df.parquet\", \"./test\", \"parquet\", spark)\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Table exists.</p> <p>Example 2: Table does not exist<pre><code>&gt;&gt;&gt; table_exists(\"bad_table_name.parquet\", \"./test\", \"parquet\", spark)\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Table does not exist.</p> See Also <ul> <li><code>toolbox_pyspark.io.read_from_path()</code></li> </ul> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef table_exists(\n    name: str,\n    path: str,\n    data_format: str,\n    spark_session: SparkSession,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Will try to read `#!py table` from `#!py path` using `#!py format`, and if successful will return `#!py True` otherwise `#!py False`.\n\n    Params:\n        name (str):\n            The name of the table to check exists.\n        path (str):\n            The directory where the table should be existing.\n        data_format (str):\n            The format of the table to try checking.\n        spark_session (SparkSession):\n            The `#!py spark` session to use for the importing.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            Returns `#!py True` if the table exists, `False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Constants\n        &gt;&gt;&gt; write_name = \"test_df\"\n        &gt;&gt;&gt; write_path = f\"./test\"\n        &gt;&gt;&gt; write_format = \"parquet\"\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Write data\n        &gt;&gt;&gt; write_to_path(df, f\"{write_name}.{write_format}\", write_path)\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Table exists\"}\n        &gt;&gt;&gt; table_exists(\"test_df.parquet\", \"./test\", \"parquet\", spark)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Table exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Table does not exist\"}\n        &gt;&gt;&gt; table_exists(\"bad_table_name.parquet\", \"./test\", \"parquet\", spark)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Table does not exist.\"\n        &lt;/div&gt;\n\n    ???+ tip \"See Also\"\n        - [`toolbox_pyspark.io.read_from_path()`][toolbox_pyspark.io.read_from_path]\n    \"\"\"\n    try:\n        _ = read_from_path(\n            name=name,\n            path=path,\n            data_format=data_format,\n            spark_session=spark_session,\n        )\n    except Exception:\n        return False\n    return True\n</code></pre>"},{"location":"code/columns/","title":"Columns","text":""},{"location":"code/columns/#toolbox_pyspark.columns","title":"toolbox_pyspark.columns","text":"<p>Summary</p> <p>The <code>columns</code> module is used to fetch columns from a given DataFrame using convenient syntax.</p>"},{"location":"code/columns/#toolbox_pyspark.columns.get_columns","title":"get_columns","text":"<pre><code>get_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n) -&gt; str_list\n</code></pre> <p>Summary</p> <p>Get a list of column names from a DataFrame based on optional filter criteria.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame from which to retrieve column names.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>Optional filter criteria for selecting columns. If a string is provided, it can be one of the following options:</p> Value Description <code>\"all\"</code> Return all columns in the DataFrame. <code>\"all_str\"</code> Return columns of string type. <code>\"all_int\"</code> Return columns of integer type. <code>\"all_numeric\"</code> Return columns of numeric types (integers and floats). <code>\"all_datetime\"</code> or <code>\"all_timestamp\"</code> Return columns of datetime or timestamp type. <code>\"all_date\"</code> Return columns of date type. Any other string Return columns matching the provided exact column name. <p>If a list or tuple of column names is provided, return only those columns. Defaults to <code>None</code> (which returns all columns).</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>str_list</code> <p>The selected column names from the DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F\n&gt;&gt;&gt; from toolbox_pyspark.columns import get_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = (\n...     spark\n...     .createDataFrame(\n...         pd.DataFrame(\n...             {\n...                 \"a\": (0, 1, 2, 3),\n...                 \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             }\n...         )\n...     )\n...     .withColumns(\n...         {\n...             \"c\": F.lit(\"1\").cast(\"int\"),\n...             \"d\": F.lit(\"2\").cast(\"string\"),\n...             \"e\": F.lit(\"1.1\").cast(\"float\"),\n...             \"f\": F.lit(\"1.2\").cast(\"double\"),\n...             \"g\": F.lit(\"2022-01-01\").cast(\"date\"),\n...             \"h\": F.lit(\"2022-02-01 01:00:00\").cast(\"timestamp\"),\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n&gt;&gt;&gt; print(df.dtypes)\n</code></pre> Terminal<pre><code>+---+---+---+---+-----+-----+------------+---------------------+\n| a | b | c | d |   e |   f |          g |                   h |\n+---+---+---+---+-----+-----+------------+---------------------+\n| 0 | a | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n| 1 | b | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n| 2 | c | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n| 3 | d | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n+---+---+---+---+-----+-----+------------+---------------------+\n</code></pre> Terminal<pre><code>[\n    (\"a\", \"bigint\"),\n    (\"b\", \"string\"),\n    (\"c\", \"int\"),\n    (\"d\", \"string\"),\n    (\"e\", \"float\"),\n    (\"f\", \"double\"),\n    (\"g\", \"date\"),\n    (\"h\", \"timestamp\"),\n]\n</code></pre> </p> <p>Example 1: Default params<pre><code>&gt;&gt;&gt; print(get_columns(df).columns)\n</code></pre> Terminal<pre><code>[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Specific columns<pre><code>&gt;&gt;&gt; print(get_columns(df, [\"a\", \"b\", \"c\"]).columns)\n</code></pre> Terminal<pre><code>[\"a\", \"b\", \"c\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Single column as list<pre><code>&gt;&gt;&gt; print(get_columns(df, [\"a\"]).columns)\n</code></pre> Terminal<pre><code>[\"a\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Single column as string<pre><code>&gt;&gt;&gt; print(get_columns(df, \"a\").columns)\n</code></pre> Terminal<pre><code>[\"a\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: All columns<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all\").columns)\n</code></pre> Terminal<pre><code>[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: All str<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_str\").columns)\n</code></pre> Terminal<pre><code>[\"b\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 7: All int<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all int\").columns)\n</code></pre> Terminal<pre><code>[\"c\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 8: All float<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_decimal\").columns)\n</code></pre> Terminal<pre><code>[\"e\", \"f\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 9: All numeric<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_numeric\").columns)\n</code></pre> Terminal<pre><code>[\"c\", \"e\", \"f\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 10: All date<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_date\").columns)\n</code></pre> Terminal<pre><code>[\"g\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 11: All datetime<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_datetime\").columns)\n</code></pre> Terminal<pre><code>[\"h\"]\n</code></pre> <p>Conclusion: Success.</p> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef get_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n) -&gt; str_list:\n    \"\"\"\n    !!! note \"Summary\"\n        Get a list of column names from a DataFrame based on optional filter criteria.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame from which to retrieve column names.\n        columns (Optional[Union[str, str_collection]], optional):\n            Optional filter criteria for selecting columns.&lt;br&gt;\n            If a string is provided, it can be one of the following options:\n\n            | Value | Description |\n            |-------|-------------|\n            | `#!py \"all\"` | Return all columns in the DataFrame.\n            | `#!py \"all_str\"` | Return columns of string type.\n            | `#!py \"all_int\"` | Return columns of integer type.\n            | `#!py \"all_numeric\"` | Return columns of numeric types (integers and floats).\n            | `#!py \"all_datetime\"` or `#!py \"all_timestamp\"` | Return columns of datetime or timestamp type.\n            | `#!py \"all_date\"` | Return columns of date type.\n            | Any other string | Return columns matching the provided exact column name.\n\n            If a list or tuple of column names is provided, return only those columns.&lt;br&gt;\n            Defaults to `#!py None` (which returns all columns).\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (str_list):\n            The selected column names from the DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F\n        &gt;&gt;&gt; from toolbox_pyspark.columns import get_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = (\n        ...     spark\n        ...     .createDataFrame(\n        ...         pd.DataFrame(\n        ...             {\n        ...                 \"a\": (0, 1, 2, 3),\n        ...                 \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             }\n        ...         )\n        ...     )\n        ...     .withColumns(\n        ...         {\n        ...             \"c\": F.lit(\"1\").cast(\"int\"),\n        ...             \"d\": F.lit(\"2\").cast(\"string\"),\n        ...             \"e\": F.lit(\"1.1\").cast(\"float\"),\n        ...             \"f\": F.lit(\"1.2\").cast(\"double\"),\n        ...             \"g\": F.lit(\"2022-01-01\").cast(\"date\"),\n        ...             \"h\": F.lit(\"2022-02-01 01:00:00\").cast(\"timestamp\"),\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        &gt;&gt;&gt; print(df.dtypes)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+-----+-----+------------+---------------------+\n        | a | b | c | d |   e |   f |          g |                   h |\n        +---+---+---+---+-----+-----+------------+---------------------+\n        | 0 | a | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n        | 1 | b | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n        | 2 | c | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n        | 3 | d | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n        +---+---+---+---+-----+-----+------------+---------------------+\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        [\n            (\"a\", \"bigint\"),\n            (\"b\", \"string\"),\n            (\"c\", \"int\"),\n            (\"d\", \"string\"),\n            (\"e\", \"float\"),\n            (\"f\", \"double\"),\n            (\"g\", \"date\"),\n            (\"h\", \"timestamp\"),\n        ]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default params\"}\n        &gt;&gt;&gt; print(get_columns(df).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Specific columns\"}\n        &gt;&gt;&gt; print(get_columns(df, [\"a\", \"b\", \"c\"]).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"b\", \"c\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Single column as list\"}\n        &gt;&gt;&gt; print(get_columns(df, [\"a\"]).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Single column as string\"}\n        &gt;&gt;&gt; print(get_columns(df, \"a\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: All columns\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: All str\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_str\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"b\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: All int\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all int\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"c\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: All float\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_decimal\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"e\", \"f\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 9: All numeric\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_numeric\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"c\", \"e\", \"f\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 10: All date\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_date\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"g\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 11: All datetime\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_datetime\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"h\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n    \"\"\"\n    if columns is None:\n        return dataframe.columns\n    elif isinstance(columns, str):\n        if \"all\" in columns:\n            if \"str\" in columns:\n                return [\n                    col for col, typ in dataframe.dtypes if typ in [\"str\", \"string\"]\n                ]\n            elif \"int\" in columns:\n                return [\n                    col for col, typ in dataframe.dtypes if typ in [\"int\", \"integer\"]\n                ]\n            elif \"numeric\" in columns:\n                return [\n                    col\n                    for col, typ in dataframe.dtypes\n                    if typ in [\"int\", \"integer\", \"float\", \"double\", \"long\"]\n                    or \"decimal\" in typ\n                ]\n            elif \"float\" in columns or \"double\" in columns or \"decimal\" in columns:\n                return [\n                    col\n                    for col, typ in dataframe.dtypes\n                    if typ in [\"float\", \"double\", \"long\"] or \"decimal\" in typ\n                ]\n            elif \"datetime\" in columns or \"timestamp\" in columns:\n                return [\n                    col\n                    for col, typ in dataframe.dtypes\n                    if typ in [\"datetime\", \"timestamp\"]\n                ]\n            elif \"date\" in columns:\n                return [col for col, typ in dataframe.dtypes if typ in [\"date\"]]\n            else:\n                return dataframe.columns\n        else:\n            return [columns]\n    else:\n        return list(columns)\n</code></pre>"},{"location":"code/columns/#toolbox_pyspark.columns.get_columns_by_likeness","title":"get_columns_by_likeness","text":"<pre><code>get_columns_by_likeness(\n    dataframe: psDataFrame,\n    starts_with: Optional[str] = None,\n    contains: Optional[str] = None,\n    ends_with: Optional[str] = None,\n    match_case: bool = False,\n    operator: Literal[\n        \"and\", \"or\", \"and not\", \"or not\"\n    ] = \"and\",\n) -&gt; str_list\n</code></pre> <p>Summary</p> <p>Extract the column names from a given <code>dataframe</code> based on text that the column name contains.</p> Details <p>You can use any combination of <code>startswith</code>, <code>contains</code>, and <code>endswith</code>. Under the hood, these will be implemented with a number of internal <code>lambda</code> functions to determine matches.</p> <p>The <code>operator</code> parameter determines how the conditions (<code>starts_with</code>, <code>contains</code>, <code>ends_with</code>) are combined:</p> Value Description <code>\"and\"</code> All conditions must be true. <code>\"or\"</code> At least one condition must be true. <code>\"and not\"</code> The first condition must be true and the second condition must be false. <code>\"or not\"</code> At least one condition must be true, but not all. <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The <code>dataframe</code> from which to extract the column names.</p> required <code>starts_with</code> <code>Optional[str]</code> <p>Extract any columns that starts with this <code>str</code>. Determined by using the <code>str.startswith()</code> method. Defaults to <code>None</code>.</p> <code>None</code> <code>contains</code> <code>Optional[str]</code> <p>Extract any columns that contains this <code>str</code> anywhere within it. Determined by using the <code>in</code> keyword. Defaults to <code>None</code>.</p> <code>None</code> <code>ends_with</code> <code>Optional[str]</code> <p>Extract any columns that ends with this <code>str</code>. Determined by using the <code>str.endswith()</code> method. Defaults to <code>None</code>.</p> <code>None</code> <code>match_case</code> <code>bool</code> <p>If you want to ensure an exact match for the columns, set this to <code>True</code>, else if you want to match the exact case for the columns, set this to <code>False</code>. Defaults to <code>False</code>.</p> <code>False</code> <code>operator</code> <code>Literal['and', 'or', 'and not', 'or not']</code> <p>The logical operator to place between the functions. Only used when there are multiple values parsed to the parameters: <code>starts_with</code>, <code>contains</code>: <code>ends_with</code>. Defaults to <code>and</code>.</p> <code>'and'</code> <p>Returns:</p> Type Description <code>str_list</code> <p>The list of columns which match the criteria specified.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.columns import get_columns_by_likeness\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; values = list(range(1, 6))\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"aaa\": values,\n...             \"aab\": values,\n...             \"aac\": values,\n...             \"afa\": values,\n...             \"afb\": values,\n...             \"afc\": values,\n...             \"bac\": values,\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+-----+-----+-----+-----+-----+-----+-----+\n| aaa | aab | aac | afa | afb | afc | bac |\n+-----+-----+-----+-----+-----+-----+-----+\n|   1 |   1 |   1 |   1 |   1 |   1 |   1 |\n|   2 |   2 |   2 |   2 |   2 |   2 |   2 |\n|   3 |   3 |   3 |   3 |   3 |   3 |   3 |\n|   4 |   4 |   4 |   4 |   4 |   4 |   4 |\n|   5 |   5 |   5 |   5 |   5 |   5 |   5 |\n+-----+-----+-----+-----+-----+-----+-----+\n</code></pre> </p> <p>Example 1: Starts With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\"))\n</code></pre> Terminal<pre><code>[\"aaa\", \"aab\", \"aac\", \"afa\", \"afb\", \"afc\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Contains<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, contains=\"f\"))\n</code></pre> Terminal<pre><code>[\"afa\", \"afb\", \"afc\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Ends With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, ends_with=\"c\"))\n</code></pre> Terminal<pre><code>[\"aac\", \"afc\", \"bac\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Starts With and Contains<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", contains=\"c\"))\n</code></pre> Terminal<pre><code>[\"aac\", \"afc\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: Starts With and Ends With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", ends_with=\"b\"))\n</code></pre> Terminal<pre><code>[\"aab\", \"afb\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: Contains and Ends With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, contains=\"f\", ends_with=\"b\"))\n</code></pre> Terminal<pre><code>[\"afb\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 7: Starts With and Contains and Ends With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", contains=\"f\", ends_with=\"b\"))\n</code></pre> Terminal<pre><code>[\"afb\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 8: Using 'or' Operator<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", operator=\"or\", contains=\"f\"))\n</code></pre> Terminal<pre><code>[\"aaa\", \"aab\", \"aac\", \"afa\", \"afb\", \"afc\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 9: Using 'and not' Operator<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", operator=\"and not\", contains=\"f\"))\n</code></pre> Terminal<pre><code>[\"aaa\", \"aab\", \"aac\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 10: Error Example 1<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=123))\n</code></pre> Terminal<pre><code>TypeError: `starts_with` must be a `string` or `None`.\n</code></pre> <p>Conclusion: Error.</p> <p>Example 11: Error Example 2<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, operator=\"xor\"))\n</code></pre> Terminal<pre><code>ValueError: `operator` must be one of 'and', 'or', 'and not', 'or not'\n</code></pre> <p>Conclusion: Error.</p> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef get_columns_by_likeness(\n    dataframe: psDataFrame,\n    starts_with: Optional[str] = None,\n    contains: Optional[str] = None,\n    ends_with: Optional[str] = None,\n    match_case: bool = False,\n    operator: Literal[\"and\", \"or\", \"and not\", \"or not\"] = \"and\",\n) -&gt; str_list:\n    \"\"\"\n    !!! note \"Summary\"\n        Extract the column names from a given `dataframe` based on text that the column name contains.\n\n    ???+ abstract \"Details\"\n        You can use any combination of `startswith`, `contains`, and `endswith`. Under the hood, these will be implemented with a number of internal `#!py lambda` functions to determine matches.\n\n        The `operator` parameter determines how the conditions (`starts_with`, `contains`, `ends_with`) are combined:\n\n        | Value | Description |\n        |-------|-------------|\n        | `\"and\"` | All conditions must be true.\n        | `\"or\"` | At least one condition must be true.\n        | `\"and not\"` | The first condition must be true and the second condition must be false.\n        | `\"or not\"` | At least one condition must be true, but not all.\n\n    Params:\n        dataframe (psDataFrame):\n            The `dataframe` from which to extract the column names.\n        starts_with (Optional[str], optional):\n            Extract any columns that starts with this `#!py str`.&lt;br&gt;\n            Determined by using the `#!py str.startswith()` method.&lt;br&gt;\n            Defaults to `#!py None`.\n        contains (Optional[str], optional):\n            Extract any columns that contains this `#!py str` anywhere within it.&lt;br&gt;\n            Determined by using the `#!py in` keyword.&lt;br&gt;\n            Defaults to `#!py None`.\n        ends_with (Optional[str], optional):\n            Extract any columns that ends with this `#!py str`.&lt;br&gt;\n            Determined by using the `#!py str.endswith()` method.&lt;br&gt;\n            Defaults to `#!py None`.\n        match_case (bool, optional):\n            If you want to ensure an exact match for the columns, set this to `#!py True`, else if you want to match the exact case for the columns, set this to `#!py False`.&lt;br&gt;\n            Defaults to `#!py False`.\n        operator (Literal[\"and\", \"or\", \"and not\", \"or not\"], optional):\n            The logical operator to place between the functions.&lt;br&gt;\n            Only used when there are multiple values parsed to the parameters: `#!py starts_with`, `#!py contains`: `#!py ends_with`.&lt;br&gt;\n            Defaults to `#!py and`.\n\n    Returns:\n        (str_list):\n            The list of columns which match the criteria specified.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.columns import get_columns_by_likeness\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; values = list(range(1, 6))\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"aaa\": values,\n        ...             \"aab\": values,\n        ...             \"aac\": values,\n        ...             \"afa\": values,\n        ...             \"afb\": values,\n        ...             \"afc\": values,\n        ...             \"bac\": values,\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-----+-----+-----+-----+-----+-----+-----+\n        | aaa | aab | aac | afa | afb | afc | bac |\n        +-----+-----+-----+-----+-----+-----+-----+\n        |   1 |   1 |   1 |   1 |   1 |   1 |   1 |\n        |   2 |   2 |   2 |   2 |   2 |   2 |   2 |\n        |   3 |   3 |   3 |   3 |   3 |   3 |   3 |\n        |   4 |   4 |   4 |   4 |   4 |   4 |   4 |\n        |   5 |   5 |   5 |   5 |   5 |   5 |   5 |\n        +-----+-----+-----+-----+-----+-----+-----+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Starts With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aaa\", \"aab\", \"aac\", \"afa\", \"afb\", \"afc\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Contains\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, contains=\"f\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"afa\", \"afb\", \"afc\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Ends With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, ends_with=\"c\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aac\", \"afc\", \"bac\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Starts With and Contains\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", contains=\"c\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aac\", \"afc\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Starts With and Ends With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", ends_with=\"b\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aab\", \"afb\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Contains and Ends With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, contains=\"f\", ends_with=\"b\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"afb\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: Starts With and Contains and Ends With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", contains=\"f\", ends_with=\"b\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"afb\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: Using 'or' Operator\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", operator=\"or\", contains=\"f\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aaa\", \"aab\", \"aac\", \"afa\", \"afb\", \"afc\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 9: Using 'and not' Operator\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", operator=\"and not\", contains=\"f\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aaa\", \"aab\", \"aac\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 10: Error Example 1\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=123))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        TypeError: `starts_with` must be a `string` or `None`.\n        ```\n        !!! failure \"Conclusion: Error.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 11: Error Example 2\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, operator=\"xor\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        ValueError: `operator` must be one of 'and', 'or', 'and not', 'or not'\n        ```\n        !!! failure \"Conclusion: Error.\"\n        &lt;/div&gt;\n    \"\"\"\n\n    # Columns\n    cols: str_list = dataframe.columns\n    if not match_case:\n        cols = [col.upper() for col in cols]\n        starts_with = starts_with.upper() if starts_with is not None else None\n        contains = contains.upper() if contains is not None else None\n        ends_with = ends_with.upper() if ends_with is not None else None\n\n    # Parameters\n    o_: Literal[\"and\", \"or\", \"and not\", \"or not\"] = operator\n    s_: bool = starts_with is not None\n    c_: bool = contains is not None\n    e_: bool = ends_with is not None\n\n    # Functions\n    _ops = {\n        \"and\": lambda x, y: x and y,\n        \"or\": lambda x, y: x or y,\n        \"and not\": lambda x, y: x and not y,\n        \"or not\": lambda x, y: x or not y,\n    }\n    _s = lambda col, s: col.startswith(s)\n    _c = lambda col, c: c in col\n    _e = lambda col, e: col.endswith(e)\n    _sc = lambda col, s, c: _ops[o_](_s(col, s), _c(col, c))\n    _se = lambda col, s, e: _ops[o_](_s(col, s), _e(col, e))\n    _ce = lambda col, c, e: _ops[o_](_c(col, c), _e(col, e))\n    _sce = lambda col, s, c, e: _ops[o_](_ops[o_](_s(col, s), _c(col, c)), _e(col, e))\n\n    # Logic\n    if s_ and not c_ and not e_:\n        return [col for col in cols if _s(col, starts_with)]\n    elif c_ and not s_ and not e_:\n        return [col for col in cols if _c(col, contains)]\n    elif e_ and not s_ and not c_:\n        return [col for col in cols if _e(col, ends_with)]\n    elif s_ and c_ and not e_:\n        return [col for col in cols if _sc(col, starts_with, contains)]\n    elif s_ and e_ and not c_:\n        return [col for col in cols if _se(col, starts_with, ends_with)]\n    elif c_ and e_ and not s_:\n        return [col for col in cols if _ce(col, contains, ends_with)]\n    elif s_ and c_ and e_:\n        return [col for col in cols if _sce(col, starts_with, contains, ends_with)]\n    else:\n        return cols\n</code></pre>"},{"location":"code/columns/#toolbox_pyspark.columns.rename_columns","title":"rename_columns","text":"<pre><code>rename_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n    string_function: str = \"upper\",\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Use one of the common Python string functions to be applied to one or multiple columns.</p> Details <p>The <code>string_function</code> must be a valid string method. For more info on available functions, see: https://docs.python.org/3/library/stdtypes.html#string-methods</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>The columns to be updated. Must be a valid column on <code>dataframe</code>. If not provided, will be applied to all columns. It is also possible to parse the values <code>\"all\"</code>, which will also apply this function to all columns in <code>dataframe</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>string_function</code> <code>str</code> <p>The string function to be applied. Defaults to <code>\"upper\"</code>.</p> <code>'upper'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Import\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.columns import rename_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Single column, default params<pre><code>&gt;&gt;&gt; print(rename_columns(df, \"a\").columns)\n</code></pre> Terminal<pre><code>[\"A\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Single column, simple function<pre><code>&gt;&gt;&gt; print(rename_columns(df, \"a\", \"upper\").columns)\n</code></pre> Terminal<pre><code>[\"A\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Single column, complex function<pre><code>&gt;&gt;&gt; print(rename_columns(df, \"a\", \"replace('b', 'test')\").columns)\n</code></pre> Terminal<pre><code>[\"a\", \"test\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Multiple columns<pre><code>&gt;&gt;&gt; print(rename_columns(df, [\"a\", \"b\"]).columns)\n</code></pre> Terminal<pre><code>[\"A\", \"B\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: Default function over all columns<pre><code>&gt;&gt;&gt; print(rename_columns(df).columns)\n</code></pre> Terminal<pre><code>[\"A\", \"B\", \"C\", \"D\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: Complex function over multiple columns<pre><code>&gt;&gt;&gt; print(rename_columns(df, [\"a\", \"b\"], \"replace('b', 'test')\").columns)\n</code></pre> Terminal<pre><code>[\"a\", \"test\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> See Also <ul> <li><code>assert_columns_exists()</code></li> <li><code>assert_column_exists()</code></li> </ul> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef rename_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n    string_function: str = \"upper\",\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Use one of the common Python string functions to be applied to one or multiple columns.\n\n    ???+ abstract \"Details\"\n        The `string_function` must be a valid string method. For more info on available functions, see: https://docs.python.org/3/library/stdtypes.html#string-methods\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        columns (Optional[Union[str, str_collection]], optional):\n            The columns to be updated.&lt;br&gt;\n            Must be a valid column on `dataframe`.&lt;br&gt;\n            If not provided, will be applied to all columns.&lt;br&gt;\n            It is also possible to parse the values `\"all\"`, which will also apply this function to all columns in `dataframe`.&lt;br&gt;\n            Defaults to `None`.\n        string_function (str, optional):\n            The string function to be applied. Defaults to `\"upper\"`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Import\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.columns import rename_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n        ...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Single column, default params\"}\n        &gt;&gt;&gt; print(rename_columns(df, \"a\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"A\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Single column, simple function\"}\n        &gt;&gt;&gt; print(rename_columns(df, \"a\", \"upper\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"A\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Single column, complex function\"}\n        &gt;&gt;&gt; print(rename_columns(df, \"a\", \"replace('b', 'test')\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"test\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Multiple columns\"}\n        &gt;&gt;&gt; print(rename_columns(df, [\"a\", \"b\"]).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"A\", \"B\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Default function over all columns\"}\n        &gt;&gt;&gt; print(rename_columns(df).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"A\", \"B\", \"C\", \"D\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Complex function over multiple columns\"}\n        &gt;&gt;&gt; print(rename_columns(df, [\"a\", \"b\"], \"replace('b', 'test')\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"test\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`assert_column_exists()`][toolbox_pyspark.checks.assert_column_exists]\n    \"\"\"\n    columns = get_columns(dataframe, columns)\n    assert_columns_exists(dataframe=dataframe, columns=columns, match_case=True)\n    cols_exprs: dict[str, str] = {\n        col: eval(\n            f\"'{col}'.{string_function}{'()' if not string_function.endswith(')') else ''}\"\n        )\n        for col in columns\n    }\n    return dataframe.withColumnsRenamed(cols_exprs)\n</code></pre>"},{"location":"code/columns/#toolbox_pyspark.columns.reorder_columns","title":"reorder_columns","text":"<pre><code>reorder_columns(\n    dataframe: psDataFrame,\n    new_order: Optional[str_collection] = None,\n    missing_columns_last: bool = True,\n    key_columns_position: Optional[\n        Literal[\"first\", \"last\"]\n    ] = \"first\",\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Reorder the columns in a given DataFrame in to a custom order, or to put the <code>key_</code> columns at the end (that is, to the far right) of the dataframe.</p> Details <p>The decision flow chart is as follows:</p> <pre><code>graph TD\n    a([begin])\n    z([end])\n    b{{new_order}}\n    c{{missing_cols_last}}\n    d{{key_cols_position}}\n    g[cols = dataframe.columns]\n    h[cols = new_order]\n    i[cols += missing_cols]\n    j[cols = non_key_cols + key_cols]\n    k[cols = key_cols + non_key_cols]\n    l[\"return dataframe.select(cols)\"]\n    a --&gt; b\n    b --is not None--&gt; h --&gt; c\n    b --is None--&gt; g --&gt; d\n    c --False--&gt; l\n    c --True--&gt; i ----&gt; l\n    d --\"first\"--&gt; k ---&gt; l\n    d --\"last\"---&gt; j --&gt; l\n    d --None--&gt; l\n    l --&gt; z</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update</p> required <code>new_order</code> <code>Optional[Union[str, str_list, str_tuple, str_set]]</code> <p>The custom order for the columns on the order. Defaults to <code>None</code>.</p> <code>None</code> <code>missing_columns_last</code> <code>bool</code> <p>For any columns existing on <code>dataframes.columns</code>, but missing from <code>new_order</code>, if <code>missing_columns_last=True</code>, then include those missing columns to the right of the dataframe, in the same order that they originally appear. Defaults to <code>True</code>.</p> <code>True</code> <code>key_columns_position</code> <code>Optional[Literal['first', 'last']]</code> <p>Where should the <code>\"key_*\"</code> columns be located?.</p> <ul> <li>If <code>\"first\"</code>, then they will be relocated to the start of the dataframe, before all other columns.</li> <li>If <code>\"last\"</code>, then they will be relocated to the end of the dataframe, after all other columns.</li> <li>If <code>None</code>, they they will remain their original order.</li> </ul> <p>Regardless of their position, their original order will be maintained. Defaults to <code>\"first\"</code>.</p> <code>'first'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.columns import reorder_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"key_a\": [\"0\", \"1\", \"2\", \"3\"],\n...             \"c\": [\"1\", \"1\", \"1\", \"1\"],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...             \"key_c\": [\"1\", \"1\", \"1\", \"1\"],\n...             \"key_e\": [\"3\", \"3\", \"3\", \"3\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+-------+---+---+-------+-------+\n| a | b | key_a | c | d | key_c | key_e |\n+---+---+-------+---+---+-------+-------+\n| 0 | a |     0 | 1 | 2 |     1 |     3 |\n| 1 | b |     1 | 1 | 2 |     1 |     3 |\n| 2 | c |     2 | 1 | 2 |     1 |     3 |\n| 3 | d |     3 | 1 | 2 |     1 |     3 |\n+---+---+-------+---+---+-------+-------+\n</code></pre> </p> <p>Example 1: Default config<pre><code>&gt;&gt;&gt; new_df = reorder_columns(dataframe=df)\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+-------+---+---+---+---+\n| key_a | key_c | key_e | a | b | c | d |\n+-------+-------+-------+---+---+---+---+\n|     0 |     1 |     3 | 0 | a | 1 | 2 |\n|     1 |     1 |     3 | 1 | b | 1 | 2 |\n|     2 |     1 |     3 | 2 | c | 1 | 2 |\n|     3 |     1 |     3 | 3 | d | 1 | 2 |\n+-------+-------+-------+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Custom order<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     new_order=[\"key_a\", \"key_c\", \"b\", \"key_e\", \"a\", \"c\", \"d\"],\n... )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+---+-------+---+---+---+\n| key_a | key_c | b | key_e | a | c | d |\n+-------+-------+---+-------+---+---+---+\n|     0 |     1 | a |     3 | 0 | 1 | 2 |\n|     1 |     1 | b |     3 | 1 | 1 | 2 |\n|     2 |     1 | c |     3 | 2 | 1 | 2 |\n|     3 |     1 | d |     3 | 3 | 1 | 2 |\n+-------+-------+---+-------+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Custom order, include missing columns<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     new_order=[\"key_a\", \"key_c\", \"a\", \"b\"],\n...     missing_columns_last=True,\n...     )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+---+---+-------+---+---+\n| key_a | key_c | a | b | key_e | c | d |\n+-------+-------+---+---+-------+---+---+\n|     0 |     1 | 0 | a |     3 | 1 | 2 |\n|     1 |     1 | 1 | b |     3 | 1 | 2 |\n|     2 |     1 | 2 | c |     3 | 1 | 2 |\n|     3 |     1 | 3 | d |     3 | 1 | 2 |\n+-------+-------+---+---+-------+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Custom order, exclude missing columns<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     new_order=[\"key_a\", \"key_c\", \"a\", \"b\"],\n...     missing_columns_last=False,\n... )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+---+---+\n| key_a | key_c | a | b |\n+-------+-------+---+---+\n|     0 |     1 | 0 | a |\n|     1 |     1 | 1 | b |\n|     2 |     1 | 2 | c |\n|     3 |     1 | 3 | d |\n+-------+-------+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: Keys last<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     key_columns_position=\"last\",\n... )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+-------+-------+-------+\n| a | b | c | d | key_a | key_c | key_e |\n+---+---+---+---+-------+-------+-------+\n| 0 | a | 1 | 2 |     0 |     1 |     3 |\n| 1 | b | 1 | 2 |     1 |     1 |     3 |\n| 2 | c | 1 | 2 |     2 |     1 |     3 |\n| 3 | d | 1 | 2 |     3 |     1 |     3 |\n+---+---+---+---+-------+-------+-------+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: Keys first<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     key_columns_position=\"first\",\n... )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+-------+---+---+---+---+\n| key_a | key_c | key_e | a | b | c | d |\n+-------+-------+-------+---+---+---+---+\n|     0 |     1 |     3 | 0 | a | 1 | 2 |\n|     1 |     1 |     3 | 1 | b | 1 | 2 |\n|     2 |     1 |     3 | 2 | c | 1 | 2 |\n|     3 |     1 |     3 | 3 | d | 1 | 2 |\n+-------+-------+-------+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef reorder_columns(\n    dataframe: psDataFrame,\n    new_order: Optional[str_collection] = None,\n    missing_columns_last: bool = True,\n    key_columns_position: Optional[Literal[\"first\", \"last\"]] = \"first\",\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Reorder the columns in a given DataFrame in to a custom order, or to put the `key_` columns at the end (that is, to the far right) of the dataframe.\n\n    ???+ abstract \"Details\"\n        The decision flow chart is as follows:\n\n        ```mermaid\n        graph TD\n            a([begin])\n            z([end])\n            b{{new_order}}\n            c{{missing_cols_last}}\n            d{{key_cols_position}}\n            g[cols = dataframe.columns]\n            h[cols = new_order]\n            i[cols += missing_cols]\n            j[cols = non_key_cols + key_cols]\n            k[cols = key_cols + non_key_cols]\n            l[\"return dataframe.select(cols)\"]\n            a --&gt; b\n            b --is not None--&gt; h --&gt; c\n            b --is None--&gt; g --&gt; d\n            c --False--&gt; l\n            c --True--&gt; i ----&gt; l\n            d --\"first\"--&gt; k ---&gt; l\n            d --\"last\"---&gt; j --&gt; l\n            d --None--&gt; l\n            l --&gt; z\n        ```\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update\n        new_order (Optional[Union[str, str_list, str_tuple, str_set]], optional):\n            The custom order for the columns on the order.&lt;br&gt;\n            Defaults to `#!py None`.\n        missing_columns_last (bool, optional):\n            For any columns existing on `#!py dataframes.columns`, but missing from `#!py new_order`, if `#!py missing_columns_last=True`, then include those missing columns to the right of the dataframe, in the same order that they originally appear.&lt;br&gt;\n            Defaults to `#!py True`.\n        key_columns_position (Optional[Literal[\"first\", \"last\"]], optional):\n            Where should the `#!py \"key_*\"` columns be located?.&lt;br&gt;\n\n            - If `#!py \"first\"`, then they will be relocated to the start of the dataframe, before all other columns.\n            - If `#!py \"last\"`, then they will be relocated to the end of the dataframe, after all other columns.\n            - If `#!py None`, they they will remain their original order.\n\n            Regardless of their position, their original order will be maintained.\n            Defaults to `#!py \"first\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.columns import reorder_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"key_a\": [\"0\", \"1\", \"2\", \"3\"],\n        ...             \"c\": [\"1\", \"1\", \"1\", \"1\"],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...             \"key_c\": [\"1\", \"1\", \"1\", \"1\"],\n        ...             \"key_e\": [\"3\", \"3\", \"3\", \"3\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+-------+---+---+-------+-------+\n        | a | b | key_a | c | d | key_c | key_e |\n        +---+---+-------+---+---+-------+-------+\n        | 0 | a |     0 | 1 | 2 |     1 |     3 |\n        | 1 | b |     1 | 1 | 2 |     1 |     3 |\n        | 2 | c |     2 | 1 | 2 |     1 |     3 |\n        | 3 | d |     3 | 1 | 2 |     1 |     3 |\n        +---+---+-------+---+---+-------+-------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default config\"}\n        &gt;&gt;&gt; new_df = reorder_columns(dataframe=df)\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+-------+---+---+---+---+\n        | key_a | key_c | key_e | a | b | c | d |\n        +-------+-------+-------+---+---+---+---+\n        |     0 |     1 |     3 | 0 | a | 1 | 2 |\n        |     1 |     1 |     3 | 1 | b | 1 | 2 |\n        |     2 |     1 |     3 | 2 | c | 1 | 2 |\n        |     3 |     1 |     3 | 3 | d | 1 | 2 |\n        +-------+-------+-------+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Custom order\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     new_order=[\"key_a\", \"key_c\", \"b\", \"key_e\", \"a\", \"c\", \"d\"],\n        ... )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+---+-------+---+---+---+\n        | key_a | key_c | b | key_e | a | c | d |\n        +-------+-------+---+-------+---+---+---+\n        |     0 |     1 | a |     3 | 0 | 1 | 2 |\n        |     1 |     1 | b |     3 | 1 | 1 | 2 |\n        |     2 |     1 | c |     3 | 2 | 1 | 2 |\n        |     3 |     1 | d |     3 | 3 | 1 | 2 |\n        +-------+-------+---+-------+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Custom order, include missing columns\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     new_order=[\"key_a\", \"key_c\", \"a\", \"b\"],\n        ...     missing_columns_last=True,\n        ...     )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+---+---+-------+---+---+\n        | key_a | key_c | a | b | key_e | c | d |\n        +-------+-------+---+---+-------+---+---+\n        |     0 |     1 | 0 | a |     3 | 1 | 2 |\n        |     1 |     1 | 1 | b |     3 | 1 | 2 |\n        |     2 |     1 | 2 | c |     3 | 1 | 2 |\n        |     3 |     1 | 3 | d |     3 | 1 | 2 |\n        +-------+-------+---+---+-------+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Custom order, exclude missing columns\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     new_order=[\"key_a\", \"key_c\", \"a\", \"b\"],\n        ...     missing_columns_last=False,\n        ... )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+---+---+\n        | key_a | key_c | a | b |\n        +-------+-------+---+---+\n        |     0 |     1 | 0 | a |\n        |     1 |     1 | 1 | b |\n        |     2 |     1 | 2 | c |\n        |     3 |     1 | 3 | d |\n        +-------+-------+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Keys last\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     key_columns_position=\"last\",\n        ... )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+-------+-------+-------+\n        | a | b | c | d | key_a | key_c | key_e |\n        +---+---+---+---+-------+-------+-------+\n        | 0 | a | 1 | 2 |     0 |     1 |     3 |\n        | 1 | b | 1 | 2 |     1 |     1 |     3 |\n        | 2 | c | 1 | 2 |     2 |     1 |     3 |\n        | 3 | d | 1 | 2 |     3 |     1 |     3 |\n        +---+---+---+---+-------+-------+-------+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Keys first\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     key_columns_position=\"first\",\n        ... )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+-------+---+---+---+---+\n        | key_a | key_c | key_e | a | b | c | d |\n        +-------+-------+-------+---+---+---+---+\n        |     0 |     1 |     3 | 0 | a | 1 | 2 |\n        |     1 |     1 |     3 | 1 | b | 1 | 2 |\n        |     2 |     1 |     3 | 2 | c | 1 | 2 |\n        |     3 |     1 |     3 | 3 | d | 1 | 2 |\n        +-------+-------+-------+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n    \"\"\"\n    df_cols: str_list = dataframe.columns\n    if new_order is not None:\n        cols: str_list = get_columns(dataframe, new_order)\n        if missing_columns_last:\n            cols += [col for col in df_cols if col not in new_order]\n    else:\n        non_key_cols: str_list = [\n            col for col in df_cols if not col.lower().startswith(\"key_\")\n        ]\n        key_cols: str_list = [col for col in df_cols if col.lower().startswith(\"key_\")]\n        if key_columns_position == \"first\":\n            cols = key_cols + non_key_cols\n        elif key_columns_position == \"last\":\n            cols = non_key_cols + key_cols\n        else:\n            cols = df_cols\n    return dataframe.select(cols)\n</code></pre>"},{"location":"code/columns/#toolbox_pyspark.columns.delete_columns","title":"delete_columns","text":"<pre><code>delete_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    missing_column_handler: Literal[\n        \"raise\", \"warn\", \"pass\"\n    ] = \"pass\",\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given <code>dataframe</code>, delete the columns listed in <code>columns</code>.</p> Details <p>You can use <code>missing_columns_handler</code> to specify how to handle missing columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The dataframe from which to delete the columns</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The list of columns to delete.</p> required <code>missing_column_handler</code> <code>Literal['raise', 'warn', 'pass']</code> <p>How to handle any columns which are missing from <code>dataframe.columns</code>.</p> <p>If any columns in <code>columns</code> are missing from <code>dataframe.columns</code>, then the following will happen for each option:</p> Option Result <code>\"raise\"</code> An <code>AttributeError</code> exception will be raised <code>\"warn\"</code> An <code>AttributeWarning</code> warning will be raised <code>\"pass\"</code> Nothing will be raised <p>Defaults to <code>\"pass\"</code>.</p> <code>'pass'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated <code>dataframe</code>, with the columns listed in <code>columns</code> having been removed.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.columns import delete_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Single column<pre><code>&gt;&gt;&gt; df.transform(delete_columns, \"a\").show()\n</code></pre> Terminal<pre><code>+---+---+---+\n| b | c | d |\n+---+---+---+\n| a | c | d |\n| b | c | d |\n| c | c | d |\n| d | c | d |\n+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Multiple columns<pre><code>&gt;&gt;&gt; df.transform(delete_columns, [\"a\", \"b\"]).show()\n</code></pre> Terminal<pre><code>+---+---+\n| c | d |\n+---+---+\n| c | d |\n| c | d |\n| c | d |\n| c | d |\n+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Single column missing, raises error<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=\"z\",\n...         missing_column_handler=\"raise\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>AttributeError: Columns [\"z\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Multiple columns, one missing, raises error<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"a\", \"b\", \"z\"],\n...         missing_column_handler=\"raise\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>AttributeError: Columns [\"z\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: Multiple columns, all missing, raises error<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"x\", \"y\", \"z\"],\n...         missing_column_handler=\"raise\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>AttributeError: Columns [\"x\", \"y\", \"z\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: Single column missing, raises warning<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=\"z\",\n...         missing_column_handler=\"warn\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>AttributeWarning: Columns missing from \"dataframe\": [\"z\"].\nWill still proceed to delete columns that do exist.\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 7: Multiple columns, one missing, raises warning<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"a\", \"b\", \"z\"],\n...         missing_column_handler=\"warn\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>AttributeWarning: Columns missing from \"dataframe\": [\"z\"].\nWill still proceed to delete columns that do exist.\n</code></pre> Terminal<pre><code>+---+---+\n| c | d |\n+---+---+\n| c | d |\n| c | d |\n| c | d |\n| c | d |\n+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 8: Multiple columns, all missing, raises warning<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"x\", \"y\", \"z\"],\n...         missing_column_handler=\"warn\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>AttributeWarning: Columns missing from \"dataframe\": [\"x\", \"y\", \"z\"].\nWill still proceed to delete columns that do exist.\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 9: Single column missing, nothing raised<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=\"z\",\n...         missing_column_handler=\"pass\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 10: Multiple columns, one missing, nothing raised<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"a\", \"b\", \"z\"],\n...         missing_column_handler=\"pass\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>+---+---+\n| c | d |\n+---+---+\n| c | d |\n| c | d |\n| c | d |\n| c | d |\n+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 11: Multiple columns, all missing, nothing raised<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"x\", \"y\", \"z\"],\n...         missing_column_handler=\"pass\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef delete_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    missing_column_handler: Literal[\"raise\", \"warn\", \"pass\"] = \"pass\",\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `#!py dataframe`, delete the columns listed in `columns`.\n\n    ???+ abstract \"Details\"\n        You can use `#!py missing_columns_handler` to specify how to handle missing columns.\n\n    Params:\n        dataframe (psDataFrame):\n            The dataframe from which to delete the columns\n        columns (Union[str, str_collection]):\n            The list of columns to delete.\n        missing_column_handler (Literal[\"raise\", \"warn\", \"pass\"], optional):\n            How to handle any columns which are missing from `#!py dataframe.columns`.\n\n            If _any_ columns in `columns` are missing from `#!py dataframe.columns`, then the following will happen for each option:\n\n            | Option | Result |\n            |--------|--------|\n            | `#!py \"raise\"` | An `#!py AttributeError` exception will be raised\n            | `#!py \"warn\"` | An `#!py AttributeWarning` warning will be raised\n            | `#!py \"pass\"` | Nothing will be raised\n\n            Defaults to `#!py \"pass\"`.\n\n    Returns:\n        (psDataFrame):\n            The updated `#!py dataframe`, with the columns listed in `#!py columns` having been removed.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.columns import delete_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n        ...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Single column\"}\n        &gt;&gt;&gt; df.transform(delete_columns, \"a\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+\n        | b | c | d |\n        +---+---+---+\n        | a | c | d |\n        | b | c | d |\n        | c | c | d |\n        | d | c | d |\n        +---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Multiple columns\"}\n        &gt;&gt;&gt; df.transform(delete_columns, [\"a\", \"b\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+\n        | c | d |\n        +---+---+\n        | c | d |\n        | c | d |\n        | c | d |\n        | c | d |\n        +---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Single column missing, raises error\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=\"z\",\n        ...         missing_column_handler=\"raise\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Columns [\"z\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Multiple columns, one missing, raises error\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"a\", \"b\", \"z\"],\n        ...         missing_column_handler=\"raise\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Columns [\"z\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Multiple columns, all missing, raises error\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"x\", \"y\", \"z\"],\n        ...         missing_column_handler=\"raise\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Columns [\"x\", \"y\", \"z\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Single column missing, raises warning\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=\"z\",\n        ...         missing_column_handler=\"warn\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeWarning: Columns missing from \"dataframe\": [\"z\"].\n        Will still proceed to delete columns that do exist.\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: Multiple columns, one missing, raises warning\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"a\", \"b\", \"z\"],\n        ...         missing_column_handler=\"warn\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeWarning: Columns missing from \"dataframe\": [\"z\"].\n        Will still proceed to delete columns that do exist.\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+\n        | c | d |\n        +---+---+\n        | c | d |\n        | c | d |\n        | c | d |\n        | c | d |\n        +---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: Multiple columns, all missing, raises warning\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"x\", \"y\", \"z\"],\n        ...         missing_column_handler=\"warn\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeWarning: Columns missing from \"dataframe\": [\"x\", \"y\", \"z\"].\n        Will still proceed to delete columns that do exist.\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 9: Single column missing, nothing raised\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=\"z\",\n        ...         missing_column_handler=\"pass\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 10: Multiple columns, one missing, nothing raised\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"a\", \"b\", \"z\"],\n        ...         missing_column_handler=\"pass\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+\n        | c | d |\n        +---+---+\n        | c | d |\n        | c | d |\n        | c | d |\n        | c | d |\n        +---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 11: Multiple columns, all missing, nothing raised\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"x\", \"y\", \"z\"],\n        ...         missing_column_handler=\"pass\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n    \"\"\"\n    columns = get_columns(dataframe, columns)\n    if missing_column_handler == \"raise\":\n        assert_columns_exists(dataframe=dataframe, columns=columns)\n    elif missing_column_handler == \"warn\":\n        warn_columns_missing(dataframe=dataframe, columns=columns)\n    elif missing_column_handler == \"pass\":\n        pass\n    return dataframe.select([col for col in dataframe.columns if col not in columns])\n</code></pre>"},{"location":"code/dimensions/","title":"Dimensions","text":""},{"location":"code/dimensions/#toolbox_pyspark.dimensions","title":"toolbox_pyspark.dimensions","text":"<p>Summary</p> <p>The <code>dimensions</code> module is used for checking the dimensions of <code>pyspark</code> <code>dataframe</code>'s.</p>"},{"location":"code/dimensions/#toolbox_pyspark.dimensions.get_dims","title":"get_dims","text":"<pre><code>get_dims(\n    dataframe: psDataFrame,\n    use_names: bool = True,\n    use_comma: bool = True,\n) -&gt; Union[\n    dict[str, str],\n    dict[str, int],\n    tuple[str, str],\n    tuple[int, int],\n]\n</code></pre> <p>Summary</p> <p>Extract the dimensions of a given <code>dataframe</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The table to check.</p> required <code>use_names</code> <code>bool</code> <p>Whether or not to add <code>names</code> to the returned object. If <code>True</code>, then will return a <code>dict</code> with two keys only, for the number of <code>rows</code> and <code>cols</code>. Defaults to <code>True</code>.</p> <code>True</code> <code>use_comma</code> <code>bool</code> <p>Whether or not to add a comma <code>,</code> to the returned object. Defaults to <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, Union[str, int]], tuple[str, ...], tTuple[int, ...]]</code> <p>The dimensions of the given <code>dataframe</code>.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.dimensions import get_dims\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame({\n...         'a': range(5000),\n...         'b': range(5000),\n...     })\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; print(df.count())\n&gt;&gt;&gt; print(len(df.columns))\n</code></pre> <pre><code>5000\n</code></pre> <p><pre><code>2\n</code></pre> </p> <p>Names and commas<pre><code>&gt;&gt;&gt; print(get_dims(dataframe=df, use_names=True, use_commas=True))\n</code></pre> <pre><code>{\"rows\": \"5,000\", \"cols\": \"2\"}\n</code></pre> </p> <p>Names but no commas<pre><code>&gt;&gt;&gt; print(get_dims(dataframe=df, use_names=True, use_commas=False))\n</code></pre> <pre><code>{\"rows\": 5000, \"cols\": 2}\n</code></pre> </p> <p>Commas but no names<pre><code>&gt;&gt;&gt; print(get_dims(dataframe=df, use_names=False, use_commas=True))\n</code></pre> <pre><code>(\"5,000\", \"2\")\n</code></pre> </p> <p>Neither names nor commas<pre><code>&gt;&gt;&gt; print(get_dims(dataframe=df, use_names=False, use_commas=False))\n</code></pre> <pre><code>(5000, 2)\n</code></pre> </p> Source code in <code>src/toolbox_pyspark/dimensions.py</code> <pre><code>@typechecked\ndef get_dims(\n    dataframe: psDataFrame,\n    use_names: bool = True,\n    use_comma: bool = True,\n) -&gt; Union[dict[str, str], dict[str, int], tuple[str, str], tuple[int, int]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Extract the dimensions of a given `dataframe`.\n\n    Params:\n        dataframe (psDataFrame):\n            The table to check.\n        use_names (bool, optional):\n            Whether or not to add `names` to the returned object.&lt;br&gt;\n            If `#!py True`, then will return a `#!py dict` with two keys only, for the number of `rows` and `cols`.&lt;br&gt;\n            Defaults to `#!py True`.\n        use_comma (bool, optional):\n            Whether or not to add a comma `,` to the returned object.&lt;br&gt;\n            Defaults to `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (Union[Dict[str, Union[str, int]], tuple[str, ...], tTuple[int, ...]]):\n            The dimensions of the given `dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.dimensions import get_dims\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame({\n        ...         'a': range(5000),\n        ...         'b': range(5000),\n        ...     })\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; print(df.count())\n        &gt;&gt;&gt; print(len(df.columns))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n        5000\n        ```\n\n        ```{.txt .text}\n        2\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Names and commas\"}\n        &gt;&gt;&gt; print(get_dims(dataframe=df, use_names=True, use_commas=True))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        {\"rows\": \"5,000\", \"cols\": \"2\"}\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Names but no commas\"}\n        &gt;&gt;&gt; print(get_dims(dataframe=df, use_names=True, use_commas=False))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        {\"rows\": 5000, \"cols\": 2}\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Commas but no names\"}\n        &gt;&gt;&gt; print(get_dims(dataframe=df, use_names=False, use_commas=True))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        (\"5,000\", \"2\")\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Neither names nor commas\"}\n        &gt;&gt;&gt; print(get_dims(dataframe=df, use_names=False, use_commas=False))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        (5000, 2)\n        ```\n        &lt;/div&gt;\n    \"\"\"\n    dims: tuple[int, int] = (dataframe.count(), len(dataframe.columns))\n    if use_names and use_comma:\n        return {\"rows\": f\"{dims[0]:,}\", \"cols\": f\"{dims[1]:,}\"}\n    elif use_names and not use_comma:\n        return {\"rows\": dims[0], \"cols\": dims[1]}\n    elif not use_names and use_comma:\n        return (f\"{dims[0]:,}\", f\"{dims[1]:,}\")\n    else:\n        return dims\n</code></pre>"},{"location":"code/dimensions/#toolbox_pyspark.dimensions.get_dims_of_tables","title":"get_dims_of_tables","text":"<pre><code>get_dims_of_tables(\n    tables: str_list,\n    scope: Optional[dict] = None,\n    use_comma: bool = True,\n) -&gt; pdDataFrame\n</code></pre> <p>Summary</p> <p>Take in a list of the names of some tables, and for each of them, check their dimensions.</p> Details <p>This function will check against the <code>global()</code> scope. So you need to be careful if you're dealing with massive amounts of data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>List[str]</code> <p>The list of the tables that will be checked.</p> required <code>scope</code> <code>dict</code> <p>This is the scope against which the tables will be checked. If <code>None</code>, then it will use the <code>global()</code> scope by default.. Defaults to <code>None</code>.</p> <code>None</code> <code>use_comma</code> <code>bool</code> <p>Whether or not the dimensions from the tables should be formatted as a string with a comma as the thousandths delimiter. Defaults to <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A <code>pandas</code> <code>dataframe</code> with four columns: <code>[\"table\", \"type\", \"rows\", \"cols\"]</code>.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.dimensions import get_dims_of_tables, get_dims\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df1 = spark.createDataFrame(\n...     pd.DataFrame({\n...         'a': range(5000),\n...         'b': range(5000),\n...     })\n... )\n&gt;&gt;&gt; df2 = spark.createDataFrame(\n...     pd.DataFrame({\n...         'a': range(10000),\n...         'b': range(10000),\n...         'c': range(10000),\n...     })\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; print(get_dims(df1))\n&gt;&gt;&gt; print(get_dims(df1))\n</code></pre> <pre><code>{\"rows\": \"5000\", \"cols\": \"2\"}\n</code></pre> <p><pre><code>{\"rows\": \"10000\", \"cols\": \"3\"}\n</code></pre> </p> <p>Basic usage<pre><code>&gt;&gt;&gt; print(get_dims_of_tables(['df1', 'df2']))\n</code></pre> <pre><code>  table type  rows cols\n0   df1      5,000    2\n1   df2      1,000    3\n</code></pre> </p> <p>No commas<pre><code>&gt;&gt;&gt; print(get_dims_of_tables(['df1', 'df2'], use_commas=False))\n</code></pre> <pre><code>  table type rows cols\n0   df1      5000    2\n1   df2      1000    3\n</code></pre> </p> <p>Missing DF<pre><code>&gt;&gt;&gt; display(get_dims_of_tables(['df1', 'df2', 'df3'], use_comma=False))\n</code></pre> <pre><code>  table type rows cols\n0   df1      5000    2\n1   df2      1000    3\n1   df3       NaN  NaN\n</code></pre> </p> Notes <ul> <li>The first column of the returned table is the name of the table from the <code>scope</code> provided.</li> <li>The second column of the returned table is the <code>type</code> of the table. That is, whether the table is one of <code>[\"prd\", \"arc\", \"acm\"]</code>, which are for 'production', 'archive', accumulation' categories. This is designated by the table containing an underscore (<code>_</code>), and having a suffic of either one of: <code>\"prd\"</code>, <code>\"arc\"</code>, or <code>\"acm\"</code>. If the table does not contain this info, then the value in this second column will just be blank.</li> <li>If one of the tables given in the <code>tables</code> list does not exist in the <code>scope</code>, then the values given in the <code>rows</code> and <code>cols</code> columns will either be the values: <code>np.nan</code> or <code>\"Did not load\"</code>.</li> </ul> Source code in <code>src/toolbox_pyspark/dimensions.py</code> <pre><code>@typechecked\ndef get_dims_of_tables(\n    tables: str_list,\n    scope: Optional[dict] = None,\n    use_comma: bool = True,\n) -&gt; pdDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Take in a list of the names of some tables, and for each of them, check their dimensions.\n\n    ???+ abstract \"Details\"\n        This function will check against the `#!py global()` scope. So you need to be careful if you're dealing with massive amounts of data in memory.\n\n    Params:\n        tables (List[str]):\n            The list of the tables that will be checked.\n        scope (dict, optional):\n            This is the scope against which the tables will be checked.&lt;br&gt;\n            If `#!py None`, then it will use the `#!py global()` scope by default..&lt;br&gt;\n            Defaults to `#!py None`.\n        use_comma (bool, optional):\n            Whether or not the dimensions from the tables should be formatted as a string with a comma as the thousandths delimiter.&lt;br&gt;\n            Defaults to `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (pdDataFrame):\n            A `pandas` `dataframe` with four columns: `#!py [\"table\", \"type\", \"rows\", \"cols\"]`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.dimensions import get_dims_of_tables, get_dims\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df1 = spark.createDataFrame(\n        ...     pd.DataFrame({\n        ...         'a': range(5000),\n        ...         'b': range(5000),\n        ...     })\n        ... )\n        &gt;&gt;&gt; df2 = spark.createDataFrame(\n        ...     pd.DataFrame({\n        ...         'a': range(10000),\n        ...         'b': range(10000),\n        ...         'c': range(10000),\n        ...     })\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; print(get_dims(df1))\n        &gt;&gt;&gt; print(get_dims(df1))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n        {\"rows\": \"5000\", \"cols\": \"2\"}\n        ```\n\n        ```{.txt .text}\n        {\"rows\": \"10000\", \"cols\": \"3\"}\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; print(get_dims_of_tables(['df1', 'df2']))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n          table type  rows cols\n        0   df1      5,000    2\n        1   df2      1,000    3\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"No commas\"}\n        &gt;&gt;&gt; print(get_dims_of_tables(['df1', 'df2'], use_commas=False))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n          table type rows cols\n        0   df1      5000    2\n        1   df2      1000    3\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Missing DF\"}\n        &gt;&gt;&gt; display(get_dims_of_tables(['df1', 'df2', 'df3'], use_comma=False))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n          table type rows cols\n        0   df1      5000    2\n        1   df2      1000    3\n        1   df3       NaN  NaN\n        ```\n        &lt;/div&gt;\n\n    ??? info \"Notes\"\n        - The first column of the returned table is the name of the table from the `scope` provided.\n        - The second column of the returned table is the `type` of the table. That is, whether the table is one of `#!py [\"prd\", \"arc\", \"acm\"]`, which are for 'production', 'archive', accumulation' categories. This is designated by the table containing an underscore (`_`), and having a suffic of either one of: `#!py \"prd\"`, `#!py \"arc\"`, or `#!py \"acm\"`. If the table does not contain this info, then the value in this second column will just be blank.\n        - If one of the tables given in the `tables` list does not exist in the `scope`, then the values given in the `rows` and `cols` columns will either be the values: `#!py np.nan` or `#!py \"Did not load\"`.\n    \"\"\"\n    sizes: Dict[str, list] = {\n        \"table\": list(),\n        \"type\": list(),\n        \"rows\": list(),\n        \"cols\": list(),\n    }\n    rows: Union[str, int, float]\n    cols: Union[str, int, float]\n    for tbl, typ in [\n        (\n            table.rsplit(\"_\", 1)\n            if \"_\" in table and table.endswith((\"acm\", \"arc\", \"prd\"))\n            else (table, \"\")\n        )\n        for table in tables\n    ]:\n        try:\n            tmp: psDataFrame = eval(\n                f\"{tbl}{f'_{typ}' if typ!='' else ''}\",\n                globals() if scope is None else scope,\n            )\n            rows, cols = get_dims(tmp, use_names=False, use_comma=use_comma)\n        except Exception:\n            if use_comma:\n                rows = cols = \"Did not load\"\n            else:\n                rows = cols = np.nan\n        sizes[\"table\"].append(tbl)\n        sizes[\"type\"].append(typ)\n        sizes[\"rows\"].append(rows)\n        sizes[\"cols\"].append(cols)\n    return pdDataFrame(sizes)\n</code></pre>"},{"location":"code/io/","title":"IO","text":""},{"location":"code/io/#toolbox_pyspark.io","title":"toolbox_pyspark.io","text":"<p>Summary</p> <p>The <code>io</code> module is used for reading and writing tables to/from directories.</p>"},{"location":"code/io/#toolbox_pyspark.io.read_from_path","title":"read_from_path","text":"<pre><code>read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Read an object from a given <code>path</code> in to memory as a <code>pyspark</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to read in.</p> required <code>path</code> <code>str</code> <p>The path from which it will be read.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The Spark session to use for the reading.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format of the object at location <code>path</code>. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>read_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. Like, for example:</p> <ul> <li>If the object is a CSV, you may want to define that it has a header row: <code>{\"header\": \"true\"}</code>.</li> <li>If the object is a Delta table, you may want to query a specific version: <code>{versionOf\": \"0\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The loaded dataframe.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         \"c\": [1, 1, 1, 1],\n...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...     }\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write data\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> Terminal<pre><code>[\"table.csv\", \"table.parquet\"]\n</code></pre> </p> <p>Example 1: Read CSV<pre><code>&gt;&gt;&gt; df_csv = read_from_path(\n...     name=\"table.csv\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"csv\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_csv.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully read CSV.</p> <p>Example 2: Read Parquet<pre><code>&gt;&gt;&gt; df_parquet = read_from_path(\n...     name=\"table.parquet\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"parquet\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_parquet.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully read Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Read an object from a given `path` in to memory as a `pyspark` dataframe.\n\n    Params:\n        name (str):\n            The name of the table to read in.\n        path (str):\n            The path from which it will be read.\n        spark_session (SparkSession):\n            The Spark session to use for the reading.\n        data_format (Optional[str], optional):\n            The format of the object at location `path`.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        read_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            Like, for example:&lt;br&gt;\n\n            - If the object is a CSV, you may want to define that it has a header row: `#!py {\"header\": \"true\"}`.\n            - If the object is a Delta table, you may want to query a specific version: `#!py {versionOf\": \"0\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The loaded dataframe.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4],\n        ...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         \"c\": [1, 1, 1, 1],\n        ...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Write data\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"table.csv\", \"table.parquet\"]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Read CSV\"}\n        &gt;&gt;&gt; df_csv = read_from_path(\n        ...     name=\"table.csv\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"csv\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df_csv.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully read CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Read Parquet\"}\n        &gt;&gt;&gt; df_parquet = read_from_path(\n        ...     name=\"table.parquet\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"parquet\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df_parquet.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully read Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    data_format: str = data_format or \"parquet\"\n    reader: DataFrameReader = spark_session.read.format(data_format)\n    if read_options:\n        reader.options(**read_options)\n    return reader.load(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.write_to_path","title":"write_to_path","text":"<pre><code>write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>For a given <code>table</code>, write it out to a specified <code>path</code> with name <code>name</code> and format <code>format</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame</code> <p>The table to be written. Must be a valid <code>pyspark</code> DataFrame (<code>pyspark.sql.DataFrame</code>).</p> required <code>name</code> <code>str</code> <p>The name of the table where it will be written.</p> required <code>path</code> <code>str</code> <p>The path location for where to save the table.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format that the <code>table</code> will be written to. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>write_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. Like, for example:</p> <ul> <li>If you are writing to a Delta object, and wanted to overwrite the schema: <code>{\"overwriteSchema\": \"true\"}</code>.</li> <li>If you\"re writing to a CSV file, and wanted to specify the header row: <code>{\"header\": \"true\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Note <p>You know that this function is successful if the table exists at the specified location, and there are no errors thrown.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Write to CSV<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     mode=\"overwrite\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully written to CSV.</p> <p>Example 2: Write to Parquet<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     mode=\"overwrite\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully written to Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `table`, write it out to a specified `path` with name `name` and format `format`.\n\n    Params:\n        table (psDataFrame):\n            The table to be written. Must be a valid `pyspark` DataFrame ([`pyspark.sql.DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)).\n        name (str):\n            The name of the table where it will be written.\n        path (str):\n            The path location for where to save the table.\n        data_format (Optional[str], optional):\n            The format that the `table` will be written to.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        write_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            Like, for example:\n\n            - If you are writing to a Delta object, and wanted to overwrite the schema: `#!py {\"overwriteSchema\": \"true\"}`.\n            - If you\"re writing to a CSV file, and wanted to specify the header row: `#!py {\"header\": \"true\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n        partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ tip \"Note\"\n        You know that this function is successful if the table exists at the specified location, and there are no errors thrown.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Write to CSV\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     mode=\"overwrite\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully written to CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Write to Parquet\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     mode=\"overwrite\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully written to Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    write_options: str_dict = write_options or dict()\n    data_format: str = data_format or \"parquet\"\n    writer: DataFrameWriter = table.write.mode(mode).format(data_format)\n    if write_options:\n        writer.options(**write_options)\n    if partition_cols is not None:\n        partition_cols = (\n            [partition_cols] if is_type(partition_cols, str) else partition_cols\n        )\n        writer = writer.partitionBy(list(partition_cols))\n    writer.save(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.transfer_table","title":"transfer_table","text":"<pre><code>transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[\n        str_collection\n    ] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Copy a table from one location to another.</p> Details <p>This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.</p> <p>Parameters:</p> Name Type Description Default <code>spark_session</code> <code>SparkSession</code> <p>The spark session to use for the transfer. Necessary in order to instantiate the reading process.</p> required <code>from_table_path</code> <code>str</code> <p>The path from which the table will be read.</p> required <code>from_table_name</code> <code>str</code> <p>The name of the table to be read.</p> required <code>from_table_format</code> <code>str</code> <p>The format of the data at the reading location.</p> required <code>to_table_path</code> <code>str</code> <p>The location where to save the table to.</p> required <code>to_table_name</code> <code>str</code> <p>The name of the table where it will be saved.</p> required <code>to_table_format</code> <code>str</code> <p>The format of the saved table.</p> required <code>from_table_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>to_table_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Note <p>You know that this function is successful if the table exists at the specified location, and there are no errors thrown.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         \"c\": [1, 1, 1 1],\n...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...     }\n... )\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> Terminal<pre><code>[\"table.csv\", \"table.parquet\"]\n</code></pre> </p> <p>Example 1: Transfer CSV<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.csv\",\n...     to_table_format=\"csv\",\n...     from_table_options={\"header\": \"true\"},\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./other\",\n...     data_format=\"csv\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred CSV to CSV.</p> <p>Example 2: Transfer Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.parquet\",\n...     from_table_format=\"parquet\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred Parquet to Parquet.</p> <p>Example 3: Transfer CSV to Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred CSV to Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Copy a table from one location to another.\n\n    ???+ abstract \"Details\"\n        This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.\n\n    Params:\n        spark_session (SparkSession):\n            The spark session to use for the transfer. Necessary in order to instantiate the reading process.\n        from_table_path (str):\n            The path from which the table will be read.\n        from_table_name (str):\n            The name of the table to be read.\n        from_table_format (str):\n            The format of the data at the reading location.\n        to_table_path (str):\n            The location where to save the table to.\n        to_table_name (str):\n            The name of the table where it will be saved.\n        to_table_format (str):\n            The format of the saved table.\n        from_table_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        to_table_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ tip \"Note\"\n        You know that this function is successful if the table exists at the specified location, and there are no errors thrown.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4],\n        ...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         \"c\": [1, 1, 1 1],\n        ...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"table.csv\", \"table.parquet\"]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Transfer CSV\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.csv\",\n        ...     to_table_format=\"csv\",\n        ...     from_table_options={\"header\": \"true\"},\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./other\",\n        ...     data_format=\"csv\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred CSV to CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Transfer Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.parquet\",\n        ...     from_table_format=\"parquet\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred Parquet to Parquet.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Transfer CSV to Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred CSV to Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    from_table_options: str_dict = from_table_options or dict()\n    to_table_options: str_dict = to_table_options or dict()\n    from_table: psDataFrame = read_from_path(\n        name=from_table_name,\n        path=from_table_path,\n        spark_session=spark_session,\n        data_format=from_table_format,\n        read_options=from_table_options,\n    )\n    write_to_path(\n        table=from_table,\n        name=to_table_name,\n        path=to_table_path,\n        data_format=to_table_format,\n        mode=to_table_mode,\n        write_options=to_table_options,\n        partition_cols=to_table_partition_cols,\n    )\n</code></pre>"},{"location":"code/keys/","title":"Keys","text":""},{"location":"code/keys/#toolbox_pyspark.keys","title":"toolbox_pyspark.keys","text":"<p>Summary</p> <p>The <code>keys</code> module is used for creating new columns to act as keys (primary and foreign), to be used for joins with other tables, or to create relationships within downstream applications, like PowerBI.</p>"},{"location":"code/keys/#toolbox_pyspark.keys.add_key_from_columns","title":"add_key_from_columns","text":"<pre><code>add_key_from_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    join_character: Optional[str] = \"_\",\n    key_name: Optional[str] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Using a list of column names, add a new column which is a combination of all of them.</p> Details <p>This is a combine key, and is especially important because PowerBI cannot handle joins on multiple columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The table to be updated.</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The columns to be combined. If <code>columns</code> is a <code>str</code>, then it will be coerced to a single-element list: <code>[columns]</code>.</p> required <code>join_character</code> <code>Optional[str]</code> <p>The character to use to combine the columns together. Defaults to <code>\"_\"</code>.</p> <code>'_'</code> <code>key_name</code> <code>Optional[str]</code> <p>The name of the column to be given to the key. If not provided, it will form as the capitalised string of all the other column names, prefixed with <code>key_</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated <code>dataframe</code>.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"b\"])\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---------+\n| a | b | c | d | key_A_B |\n+---+---+---+---+---------+\n| 1 | a | 1 | 2 | 1_a     |\n| 2 | b | 1 | 2 | 2_b     |\n| 3 | c | 1 | 2 | 3_c     |\n| 4 | d | 1 | 2 | 4_d     |\n+---+---+---+---+---------+\n</code></pre> <p>Conclusion: Successfully added new key column to DataFrame.</p> <p>Example 2: Single column<pre><code>&gt;&gt;&gt; new_df = add_key_from_columns(df, \"a\")\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+-------+\n| a | b | c | d | key_A |\n+---+---+---+---+-------+\n| 1 | a | 1 | 2 | 1     |\n| 2 | b | 1 | 2 | 2     |\n| 3 | c | 1 | 2 | 3     |\n| 4 | d | 1 | 2 | 4     |\n+---+---+---+---+-------+\n</code></pre> <p>Conclusion: Successfully added new key column to DataFrame.</p> <p>Example 3: New name<pre><code>&gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"b\"], \"new_key\")\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---------+\n| a | b | c | d | new_key |\n+---+---+---+---+---------+\n| 1 | a | 1 | 2 | 1_a     |\n| 2 | b | 1 | 2 | 2_b     |\n| 3 | c | 1 | 2 | 3_c     |\n| 4 | d | 1 | 2 | 4_d     |\n+---+---+---+---+---------+\n</code></pre> <p>Conclusion: Successfully added new key column to DataFrame.</p> <p>Example 4: Raise error<pre><code>&gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"x\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"x\"] do not exist in \"dataframe\". Try one of: [\"a\", \"b\", \"c\", \"d\"].\n</code></pre> <p>Conclusion: Invalid column selection.</p> Source code in <code>src/toolbox_pyspark/keys.py</code> <pre><code>@typechecked\ndef add_key_from_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    join_character: Optional[str] = \"_\",\n    key_name: Optional[str] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Using a list of column names, add a new column which is a combination of all of them.\n\n    ???+ abstract \"Details\"\n        This is a combine key, and is especially important because PowerBI cannot handle joins on multiple columns.\n\n    Params:\n        dataframe (psDataFrame):\n            The table to be updated.\n        columns (Union[str, str_collection]):\n            The columns to be combined.&lt;br&gt;\n            If `columns` is a `#!py str`, then it will be coerced to a single-element list: `#!py [columns]`.\n        join_character (Optional[str], optional):\n            The character to use to combine the columns together.&lt;br&gt;\n            Defaults to `#!py \"_\"`.\n        key_name (Optional[str], optional):\n            The name of the column to be given to the key.\n            If not provided, it will form as the capitalised string of all the other column names, prefixed with `key_`.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If any of the `columns` do not exist within `dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated `dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"b\"])\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---------+\n        | a | b | c | d | key_A_B |\n        +---+---+---+---+---------+\n        | 1 | a | 1 | 2 | 1_a     |\n        | 2 | b | 1 | 2 | 2_b     |\n        | 3 | c | 1 | 2 | 3_c     |\n        | 4 | d | 1 | 2 | 4_d     |\n        +---+---+---+---+---------+\n        ```\n        !!! success \"Conclusion: Successfully added new key column to DataFrame.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Single column\"}\n        &gt;&gt;&gt; new_df = add_key_from_columns(df, \"a\")\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+-------+\n        | a | b | c | d | key_A |\n        +---+---+---+---+-------+\n        | 1 | a | 1 | 2 | 1     |\n        | 2 | b | 1 | 2 | 2     |\n        | 3 | c | 1 | 2 | 3     |\n        | 4 | d | 1 | 2 | 4     |\n        +---+---+---+---+-------+\n        ```\n        !!! success \"Conclusion: Successfully added new key column to DataFrame.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: New name\"}\n        &gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"b\"], \"new_key\")\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---------+\n        | a | b | c | d | new_key |\n        +---+---+---+---+---------+\n        | 1 | a | 1 | 2 | 1_a     |\n        | 2 | b | 1 | 2 | 2_b     |\n        | 3 | c | 1 | 2 | 3_c     |\n        | 4 | d | 1 | 2 | 4_d     |\n        +---+---+---+---+---------+\n        ```\n        !!! success \"Conclusion: Successfully added new key column to DataFrame.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Raise error\"}\n        &gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"x\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"x\"] do not exist in \"dataframe\". Try one of: [\"a\", \"b\", \"c\", \"d\"].\n        ```\n        !!! failure \"Conclusion: Invalid column selection.\"\n        &lt;/div&gt;\n    \"\"\"\n    columns = [columns] if isinstance(columns, str) else columns\n    assert_columns_exists(dataframe, columns)\n    join_character = join_character or \"\"\n    key_name = key_name or f\"key_{'_'.join([col.upper() for col in columns])}\"\n    return dataframe.withColumn(\n        key_name,\n        F.concat_ws(join_character, *columns),\n    )\n</code></pre>"},{"location":"code/keys/#toolbox_pyspark.keys.add_keys_from_columns","title":"add_keys_from_columns","text":"<pre><code>add_keys_from_columns(\n    dataframe: psDataFrame,\n    collection_of_columns: Union[\n        tuple[Union[str, str_collection], ...],\n        list[Union[str, str_collection]],\n        dict[str, Union[str, str_collection]],\n    ],\n    join_character: Optional[str] = \"_\",\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Add multiple new keys, each of which are collections of other columns.</p> Details <p>There are a few reasons why this functionality would be needed:</p> <ol> <li>When you wanted to create a new single column to act as a combine key, derived from multiple other columns.</li> <li>When you're interacting with PowerBI, it will only allow you to create relationships on one single column, not a combination of multiple columns.</li> <li>When you're joining multiple tables together, each of them join on a different combination of different columns, and you want to make your <code>pyspark</code> joins cleaner, instead of using <code>list</code>'s of multiple <code>F.col(...)</code> equality checks.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The table to be updated.</p> required <code>collection_of_columns</code> <code>Union[tuple[Union[str, str_collection], ...], [Union[str, str_collection]], dict[str, Union[str, str_collection]]]</code> <p>The collection of columns to be combined together. If it is a <code>list</code> of <code>list</code>'s of <code>str</code>'s (or similar), then the key name will be derived from a concatenation of the original columns names. If it's a <code>dict</code> where the values are a <code>list</code> of <code>str</code>'s (or similar), then the column name for the new key is taken from the key of the dictionary.</p> required <code>join_character</code> <code>Optional[str]</code> <p>The character to use to combine the columns together. Defaults to <code>\"_\"</code>.</p> <code>'_'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated <code>dataframe</code>.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; new_df = add_keys_from_columns(df, [[\"a\", \"b\"], [\"b\", \"c\"]])\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---------+---------+\n| a | b | c | d | key_A_B | key_B_C |\n+---+---+---+---+---------+---------+\n| 1 | a | 1 | 2 | 1_a     | a_1     |\n| 2 | b | 1 | 2 | 2_b     | b_1     |\n| 3 | c | 1 | 2 | 3_c     | c_1     |\n| 4 | d | 1 | 2 | 4_d     | d_1     |\n+---+---+---+---+---------+---------+\n</code></pre> <p>Conclusion: Successfully added two new key columns to DataFrame.</p> <p>Example 2: Created from dict<pre><code>&gt;&gt;&gt; new_df = add_keys_from_columns(df, {\"first\": [\"a\", \"b\"], \"second\": [\"b\", \"c\"]])\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+-------+--------+\n| a | b | c | d | first | second |\n+---+---+---+---+-------+--------+\n| 1 | a | 1 | 2 | 1_a   | a_1    |\n| 2 | b | 1 | 2 | 2_b   | b_1    |\n| 3 | c | 1 | 2 | 3_c   | c_1    |\n| 4 | d | 1 | 2 | 4_d   | d_1    |\n+---+---+---+---+-------+--------+\n</code></pre> <p>Conclusion: Successfully added two new key columns to DataFrame.</p> Source code in <code>src/toolbox_pyspark/keys.py</code> <pre><code>@typechecked\ndef add_keys_from_columns(\n    dataframe: psDataFrame,\n    collection_of_columns: Union[\n        tuple[Union[str, str_collection], ...],\n        list[Union[str, str_collection]],\n        dict[str, Union[str, str_collection]],\n    ],\n    join_character: Optional[str] = \"_\",\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Add multiple new keys, each of which are collections of other columns.\n\n    ???+ abstract \"Details\"\n        There are a few reasons why this functionality would be needed:\n\n        1. When you wanted to create a new single column to act as a combine key, derived from multiple other columns.\n        1. When you're interacting with PowerBI, it will only allow you to create relationships on one single column, not a combination of multiple columns.\n        1. When you're joining multiple tables together, each of them join on a different combination of different columns, and you want to make your `pyspark` joins cleaner, instead of using `#!py list`'s of multiple `#!py F.col(...)` equality checks.\n\n    Params:\n        dataframe (psDataFrame):\n            The table to be updated.\n        collection_of_columns (Union[tuple[Union[str, str_collection], ...], [Union[str, str_collection]], dict[str, Union[str, str_collection]]]):\n            The collection of columns to be combined together.&lt;br&gt;\n            If it is a `#!py list` of `#!py list`'s of `#!py str`'s (or similar), then the key name will be derived from a concatenation of the original columns names.&lt;br&gt;\n            If it's a `#!py dict` where the values are a `#!py list` of `#!py str`'s (or similar), then the column name for the new key is taken from the key of the dictionary.\n        join_character (Optional[str], optional):\n            The character to use to combine the columns together.&lt;br&gt;\n            Defaults to `#!py \"_\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If any of the `columns` do not exist within `dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated `dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; new_df = add_keys_from_columns(df, [[\"a\", \"b\"], [\"b\", \"c\"]])\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---------+---------+\n        | a | b | c | d | key_A_B | key_B_C |\n        +---+---+---+---+---------+---------+\n        | 1 | a | 1 | 2 | 1_a     | a_1     |\n        | 2 | b | 1 | 2 | 2_b     | b_1     |\n        | 3 | c | 1 | 2 | 3_c     | c_1     |\n        | 4 | d | 1 | 2 | 4_d     | d_1     |\n        +---+---+---+---+---------+---------+\n        ```\n        !!! success \"Conclusion: Successfully added two new key columns to DataFrame.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Created from dict\"}\n        &gt;&gt;&gt; new_df = add_keys_from_columns(df, {\"first\": [\"a\", \"b\"], \"second\": [\"b\", \"c\"]])\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+-------+--------+\n        | a | b | c | d | first | second |\n        +---+---+---+---+-------+--------+\n        | 1 | a | 1 | 2 | 1_a   | a_1    |\n        | 2 | b | 1 | 2 | 2_b   | b_1    |\n        | 3 | c | 1 | 2 | 3_c   | c_1    |\n        | 4 | d | 1 | 2 | 4_d   | d_1    |\n        +---+---+---+---+-------+--------+\n        ```\n        !!! success \"Conclusion: Successfully added two new key columns to DataFrame.\"\n        &lt;/div&gt;\n    \"\"\"\n    join_character = join_character or \"\"\n    if isinstance(collection_of_columns, dict):\n        for key_name, columns in collection_of_columns.items():\n            dataframe = add_key_from_columns(\n                dataframe, columns, join_character, key_name\n            )\n    elif isinstance(collection_of_columns, (tuple, list)):\n        for columns in collection_of_columns:\n            dataframe = add_key_from_columns(dataframe, columns, join_character)\n    return dataframe\n</code></pre>"},{"location":"code/scale/","title":"Scale","text":""},{"location":"code/scale/#toolbox_pyspark.scale","title":"toolbox_pyspark.scale","text":"<p>Summary</p> <p>The <code>scale</code> module is used for rounding a column (or columns) to a given rounding accuracy.</p>"},{"location":"code/scale/#toolbox_pyspark.scale.round_column","title":"round_column","text":"<pre><code>round_column(\n    dataframe: psDataFrame,\n    column: str,\n    scale: int = DEFAULT_DECIMAL_ACCURACY,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given <code>dataframe</code>, on a given <code>column</code> if the column data type is decimal (that is, one of: <code>[\"float\", \"double\", \"decimal\"]</code>), then round that column to a <code>scale</code> accuracy at a given number of decimal places.</p> Details <p>Realistically, under the hood, this function is super simple. It merely runs: Python<pre><code>dataframe = dataframe.withColumn(colName=column, col=F.round(col=column, scale=scale))\n</code></pre> This function merely adds some additional validation, and is enabled to run in a pyspark <code>.transform()</code> method. For more info, see: <code>pyspark.sql.DataFrame.transform</code></p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The <code>dataframe</code> to be transformed.</p> required <code>column</code> <code>str</code> <p>The desired column to be rounded.</p> required <code>scale</code> <code>int</code> <p>The required level of rounding for the column. If not provided explicitly, it will default to the global value <code>DEFAULT_DECIMAL_ACCURACY</code>; which is <code>10</code>. Defaults to <code>DEFAULT_DECIMAL_ACCURACY</code>.</p> <code>DEFAULT_DECIMAL_ACCURACY</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>TypeError</code> <p>If the given <code>column</code> is not one of the correct data types for rounding. It must be one of: <code>[\"float\", \"double\", \"decimal\"]</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed <code>dataframe</code> containing the column which has now been rounded.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F, types as T\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = (\n...     spark\n...     .createDataFrame(\n...         pd.DataFrame(\n...             {\n...                 \"a\": range(20),\n...                 \"b\": [f\"1.{'0'*val}1\" for val in range(20)],\n...                 \"c\": [f\"1.{'0'*val}6\" for val in range(20)],\n...             }\n...         )\n...     )\n...     .withColumns(\n...         {\n...             \"b\": F.col(\"b\").cast(T.DecimalType(21, 20)),\n...             \"c\": F.col(\"c\").cast(T.DecimalType(21, 20)),\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show(truncate=False)\n</code></pre> Terminal<pre><code>+---+----------------------+----------------------+\n|a  |b                     |c                     |\n+---+----------------------+----------------------+\n|0  |1.10000000000000000000|1.60000000000000000000|\n|1  |1.01000000000000000000|1.06000000000000000000|\n|2  |1.00100000000000000000|1.00600000000000000000|\n|3  |1.00010000000000000000|1.00060000000000000000|\n|4  |1.00001000000000000000|1.00006000000000000000|\n|5  |1.00000100000000000000|1.00000600000000000000|\n|6  |1.00000010000000000000|1.00000060000000000000|\n|7  |1.00000001000000000000|1.00000006000000000000|\n|8  |1.00000000100000000000|1.00000000600000000000|\n|9  |1.00000000010000000000|1.00000000060000000000|\n|10 |1.00000000001000000000|1.00000000006000000000|\n|11 |1.00000000000100000000|1.00000000000600000000|\n|12 |1.00000000000010000000|1.00000000000060000000|\n|13 |1.00000000000001000000|1.00000000000006000000|\n|14 |1.00000000000000100000|1.00000000000000600000|\n|15 |1.00000000000000010000|1.00000000000000060000|\n|16 |1.00000000000000001000|1.00000000000000006000|\n|17 |1.00000000000000000100|1.00000000000000000600|\n|18 |1.00000000000000000010|1.00000000000000000060|\n|19 |1.00000000000000000001|1.00000000000000000006|\n+---+----------------------+----------------------+\n</code></pre> </p> <p>Example 1: Round with defaults<pre><code>&gt;&gt;&gt; round_column(df, \"b\").show(truncate=False)\n</code></pre> Terminal<pre><code>+---+------------+----------------------+\n|a  |b           |c                     |\n+---+------------+----------------------+\n|0  |1.1000000000|1.60000000000000000000|\n|1  |1.0100000000|1.06000000000000000000|\n|2  |1.0010000000|1.00600000000000000000|\n|3  |1.0001000000|1.00060000000000000000|\n|4  |1.0000100000|1.00006000000000000000|\n|5  |1.0000010000|1.00000600000000000000|\n|6  |1.0000001000|1.00000060000000000000|\n|7  |1.0000000100|1.00000006000000000000|\n|8  |1.0000000010|1.00000000600000000000|\n|9  |1.0000000001|1.00000000060000000000|\n|10 |1.0000000000|1.00000000006000000000|\n|11 |1.0000000000|1.00000000000600000000|\n|12 |1.0000000000|1.00000000000060000000|\n|13 |1.0000000000|1.00000000000006000000|\n|14 |1.0000000000|1.00000000000000600000|\n|15 |1.0000000000|1.00000000000000060000|\n|16 |1.0000000000|1.00000000000000006000|\n|17 |1.0000000000|1.00000000000000000600|\n|18 |1.0000000000|1.00000000000000000060|\n|19 |1.0000000000|1.00000000000000000006|\n+---+------------+----------------------+\n</code></pre> <p>Conclusion: Successfully rounded column <code>b</code>.</p> <p>Example 2: Round to custom number<pre><code>&gt;&gt;&gt; round_column(df, \"c\", 5).show(truncate=False)\n</code></pre> Terminal<pre><code>+---+----------------------+-------+\n|a  |b                     |c      |\n+---+----------------------+-------+\n|0  |1.10000000000000000000|1.60000|\n|1  |1.01000000000000000000|1.06000|\n|2  |1.00100000000000000000|1.00600|\n|3  |1.00010000000000000000|1.00060|\n|4  |1.00001000000000000000|1.00006|\n|5  |1.00000100000000000000|1.00001|\n|6  |1.00000010000000000000|1.00000|\n|7  |1.00000001000000000000|1.00000|\n|8  |1.00000000100000000000|1.00000|\n|9  |1.00000000010000000000|1.00000|\n|10 |1.00000000001000000000|1.00000|\n|11 |1.00000000000100000000|1.00000|\n|12 |1.00000000000010000000|1.00000|\n|13 |1.00000000000001000000|1.00000|\n|14 |1.00000000000000100000|1.00000|\n|15 |1.00000000000000010000|1.00000|\n|16 |1.00000000000000001000|1.00000|\n|17 |1.00000000000000000100|1.00000|\n|18 |1.00000000000000000010|1.00000|\n|19 |1.00000000000000000001|1.00000|\n+---+----------------------+-------+\n</code></pre> <p>Conclusion: Successfully rounded column <code>b</code> to 5 decimal points.</p> <p>Example 3: Raise error<pre><code>&gt;&gt;&gt; round_column(df, \"a\").show(truncate=False)\n</code></pre> Terminal<pre><code>TypeError: Column is not the correct type. Please check.\nFor column 'a', the type is 'bigint'.\nIn order to round it, it needs to be one of: '[\"float\", \"double\", \"decimal\"]'.\n</code></pre> <p>Conclusion: Cannot round a column <code>a</code>.</p> Source code in <code>src/toolbox_pyspark/scale.py</code> <pre><code>@typechecked\ndef round_column(\n    dataframe: psDataFrame,\n    column: str,\n    scale: int = DEFAULT_DECIMAL_ACCURACY,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `dataframe`, on a given `column` if the column data type is decimal (that is, one of: `#!py [\"float\", \"double\", \"decimal\"]`), then round that column to a `scale` accuracy at a given number of decimal places.\n\n    ???+ abstract \"Details\"\n        Realistically, under the hood, this function is super simple. It merely runs:\n        ```{.py .python linenums=\"1\" title=\"Python\"}\n        dataframe = dataframe.withColumn(colName=column, col=F.round(col=column, scale=scale))\n        ```\n        This function merely adds some additional validation, and is enabled to run in a pyspark `.transform()` method.\n        For more info, see: [`pyspark.sql.DataFrame.transform`](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.transform.html)\n\n    Params:\n        dataframe (psDataFrame):\n            The `dataframe` to be transformed.\n        column (str):\n            The desired column to be rounded.\n        scale (int, optional):\n            The required level of rounding for the column.&lt;br&gt;\n            If not provided explicitly, it will default to the global value `#!py DEFAULT_DECIMAL_ACCURACY`; which is `#!py 10`.&lt;br&gt;\n            Defaults to `#!py DEFAULT_DECIMAL_ACCURACY`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        TypeError:\n            If the given `column` is not one of the correct data types for rounding. It must be one of: `#!py [\"float\", \"double\", \"decimal\"]`.\n\n    Returns:\n        (psDataFrame):\n            The transformed `dataframe` containing the column which has now been rounded.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F, types as T\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = (\n        ...     spark\n        ...     .createDataFrame(\n        ...         pd.DataFrame(\n        ...             {\n        ...                 \"a\": range(20),\n        ...                 \"b\": [f\"1.{'0'*val}1\" for val in range(20)],\n        ...                 \"c\": [f\"1.{'0'*val}6\" for val in range(20)],\n        ...             }\n        ...         )\n        ...     )\n        ...     .withColumns(\n        ...         {\n        ...             \"b\": F.col(\"b\").cast(T.DecimalType(21, 20)),\n        ...             \"c\": F.col(\"c\").cast(T.DecimalType(21, 20)),\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----------------------+----------------------+\n        |a  |b                     |c                     |\n        +---+----------------------+----------------------+\n        |0  |1.10000000000000000000|1.60000000000000000000|\n        |1  |1.01000000000000000000|1.06000000000000000000|\n        |2  |1.00100000000000000000|1.00600000000000000000|\n        |3  |1.00010000000000000000|1.00060000000000000000|\n        |4  |1.00001000000000000000|1.00006000000000000000|\n        |5  |1.00000100000000000000|1.00000600000000000000|\n        |6  |1.00000010000000000000|1.00000060000000000000|\n        |7  |1.00000001000000000000|1.00000006000000000000|\n        |8  |1.00000000100000000000|1.00000000600000000000|\n        |9  |1.00000000010000000000|1.00000000060000000000|\n        |10 |1.00000000001000000000|1.00000000006000000000|\n        |11 |1.00000000000100000000|1.00000000000600000000|\n        |12 |1.00000000000010000000|1.00000000000060000000|\n        |13 |1.00000000000001000000|1.00000000000006000000|\n        |14 |1.00000000000000100000|1.00000000000000600000|\n        |15 |1.00000000000000010000|1.00000000000000060000|\n        |16 |1.00000000000000001000|1.00000000000000006000|\n        |17 |1.00000000000000000100|1.00000000000000000600|\n        |18 |1.00000000000000000010|1.00000000000000000060|\n        |19 |1.00000000000000000001|1.00000000000000000006|\n        +---+----------------------+----------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Round with defaults\"}\n        &gt;&gt;&gt; round_column(df, \"b\").show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+------------+----------------------+\n        |a  |b           |c                     |\n        +---+------------+----------------------+\n        |0  |1.1000000000|1.60000000000000000000|\n        |1  |1.0100000000|1.06000000000000000000|\n        |2  |1.0010000000|1.00600000000000000000|\n        |3  |1.0001000000|1.00060000000000000000|\n        |4  |1.0000100000|1.00006000000000000000|\n        |5  |1.0000010000|1.00000600000000000000|\n        |6  |1.0000001000|1.00000060000000000000|\n        |7  |1.0000000100|1.00000006000000000000|\n        |8  |1.0000000010|1.00000000600000000000|\n        |9  |1.0000000001|1.00000000060000000000|\n        |10 |1.0000000000|1.00000000006000000000|\n        |11 |1.0000000000|1.00000000000600000000|\n        |12 |1.0000000000|1.00000000000060000000|\n        |13 |1.0000000000|1.00000000000006000000|\n        |14 |1.0000000000|1.00000000000000600000|\n        |15 |1.0000000000|1.00000000000000060000|\n        |16 |1.0000000000|1.00000000000000006000|\n        |17 |1.0000000000|1.00000000000000000600|\n        |18 |1.0000000000|1.00000000000000000060|\n        |19 |1.0000000000|1.00000000000000000006|\n        +---+------------+----------------------+\n        ```\n        !!! success \"Conclusion: Successfully rounded column `b`.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Round to custom number\"}\n        &gt;&gt;&gt; round_column(df, \"c\", 5).show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----------------------+-------+\n        |a  |b                     |c      |\n        +---+----------------------+-------+\n        |0  |1.10000000000000000000|1.60000|\n        |1  |1.01000000000000000000|1.06000|\n        |2  |1.00100000000000000000|1.00600|\n        |3  |1.00010000000000000000|1.00060|\n        |4  |1.00001000000000000000|1.00006|\n        |5  |1.00000100000000000000|1.00001|\n        |6  |1.00000010000000000000|1.00000|\n        |7  |1.00000001000000000000|1.00000|\n        |8  |1.00000000100000000000|1.00000|\n        |9  |1.00000000010000000000|1.00000|\n        |10 |1.00000000001000000000|1.00000|\n        |11 |1.00000000000100000000|1.00000|\n        |12 |1.00000000000010000000|1.00000|\n        |13 |1.00000000000001000000|1.00000|\n        |14 |1.00000000000000100000|1.00000|\n        |15 |1.00000000000000010000|1.00000|\n        |16 |1.00000000000000001000|1.00000|\n        |17 |1.00000000000000000100|1.00000|\n        |18 |1.00000000000000000010|1.00000|\n        |19 |1.00000000000000000001|1.00000|\n        +---+----------------------+-------+\n        ```\n        !!! success \"Conclusion: Successfully rounded column `b` to 5 decimal points.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Raise error\"}\n        &gt;&gt;&gt; round_column(df, \"a\").show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        TypeError: Column is not the correct type. Please check.\n        For column 'a', the type is 'bigint'.\n        In order to round it, it needs to be one of: '[\"float\", \"double\", \"decimal\"]'.\n        ```\n        !!! failure \"Conclusion: Cannot round a column `a`.\"\n        &lt;/div&gt;\n    \"\"\"\n    assert_column_exists(dataframe, column)\n    col_type: str = [\n        typ.split(\"(\")[0] for col, typ in dataframe.dtypes if col == column\n    ][0]\n    if col_type not in VALID_TYPES:\n        raise TypeError(\n            f\"Column is not the correct type. Please check.\\n\"\n            f\"For column '{column}', the type is '{col_type}'.\\n\"\n            f\"In order to round it, it needs to be one of: '{VALID_TYPES}'.\"\n        )\n    return dataframe.withColumn(colName=column, col=F.round(col=column, scale=scale))\n</code></pre>"},{"location":"code/scale/#toolbox_pyspark.scale.round_columns","title":"round_columns","text":"<pre><code>round_columns(\n    dataframe: psDataFrame,\n    columns: Optional[\n        Union[str, str_collection]\n    ] = \"all_float\",\n    scale: int = DEFAULT_DECIMAL_ACCURACY,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given <code>dataframe</code>, on a set of <code>columns</code> if the column data type is decimal (that is, one of: <code>[\"float\", \"double\", \"decimal\"]</code>), then round that column to a <code>scale</code> accuracy at a given number of decimal places.</p> Details <p>Realistically, under the hood, this function is super simple. It merely runs: Python<pre><code>dataframe = dataframe.withColumns({col: F.round(col, scale) for col in columns})\n</code></pre> This function merely adds some additional validation, and is enabled to run in a pyspark <code>.transform()</code> method. For more info, see: <code>pyspark.sql.DataFrame.transform</code></p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The <code>dataframe</code> to be transformed.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>The desired column to be rounded. If no value is parsed, or is the value <code>None</code>, or one of <code>[\"all\", \"all_float\"]</code>, then it will default to all numeric decimal columns on the <code>dataframe</code>. If the value is a <code>str</code>, then it will be coerced to a single-element list, like: <code>[columns]</code>. Defaults to <code>\"all_float\"</code>.</p> <code>'all_float'</code> <code>scale</code> <code>int</code> <p>The required level of rounding for the column. If not provided explicitly, it will default to the global value <code>DEFAULT_DECIMAL_ACCURACY</code>; which is <code>10</code>. Defaults to <code>DEFAULT_DECIMAL_ACCURACY</code>.</p> <code>DEFAULT_DECIMAL_ACCURACY</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>TypeError</code> <p>If any of the given <code>columns</code> are not one of the correct data types for rounding. They must be one of: <code>[\"float\", \"double\", \"decimal\"]</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed <code>dataframe</code> containing the column which has now been rounded.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F, types as T\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = (\n...     spark\n...     .createDataFrame(\n...         pd.DataFrame(\n...             {\n...                 \"a\": range(20),\n...                 \"b\": [f\"1.{'0'*val}1\" for val in range(20)],\n...                 \"c\": [f\"1.{'0'*val}6\" for val in range(20)],\n...             }\n...         )\n...     )\n...     .withColumns(\n...         {\n...             \"b\": F.col(\"b\").cast(T.DecimalType(21, 20)),\n...             \"c\": F.col(\"c\").cast(T.DecimalType(21, 20)),\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show(truncate=False)\n</code></pre> Terminal<pre><code>+---+----------------------+----------------------+\n|a  |b                     |c                     |\n+---+----------------------+----------------------+\n|0  |1.10000000000000000000|1.60000000000000000000|\n|1  |1.01000000000000000000|1.06000000000000000000|\n|2  |1.00100000000000000000|1.00600000000000000000|\n|3  |1.00010000000000000000|1.00060000000000000000|\n|4  |1.00001000000000000000|1.00006000000000000000|\n|5  |1.00000100000000000000|1.00000600000000000000|\n|6  |1.00000010000000000000|1.00000060000000000000|\n|7  |1.00000001000000000000|1.00000006000000000000|\n|8  |1.00000000100000000000|1.00000000600000000000|\n|9  |1.00000000010000000000|1.00000000060000000000|\n|10 |1.00000000001000000000|1.00000000006000000000|\n|11 |1.00000000000100000000|1.00000000000600000000|\n|12 |1.00000000000010000000|1.00000000000060000000|\n|13 |1.00000000000001000000|1.00000000000006000000|\n|14 |1.00000000000000100000|1.00000000000000600000|\n|15 |1.00000000000000010000|1.00000000000000060000|\n|16 |1.00000000000000001000|1.00000000000000006000|\n|17 |1.00000000000000000100|1.00000000000000000600|\n|18 |1.00000000000000000010|1.00000000000000000060|\n|19 |1.00000000000000000001|1.00000000000000000006|\n+---+----------------------+----------------------+\n</code></pre> </p> <p>Example 1: Round with defaults<pre><code>&gt;&gt;&gt; round_columns(df).show(truncate=False)\n</code></pre> Terminal<pre><code>+---+------------+------------+\n|  a|           b|           c|\n+---+------------+------------+\n|  0|1.1000000000|1.6000000000|\n|  1|1.0100000000|1.0600000000|\n|  2|1.0010000000|1.0060000000|\n|  3|1.0001000000|1.0006000000|\n|  4|1.0000100000|1.0000600000|\n|  5|1.0000010000|1.0000060000|\n|  6|1.0000001000|1.0000006000|\n|  7|1.0000000100|1.0000000600|\n|  8|1.0000000010|1.0000000060|\n|  9|1.0000000001|1.0000000006|\n| 10|1.0000000000|1.0000000001|\n| 11|1.0000000000|1.0000000000|\n| 12|1.0000000000|1.0000000000|\n| 13|1.0000000000|1.0000000000|\n| 14|1.0000000000|1.0000000000|\n| 15|1.0000000000|1.0000000000|\n| 16|1.0000000000|1.0000000000|\n| 17|1.0000000000|1.0000000000|\n| 18|1.0000000000|1.0000000000|\n| 19|1.0000000000|1.0000000000|\n+---+------------+------------+\n</code></pre> </p> <p>Example 2: Round to custom number<pre><code>&gt;&gt;&gt; round_columns(df, \"c\", 5).show(truncate=False)\n</code></pre> Terminal<pre><code>+---+----------------------+-------+\n|a  |b                     |c      |\n+---+----------------------+-------+\n|0  |1.10000000000000000000|1.60000|\n|1  |1.01000000000000000000|1.06000|\n|2  |1.00100000000000000000|1.00600|\n|3  |1.00010000000000000000|1.00060|\n|4  |1.00001000000000000000|1.00006|\n|5  |1.00000100000000000000|1.00001|\n|6  |1.00000010000000000000|1.00000|\n|7  |1.00000001000000000000|1.00000|\n|8  |1.00000000100000000000|1.00000|\n|9  |1.00000000010000000000|1.00000|\n|10 |1.00000000001000000000|1.00000|\n|11 |1.00000000000100000000|1.00000|\n|12 |1.00000000000010000000|1.00000|\n|13 |1.00000000000001000000|1.00000|\n|14 |1.00000000000000100000|1.00000|\n|15 |1.00000000000000010000|1.00000|\n|16 |1.00000000000000001000|1.00000|\n|17 |1.00000000000000000100|1.00000|\n|18 |1.00000000000000000010|1.00000|\n|19 |1.00000000000000000001|1.00000|\n+---+----------------------+-------+\n</code></pre> </p> <p>Example 3: Raise error<pre><code>&gt;&gt;&gt; round_columns(df, [\"a\", \"b\"]).show(truncate=False)\n</code></pre> Terminal<pre><code>TypeError: Columns are not the correct types. Please check.\nThese columns are invalid: '[(\"a\", \"bigint\")]'.\nIn order to round them, they need to be one of: '[\"float\", \"double\", \"decimal\"]'.\n</code></pre> </p> Source code in <code>src/toolbox_pyspark/scale.py</code> <pre><code>@typechecked\ndef round_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = \"all_float\",\n    scale: int = DEFAULT_DECIMAL_ACCURACY,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `dataframe`, on a set of `columns` if the column data type is decimal (that is, one of: `#!py [\"float\", \"double\", \"decimal\"]`), then round that column to a `scale` accuracy at a given number of decimal places.\n\n    ???+ abstract \"Details\"\n        Realistically, under the hood, this function is super simple. It merely runs:\n        ```{.py .python linenums=\"1\" title=\"Python\"}\n        dataframe = dataframe.withColumns({col: F.round(col, scale) for col in columns})\n        ```\n        This function merely adds some additional validation, and is enabled to run in a pyspark `.transform()` method.\n        For more info, see: [`pyspark.sql.DataFrame.transform`](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.transform.html)\n\n    Params:\n        dataframe (psDataFrame):\n            The `dataframe` to be transformed.\n        columns (Optional[Union[str, str_collection]], optional):\n            The desired column to be rounded.&lt;br&gt;\n            If no value is parsed, or is the value `#!py None`, or one of `#!py [\"all\", \"all_float\"]`, then it will default to all numeric decimal columns on the `dataframe`.&lt;br&gt;\n            If the value is a `#!py str`, then it will be coerced to a single-element list, like: `#!py [columns]`.&lt;br&gt;\n            Defaults to `#!py \"all_float\"`.\n        scale (int, optional):\n            The required level of rounding for the column.&lt;br&gt;\n            If not provided explicitly, it will default to the global value `#!py DEFAULT_DECIMAL_ACCURACY`; which is `#!py 10`.&lt;br&gt;\n            Defaults to `#!py DEFAULT_DECIMAL_ACCURACY`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        TypeError:\n            If any of the given `columns` are not one of the correct data types for rounding. They must be one of: `#!py [\"float\", \"double\", \"decimal\"]`.\n\n    Returns:\n        (psDataFrame):\n            The transformed `dataframe` containing the column which has now been rounded.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F, types as T\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = (\n        ...     spark\n        ...     .createDataFrame(\n        ...         pd.DataFrame(\n        ...             {\n        ...                 \"a\": range(20),\n        ...                 \"b\": [f\"1.{'0'*val}1\" for val in range(20)],\n        ...                 \"c\": [f\"1.{'0'*val}6\" for val in range(20)],\n        ...             }\n        ...         )\n        ...     )\n        ...     .withColumns(\n        ...         {\n        ...             \"b\": F.col(\"b\").cast(T.DecimalType(21, 20)),\n        ...             \"c\": F.col(\"c\").cast(T.DecimalType(21, 20)),\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----------------------+----------------------+\n        |a  |b                     |c                     |\n        +---+----------------------+----------------------+\n        |0  |1.10000000000000000000|1.60000000000000000000|\n        |1  |1.01000000000000000000|1.06000000000000000000|\n        |2  |1.00100000000000000000|1.00600000000000000000|\n        |3  |1.00010000000000000000|1.00060000000000000000|\n        |4  |1.00001000000000000000|1.00006000000000000000|\n        |5  |1.00000100000000000000|1.00000600000000000000|\n        |6  |1.00000010000000000000|1.00000060000000000000|\n        |7  |1.00000001000000000000|1.00000006000000000000|\n        |8  |1.00000000100000000000|1.00000000600000000000|\n        |9  |1.00000000010000000000|1.00000000060000000000|\n        |10 |1.00000000001000000000|1.00000000006000000000|\n        |11 |1.00000000000100000000|1.00000000000600000000|\n        |12 |1.00000000000010000000|1.00000000000060000000|\n        |13 |1.00000000000001000000|1.00000000000006000000|\n        |14 |1.00000000000000100000|1.00000000000000600000|\n        |15 |1.00000000000000010000|1.00000000000000060000|\n        |16 |1.00000000000000001000|1.00000000000000006000|\n        |17 |1.00000000000000000100|1.00000000000000000600|\n        |18 |1.00000000000000000010|1.00000000000000000060|\n        |19 |1.00000000000000000001|1.00000000000000000006|\n        +---+----------------------+----------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Round with defaults\"}\n        &gt;&gt;&gt; round_columns(df).show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+------------+------------+\n        |  a|           b|           c|\n        +---+------------+------------+\n        |  0|1.1000000000|1.6000000000|\n        |  1|1.0100000000|1.0600000000|\n        |  2|1.0010000000|1.0060000000|\n        |  3|1.0001000000|1.0006000000|\n        |  4|1.0000100000|1.0000600000|\n        |  5|1.0000010000|1.0000060000|\n        |  6|1.0000001000|1.0000006000|\n        |  7|1.0000000100|1.0000000600|\n        |  8|1.0000000010|1.0000000060|\n        |  9|1.0000000001|1.0000000006|\n        | 10|1.0000000000|1.0000000001|\n        | 11|1.0000000000|1.0000000000|\n        | 12|1.0000000000|1.0000000000|\n        | 13|1.0000000000|1.0000000000|\n        | 14|1.0000000000|1.0000000000|\n        | 15|1.0000000000|1.0000000000|\n        | 16|1.0000000000|1.0000000000|\n        | 17|1.0000000000|1.0000000000|\n        | 18|1.0000000000|1.0000000000|\n        | 19|1.0000000000|1.0000000000|\n        +---+------------+------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Round to custom number\"}\n        &gt;&gt;&gt; round_columns(df, \"c\", 5).show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----------------------+-------+\n        |a  |b                     |c      |\n        +---+----------------------+-------+\n        |0  |1.10000000000000000000|1.60000|\n        |1  |1.01000000000000000000|1.06000|\n        |2  |1.00100000000000000000|1.00600|\n        |3  |1.00010000000000000000|1.00060|\n        |4  |1.00001000000000000000|1.00006|\n        |5  |1.00000100000000000000|1.00001|\n        |6  |1.00000010000000000000|1.00000|\n        |7  |1.00000001000000000000|1.00000|\n        |8  |1.00000000100000000000|1.00000|\n        |9  |1.00000000010000000000|1.00000|\n        |10 |1.00000000001000000000|1.00000|\n        |11 |1.00000000000100000000|1.00000|\n        |12 |1.00000000000010000000|1.00000|\n        |13 |1.00000000000001000000|1.00000|\n        |14 |1.00000000000000100000|1.00000|\n        |15 |1.00000000000000010000|1.00000|\n        |16 |1.00000000000000001000|1.00000|\n        |17 |1.00000000000000000100|1.00000|\n        |18 |1.00000000000000000010|1.00000|\n        |19 |1.00000000000000000001|1.00000|\n        +---+----------------------+-------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Raise error\"}\n        &gt;&gt;&gt; round_columns(df, [\"a\", \"b\"]).show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        TypeError: Columns are not the correct types. Please check.\n        These columns are invalid: '[(\"a\", \"bigint\")]'.\n        In order to round them, they need to be one of: '[\"float\", \"double\", \"decimal\"]'.\n        ```\n        &lt;/div&gt;\n    \"\"\"\n    if columns is None or columns in [\"all\", \"all_float\"]:\n        columns = [\n            col for col, typ in dataframe.dtypes if typ.split(\"(\")[0] in VALID_TYPES\n        ]\n    elif isinstance(columns, str):\n        columns = [columns]\n    assert_columns_exists(dataframe, columns)\n    invalid_cols: list[tuple[str, str]] = [\n        (col, typ.split(\"(\")[0])\n        for col, typ in dataframe.dtypes\n        if col in columns and typ.split(\"(\")[0] not in VALID_TYPES\n    ]\n    if len(invalid_cols) &gt; 0:\n        raise TypeError(\n            f\"Columns are not the correct types. Please check.\\n\"\n            f\"These columns are invalid: '{invalid_cols}'.\\n\"\n            f\"In order to round them, they need to be one of: '{VALID_TYPES}'.\"\n        )\n    return dataframe.withColumns({col: F.round(col, scale) for col in columns})\n</code></pre>"},{"location":"code/types/","title":"Types","text":""},{"location":"code/types/#toolbox_pyspark.types","title":"toolbox_pyspark.types","text":"<p>Summary</p> <p>The <code>types</code> module is used to get, check, and change a datafames column data types.</p>"},{"location":"code/types/#toolbox_pyspark.types.get_column_types","title":"get_column_types","text":"<pre><code>get_column_types(\n    dataframe: psDataFrame, output_type: str = \"psDataFrame\"\n) -&gt; Union[psDataFrame, pdDataFrame]\n</code></pre> <p>Summary</p> <p>This is a convenient function to return the data types from a given table as either a <code>pyspark.sql.DataFrame</code> or <code>pandas.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be checked.</p> required <code>output_type</code> <code>str</code> <p>How should the data be returned? As <code>pdDataFrame</code> or <code>psDataFrame</code>.</p> <p>For <code>pandas</code>, use one of:</p> <pre><code>[\n    \"pandas\", \"pandas.DataFrame\",\n    \"pd.df\",  \"pd.DataFrame\",\n    \"pddf\",   \"pdDataFrame\",\n    \"pd\",     \"pdDF\",\n]\n</code></pre> <p>For <code>pyspark</code> use one of:</p> <pre><code>[\n    \"pyspark\", \"spark.DataFrame\",\n    \"spark\",   \"pyspark.DataFrame\",\n    \"ps.df\",   \"ps.DataFrame\",\n    \"psdf\",    \"psDataFrame\",\n    \"ps\",      \"psDF\",\n]\n</code></pre> <p>Any other options are invalid. Defaults to <code>\"psDataFrame\"</code>.</p> <code>'psDataFrame'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If the given value parsed to <code>output_type</code> is not one of the given valid types.</p> <p>Returns:</p> Type Description <code>Union[DataFrame, DataFrame]</code> <p>The DataFrame where each row represents a column on the original <code>dataframe</code> object, and which has two columns:</p> <ol> <li>The column name from <code>dataframe</code>; and</li> <li>The data type for that column in <code>dataframe</code>.</li> </ol> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; print(df.dtypes)\n</code></pre> Terminal<pre><code>[\n    (\"a\", \"bigint\"),\n    (\"b\", \"string\"),\n    (\"c\", \"bigint\"),\n    (\"d\", \"string\"),\n]\n</code></pre> </p> <p>Example 1: Return PySpark<pre><code>&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully print PySpark output.</p> <p>Example 2: Return Pandas<pre><code>&gt;&gt;&gt; print(get_column_types(df, \"pd\"))\n</code></pre> Terminal<pre><code>   col_name  col_type\n0         a    bigint\n1         b    string\n2         c    bigint\n3         d    string\n</code></pre> <p>Conclusion: Successfully print Pandas output.</p> <p>Example 3: Invalid output<pre><code>&gt;&gt;&gt; print(get_column_types(df, \"foo\"))\n</code></pre> Terminal<pre><code>AttributeError: Invalid value for `output_type`: \"foo\".\nMust be one of: [\"pandas.DataFrame\", \"pandas\", \"pd.DataFrame\", \"pd.df\", \"pddf\", \"pdDataFrame\", \"pdDF\", \"pd\", \"spark.DataFrame\", \"pyspark.DataFrame\", \"pyspark\", \"spark\", \"ps.DataFrame\", \"ps.df\", \"psdf\", \"psDataFrame\", \"psDF\", \"ps\"]\n</code></pre> <p>Conclusion: Invalid input.</p> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef get_column_types(\n    dataframe: psDataFrame,\n    output_type: str = \"psDataFrame\",\n) -&gt; Union[psDataFrame, pdDataFrame]:\n    \"\"\"\n    !!! note \"Summary\"\n        This is a convenient function to return the data types from a given table as either a `#!py pyspark.sql.DataFrame` or `#!py pandas.DataFrame`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be checked.\n\n        output_type (str, optional):\n            How should the data be returned? As `#!py pdDataFrame` or `#!py psDataFrame`.\n\n            For `#!py pandas`, use one of:\n\n            ```{.py .python}\n            [\n                \"pandas\", \"pandas.DataFrame\",\n                \"pd.df\",  \"pd.DataFrame\",\n                \"pddf\",   \"pdDataFrame\",\n                \"pd\",     \"pdDF\",\n            ]\n            ```\n\n            &lt;/div&gt;\n\n            For `#!py pyspark` use one of:\n\n            ```{.py .python}\n            [\n                \"pyspark\", \"spark.DataFrame\",\n                \"spark\",   \"pyspark.DataFrame\",\n                \"ps.df\",   \"ps.DataFrame\",\n                \"psdf\",    \"psDataFrame\",\n                \"ps\",      \"psDF\",\n            ]\n            ```\n\n            Any other options are invalid.&lt;br&gt;\n            Defaults to `#!py \"psDataFrame\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If the given value parsed to `#!py output_type` is not one of the given valid types.\n\n    Returns:\n        (Union[psDataFrame, pdDataFrame]):\n            The DataFrame where each row represents a column on the original `#!py dataframe` object, and which has two columns:\n\n            1. The column name from `#!py dataframe`; and\n            2. The data type for that column in `#!py dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; print(df.dtypes)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        [\n            (\"a\", \"bigint\"),\n            (\"b\", \"string\"),\n            (\"c\", \"bigint\"),\n            (\"d\", \"string\"),\n        ]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Return PySpark\"}\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully print PySpark output.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Return Pandas\"}\n        &gt;&gt;&gt; print(get_column_types(df, \"pd\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n           col_name  col_type\n        0         a    bigint\n        1         b    string\n        2         c    bigint\n        3         d    string\n        ```\n        !!! success \"Conclusion: Successfully print Pandas output.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid output\"}\n        &gt;&gt;&gt; print(get_column_types(df, \"foo\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Invalid value for `output_type`: \"foo\".\n        Must be one of: [\"pandas.DataFrame\", \"pandas\", \"pd.DataFrame\", \"pd.df\", \"pddf\", \"pdDataFrame\", \"pdDF\", \"pd\", \"spark.DataFrame\", \"pyspark.DataFrame\", \"pyspark\", \"spark\", \"ps.DataFrame\", \"ps.df\", \"psdf\", \"psDataFrame\", \"psDF\", \"ps\"]\n        ```\n        !!! failure \"Conclusion: Invalid input.\"\n        &lt;/div&gt;\n    \"\"\"\n    if output_type not in VALID_DATAFRAME_NAMES:\n        raise AttributeError(\n            f\"Invalid value for `output_type`: '{output_type}'.\\n\"\n            f\"Must be one of: {VALID_DATAFRAME_NAMES}\"\n        )\n    output = pd.DataFrame(dataframe.dtypes, columns=[\"col_name\", \"col_type\"])\n    if output_type in VALID_PYSPARK_DATAFRAME_NAMES:\n        return dataframe.sparkSession.createDataFrame(output)\n    else:\n        return output\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.cast_column_to_type","title":"cast_column_to_type","text":"<pre><code>cast_column_to_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>This is a convenience function for casting a single column on a given table to another data type.</p> Details <p>At it's core, it will call the function like this:</p> <pre><code>dataframe = dataframe.withColumn(column, F.col(column).cast(datatype))\n</code></pre> <p>The reason for wrapping it up in this function is for validation of a columns existence and convenient re-declaration of the same.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>column</code> <code>str</code> <p>The column to be updated.</p> required <code>datatype</code> <code>Union[str, type, DataType]</code> <p>The datatype to be cast to. Must be a valid <code>pyspark</code> DataType.</p> <p>Use one of the following: <pre><code>[\n    \"string\",  \"char\",\n    \"varchar\", \"binary\",\n    \"boolean\", \"decimal\",\n    \"float\",   \"double\",\n    \"byte\",    \"short\",\n    \"integer\", \"long\",\n    \"date\",    \"timestamp\",\n    \"void\",    \"timestamp_ntz\",\n]\n</code></pre></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <code>ParseException</code> <p>If the given <code>datatype</code> is not a valid PySpark DataType.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 1: Valid casting<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, \"a\", \"string\")\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast column to type.</p> <p>Example 2: Invalid column<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, \"x\", \"string\")\n</code></pre> Terminal<pre><code>AttributeError: Column \"x\" does not exist in DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\"].\n</code></pre> <p>Conclusion: Column <code>x</code> does not exist as a valid column.</p> <p>Example 3: Invalid datatype<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, \"b\", \"foo\")\n</code></pre> Terminal<pre><code>ParseException: DataType \"foo\" is not supported.\n</code></pre> <p>Conclusion: Datatype <code>foo</code> is not valid.</p> See Also <ul> <li><code>assert_column_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef cast_column_to_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        This is a convenience function for casting a single column on a given table to another data type.\n\n    ???+ abstract \"Details\"\n\n        At it's core, it will call the function like this:\n\n        ```{.py .python linenums=\"1\"}\n        dataframe = dataframe.withColumn(column, F.col(column).cast(datatype))\n        ```\n\n        The reason for wrapping it up in this function is for validation of a columns existence and convenient re-declaration of the same.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        column (str):\n            The column to be updated.\n        datatype (Union[str, type, T.DataType]):\n            The datatype to be cast to.\n            Must be a valid `#!py pyspark` DataType.\n\n            Use one of the following:\n            ```{.py .python}\n            [\n                \"string\",  \"char\",\n                \"varchar\", \"binary\",\n                \"boolean\", \"decimal\",\n                \"float\",   \"double\",\n                \"byte\",    \"short\",\n                \"integer\", \"long\",\n                \"date\",    \"timestamp\",\n                \"void\",    \"timestamp_ntz\",\n            ]\n            ```\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If `#!py column` does not exist within `#!py dataframe.columns`.\n        ParseException:\n            If the given `#!py datatype` is not a valid PySpark DataType.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Valid casting\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, \"a\", \"string\")\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast column to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Invalid column\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, \"x\", \"string\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Column \"x\" does not exist in DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\"].\n        ```\n        !!! failure \"Conclusion: Column `x` does not exist as a valid column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid datatype\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, \"b\", \"foo\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ParseException: DataType \"foo\" is not supported.\n        ```\n        !!! failure \"Conclusion: Datatype `foo` is not valid.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_column_exists()`][toolbox_pyspark.checks.column_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n    assert_column_exists(dataframe, column)\n    datatype = _validate_pyspark_datatype(datatype=datatype)\n    return dataframe.withColumn(column, F.col(column).cast(datatype))  # type:ignore\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.cast_columns_to_type","title":"cast_columns_to_type","text":"<pre><code>cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_list],\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Cast multiple columns to a given type.</p> Details <p>An extension of <code>cast_column_to_type()</code> to allow casting of multiple columns simultaneously.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>columns</code> <code>Union[str, str_list]</code> <p>The list of columns to be updated. They all must be valid columns existing on <code>DataFrame</code>.</p> required <code>datatype</code> <code>Union[str, type, DataType]</code> <p>The datatype to be cast to. Must be a valid PySpark DataType.</p> <p>Use one of the following:     <pre><code>[\n    \"string\",  \"char\",\n    \"varchar\", \"binary\",\n    \"boolean\", \"decimal\",\n    \"float\",   \"double\",\n    \"byte\",    \"short\",\n    \"integer\", \"long\",\n    \"date\",    \"timestamp\",\n    \"void\",    \"timestamp_ntz\",\n]\n</code></pre></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, [\"a\"], \"string\")\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | bigint   |\n| d        | bigint   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast column to type.</p> <p>Example 2: Multiple columns<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, [\"c\", \"d\"], \"string\")\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast columns to type.</p> <p>Example 3: Invalid column<pre><code>&gt;&gt;&gt; df = cast_columns_to_type(df, [\"x\", \"y\"], \"string\")\n</code></pre> Terminal<pre><code>AttributeError: Columns [\"x\", \"y\"] do not exist in DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\"].\n</code></pre> <p>Conclusion: Columns <code>[x]</code> does not exist as a valid column.</p> <p>Example 4: Invalid datatype<pre><code>&gt;&gt;&gt; df = cast_columns_to_type(df, [\"a\", \"b\"], \"foo\")\n</code></pre> Terminal<pre><code>ParseException: DataType \"foo\" is not supported.\n</code></pre> <p>Conclusion: Datatype <code>foo</code> is not valid.</p> See Also <ul> <li><code>assert_columns_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_list],\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Cast multiple columns to a given type.\n\n    ???+ abstract \"Details\"\n        An extension of [`#!py cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type] to allow casting of multiple columns simultaneously.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        columns (Union[str, str_list]):\n            The list of columns to be updated. They all must be valid columns existing on `#!py DataFrame`.\n        datatype (Union[str, type, T.DataType]):\n            The datatype to be cast to.\n            Must be a valid PySpark DataType.\n\n            Use one of the following:\n                ```{.py .python}\n                [\n                    \"string\",  \"char\",\n                    \"varchar\", \"binary\",\n                    \"boolean\", \"decimal\",\n                    \"float\",   \"double\",\n                    \"byte\",    \"short\",\n                    \"integer\", \"long\",\n                    \"date\",    \"timestamp\",\n                    \"void\",    \"timestamp_ntz\",\n                ]\n                ```\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, [\"a\"], \"string\")\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | bigint   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast column to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Multiple columns\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, [\"c\", \"d\"], \"string\")\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast columns to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid column\"}\n        &gt;&gt;&gt; df = cast_columns_to_type(df, [\"x\", \"y\"], \"string\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Columns [\"x\", \"y\"] do not exist in DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\"].\n        ```\n        !!! failure \"Conclusion: Columns `[x]` does not exist as a valid column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Invalid datatype\"}\n        &gt;&gt;&gt; df = cast_columns_to_type(df, [\"a\", \"b\"], \"foo\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ParseException: DataType \"foo\" is not supported.\n        ```\n        !!! failure \"Conclusion: Datatype `foo` is not valid.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n    columns = [columns] if isinstance(columns, str) else columns\n    assert_columns_exists(dataframe, columns)\n    datatype = _validate_pyspark_datatype(datatype=datatype)\n    return dataframe.withColumns({col: F.col(col).cast(datatype) for col in columns})\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.map_cast_columns_to_type","title":"map_cast_columns_to_type","text":"<pre><code>map_cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns_type_mapping: dict[\n        Union[str, type, T.DataType],\n        Union[str, str_list, str_tuple],\n    ],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Take a dictionary mapping of where the keys is the type and the values are the column(s), and apply that to the given dataframe.</p> Details <p>Applies <code>cast_columns_to_type()</code> and <code>cast_column_to_type()</code> under the hood.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <code>columns_type_mapping</code> <code>Dict[Union[str, type, DataType], Union[str, str_list, str_tuple]]</code> <p>The mapping of the columns to manipulate. The format must be: <code>{type: columns}</code>. Where the keys are the relevant type to cast to, and the values are the column(s) for casting.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data frame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"str\": [\"a\", \"c\"]})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast columns to type.</p> <p>Example 2: Multiple types<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"int\": [\"a\", \"c\"], \"str\": [\"b\"], \"float\": \"d\"})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | float    |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast columns to types.</p> <p>Example 3: All to single type<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {str: [col for col in df.columns]})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast all columns to type.</p> See Also <ul> <li><code>cast_column_to_type()</code></li> <li><code>cast_columns_to_type()</code></li> <li><code>assert_columns_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef map_cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns_type_mapping: dict[\n        Union[str, type, T.DataType],\n        Union[str, str_list, str_tuple],\n    ],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Take a dictionary mapping of where the keys is the type and the values are the column(s), and apply that to the given dataframe.\n\n    ???+ abstract \"Details\"\n        Applies [`#!py cast_columns_to_type()`][toolbox_pyspark.types.cast_columns_to_type] and [`#!py cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type] under the hood.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to transform.\n        columns_type_mapping (Dict[ Union[str, type, T.DataType], Union[str, str_list, str_tuple], ]):\n            The mapping of the columns to manipulate.&lt;br&gt;\n            The format must be: `#!py {type: columns}`.&lt;br&gt;\n            Where the keys are the relevant type to cast to, and the values are the column(s) for casting.\n\n    Returns:\n        (psDataFrame):\n            The transformed data frame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"str\": [\"a\", \"c\"]})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast columns to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Multiple types\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"int\": [\"a\", \"c\"], \"str\": [\"b\"], \"float\": \"d\"})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | float    |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast columns to types.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: All to single type\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {str: [col for col in df.columns]})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast all columns to type.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type]\n        - [`cast_columns_to_type()`][toolbox_pyspark.types.cast_columns_to_type]\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n\n    # Ensure all keys are `str`\n    keys = (*columns_type_mapping.keys(),)\n    for key in keys:\n        if isinstance(key, type):\n            if key.__name__ in keys:\n                columns_type_mapping[key.__name__] = list(\n                    columns_type_mapping[key.__name__]\n                ) + list(columns_type_mapping.pop(key))\n            else:\n                columns_type_mapping[key.__name__] = columns_type_mapping.pop(key)\n\n    # Reverse keys and values\n    reversed_mapping = dict_reverse_keys_and_values(dictionary=columns_type_mapping)\n\n    # Validate\n    assert_columns_exists(dataframe, reversed_mapping.keys())\n\n    # Apply mapping to dataframe\n    try:\n        dataframe = dataframe.withColumns(\n            {\n                col: F.col(col).cast(_validate_pyspark_datatype(typ))\n                for col, typ in reversed_mapping.items()\n            }\n        )\n    except Exception as e:  # pragma: no cover\n        raise RuntimeError(f\"Raised {e.__class__.__name__}: {e}\") from e\n\n    # Return\n    return dataframe\n</code></pre>"},{"location":"usage/overview/","title":"Overview<code>toolbox-pyspark</code>","text":""},{"location":"usage/overview/#introduction","title":"Introduction","text":"<p>The purpose of this package is to provide some helper files/functions/classes for generic PySpark processes.</p>"},{"location":"usage/overview/#key-urls","title":"Key URLs","text":"<p>For reference, these URL's are used:</p> Type Source URL Git Repo GitHub https://github.com/data-science-extensions/toolbox-pyspark Python Package PyPI https://pypi.org/project/toolbox-pyspark Package Docs Pages https://data-science-extensions.com/toolbox-pyspark"},{"location":"usage/overview/#installation","title":"Installation","text":"<p>You can install and use this package multiple ways by using <code>pip</code>, <code>pipenv</code>, or <code>poetry</code>.</p>"},{"location":"usage/overview/#using-pip","title":"Using <code>pip</code>:","text":"<ol> <li> <p>In your terminal, run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install --requirement=requirements.txt\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-pipenv","title":"Using <code>pipenv</code>:","text":"<ol> <li> <p>Install using environment variables:</p> <p>In your <code>Pipfile</code> file, add:</p> <pre><code>[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = false\nname = \"pypi\"\n\n[packages]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install pipenv\npython3 -m pipenv install --verbose --skip-lock --categories=root index=pypi toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m run pipenv install --verbose --skip-lock --requirements=requirements.txt\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>python3 -m pipenv install --verbose --skip-lock toolbox-pyspark\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-poetry","title":"Using <code>poetry</code>:","text":"<ol> <li> <p>In your <code>pyproject.toml</code> file, add:</p> <pre><code>[tool.poetry.dependencies]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>poetry add toolbox-pyspark\npoetry install\npoetry sync\n</code></pre> </li> </ol>"},{"location":"usage/overview/#contribution","title":"Contribution","text":"<p>Contribution is always welcome.</p> <ol> <li> <p>First, either fork or branch the main repo.</p> </li> <li> <p>Clone your forked/branched repo.</p> </li> <li> <p>Build your environment:</p> <ol> <li> <p>With <code>pipenv</code> on Windows:</p> <pre><code>if (-not (Test-Path .venv)) {mkdir .venv}\npython -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>pipenv</code> on Linux:</p> <pre><code>mkdir .venv\npython3 -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Windows:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install poetry\npython -m poetry init\npython -m poetry add $(cat requirements/root.txt)\npython -m poetry add --group=dev $(cat requirements/dev.txt)\npython -m poetry add --group=test $(cat requirements/test.txt)\npython -m poetry add --group=docs $(cat requirements/docs.txt)\npython -m poetry install\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Linux:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install poetry\npython3 -m poetry init\npython3 -m poetry add $(cat requirements/root.txt)\npython3 -m poetry add --group=dev $(cat requirements/dev.txt)\npython3 -m poetry add --group=test $(cat requirements/test.txt)\npython3 -m poetry add --group=docs $(cat requirements/docs.txt)\npython3 -m poetry install\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> </ol> </li> <li> <p>Start contributing.</p> </li> <li> <p>When you're happy with the changes, raise a Pull Request to merge with the main branch again.</p> </li> </ol>"},{"location":"usage/overview/#build-and-test","title":"Build and Test","text":"<p>To ensure that the package is working as expected, please ensure that:</p> <ol> <li>You write your code as per PEP8 requirements.</li> <li>You write a UnitTest for each function/feature you include.</li> <li>The CodeCoverage is 100%.</li> <li>All UnitTests are passing.</li> <li>MyPy is passing 100%.</li> </ol>"},{"location":"usage/overview/#testing","title":"Testing","text":"<ul> <li> <p>Run them all together</p> <pre><code>poetry run make check\n</code></pre> </li> <li> <p>Or run them individually:</p> <ul> <li> <p>Black <pre><code>poetry run make check-black\n</code></pre></p> </li> <li> <p>PyTests:     <pre><code>poetry run make ckeck-pytest\n</code></pre></p> </li> <li> <p>MyPy:     <pre><code>poetry run make check-mypy\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"usage/roadmap/","title":"Roadmap","text":"Module Completed Issue Milestone <code>io</code> \u2705 <code>checks</code> \u2705 <code>types</code> \u2705 <code>keys</code> \u2705 <code>scale</code> \u2705 <code>dimensions</code> \u2705 <code>columns</code> \u2705 <code>cleaning</code> \u2b1c <code>datetime</code> \u2b1c <code>delta</code> \u2b1c <code>duplication</code> \u2b1c <code>schema</code> \u2b1c"}]}