{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"code/","title":"Modules","text":""},{"location":"code/#overview","title":"Overview","text":"<p>There are 12 modules used in this package, which covers 41 functions</p>"},{"location":"code/#module-descriptions","title":"Module Descriptions","text":"Module Description io The <code>io</code> module is used for reading and writing tables to/from directories. checks The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe."},{"location":"code/#functions-by-module","title":"Functions by Module","text":"Module Function <code>io</code> <code>read_from_path()</code> <code>write_to_path()</code> <code>transfer_table()</code> <code>checks</code> <code>column_exists()</code> <code>columns_exists()</code> <code>is_vaid_spark_type()</code> <code>table_exists()</code>"},{"location":"code/#testing","title":"Testing","text":"<p>This package is fully tested against:</p> <ol> <li>Unit tests</li> <li>Lint tests</li> <li>MyPy tests</li> <li>Build tests</li> </ol>"},{"location":"code/#latest-code-coverage","title":"Latest Code Coverage","text":""},{"location":"code/checks/","title":"Checks","text":""},{"location":"code/checks/#toolbox_pyspark.checks","title":"toolbox_pyspark.checks","text":"<p>Summary</p> <p>The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe.</p>"},{"location":"code/checks/#toolbox_pyspark.checks.column_exists","title":"column_exists","text":"<pre><code>column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>column.upper()</code>. Default: <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if exists or <code>False</code> otherwise.</p> Examples Set Up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import column_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example1: Column Exists<pre><code>&gt;&gt;&gt; result = column_exists(df, \"a\")\n&gt;&gt;&gt; print(result)\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Column exists.</p> <p>Example 2: Column Missing<pre><code>&gt;&gt;&gt; result = column_exists(df, \"c\")\n&gt;&gt;&gt; print(result)\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py column.upper()`.&lt;br&gt;\n            Default: `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            `#!py True` if exists or `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\", title=\"Set Up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import column_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example1: Column Exists\"}\n        &gt;&gt;&gt; result = column_exists(df, \"a\")\n        &gt;&gt;&gt; print(result)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Column exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Column Missing\"}\n        &gt;&gt;&gt; result = column_exists(df, \"c\")\n        &gt;&gt;&gt; print(result)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_exists(dataframe, [column], match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.columns_exists","title":"columns_exists","text":"<pre><code>columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str_list, str_tuple, str_set],\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check</p> required <code>columns</code> <code>Union[str_list, str_tuple, str_set]</code> <p>The columns to check</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>[col.upper() for col in columns]</code>. Default: <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all columns exist or <code>False</code> otherwise.</p> Examples Set Up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import columns_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: Columns exist<pre><code>&gt;&gt;&gt; columns_exists(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: All columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; columns_exists(df, [\"b\", \"d\"])\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: One column is missing.</p> <p>Example 3: All columns missing<pre><code>&gt;&gt;&gt; columns_exists(df, [\"c\", \"d\"])\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: All columns are missing.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str_list, str_tuple, str_set],\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check\n        columns (Union[str_list, str_tuple, str_set]):\n            The columns to check\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py [col.upper() for col in columns]`.&lt;br&gt;\n            Default: `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            `#!py True` if all columns exist or `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set Up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import columns_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Columns exist\"}\n        &gt;&gt;&gt; columns_exists(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: All columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; columns_exists(df, [\"b\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: One column is missing.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: All columns missing\"}\n        &gt;&gt;&gt; columns_exists(df, [\"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: All columns are missing.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_exists(dataframe, columns, match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_column_exists","title":"assert_column_exists","text":"<pre><code>assert_column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>column.upper()</code>. Default: <code>True</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set Up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1,2,3,4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_column_exists(df, \"a\")\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Column exists.</p> <p>Example 2: Error raised<pre><code>&gt;&gt;&gt; assert_column_exists(df, \"c\")\n</code></pre> Terminal<pre><code>Attribute Error: Column 'c' does not exist in 'dataframe'.\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py column.upper()`.&lt;br&gt;\n            Default: `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If `#!py column` does not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set Up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1,2,3,4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_column_exists(df, \"a\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Column exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Error raised\"}\n        &gt;&gt;&gt; assert_column_exists(df, \"c\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Column 'c' does not exist in 'dataframe'.\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    if not column_exists(dataframe, column, match_case):\n        raise AttributeError(\n            f\"Column '{column}' does not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}.\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_columns_exists","title":"assert_columns_exists","text":"<pre><code>assert_columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str_list, str_tuple, str_set],\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check</p> required <code>columns</code> <code>Union[str_list, str_tuple, str_set]</code> <p>The columns to check</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>[col.upper() for col in columns]</code>. Default: <code>True</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set Up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"c\"] do not exist in 'dataframe'.\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column 'c' does not exist.</p> <p>Example 3: Multiple columns missing<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\", \"d\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"c\", \"d\"] do not exist in 'dataframe'.\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Columns 'c' and 'd' does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str_list, str_tuple, str_set],\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check\n        columns (Union[str_list, str_tuple, str_set]):\n            The columns to check\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py [col.upper() for col in columns]`.&lt;br&gt;\n            Default: `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set Up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"c\"] do not exist in 'dataframe'.\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column 'c' does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Multiple columns missing\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"c\", \"d\"] do not exist in 'dataframe'.\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Columns 'c' and 'd' does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    (exist, missing_cols) = _columns_exists(dataframe, columns, match_case)\n    if not exist:\n        raise AttributeError(\n            f\"Columns {missing_cols} do not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.is_vaid_spark_type","title":"is_vaid_spark_type","text":"<pre><code>is_vaid_spark_type(datatype: str) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>datatype</code> is a correct and valid <code>pyspark</code> data type.</p> <p>Parameters:</p> Name Type Description Default <code>datatype</code> <code>str</code> <p>The name of the data type to check.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If the given <code>datatype</code> is not a valid <code>pyspark</code> data type.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set Up<pre><code>&gt;&gt;&gt; from toolbox_pyspark.checks import is_vaid_spark_type\n</code></pre> <p>Loop through all valid types<pre><code>&gt;&gt;&gt; type_names = [\"string\", \"char\", \"varchar\", \"binary\", \"boolean\", \"decimal\", \"float\", \"double\", \"byte\", \"short\", \"integer\", \"long\", \"date\", \"timestamp\", \"timestamp_ntz\", \"void\"]\n&gt;&gt;&gt; for type_name in type_names:\n...     is_vaid_spark_type(type_name)\n</code></pre>  Nothing is returned each time. Because they're all valid. <p>Conclusion: They're all valid.</p> <p>Check some invalid types<pre><code>&gt;&gt;&gt; type_names = [\"np.ndarray\", \"pd.DataFrame\", \"dict\"]\n&gt;&gt;&gt; for type_name in type_names:\n...     is_vaid_spark_type(type_name)\n</code></pre> Terminal<pre><code>AttributeError: DataType 'np.ndarray' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> Terminal<pre><code>AttributeError: DataType 'pd.DataFrame' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> Terminal<pre><code>AttributeError: DataType 'dict' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> <p>Conclusion: All of these types are invalid.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef is_vaid_spark_type(datatype: str) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py datatype` is a correct and valid `#!py pyspark` data type.\n\n    Params:\n        datatype (str):\n            The name of the data type to check.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If the given `#!py datatype` is not a valid `#!py pyspark` data type.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set Up\"}\n        &gt;&gt;&gt; from toolbox_pyspark.checks import is_vaid_spark_type\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Loop through all valid types\"}\n        &gt;&gt;&gt; type_names = [\"string\", \"char\", \"varchar\", \"binary\", \"boolean\", \"decimal\", \"float\", \"double\", \"byte\", \"short\", \"integer\", \"long\", \"date\", \"timestamp\", \"timestamp_ntz\", \"void\"]\n        &gt;&gt;&gt; for type_name in type_names:\n        ...     is_vaid_spark_type(type_name)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        Nothing is returned each time. Because they're all valid.\n        !!! success \"Conclusion: They're all valid.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Check some invalid types\"}\n        &gt;&gt;&gt; type_names = [\"np.ndarray\", \"pd.DataFrame\", \"dict\"]\n        &gt;&gt;&gt; for type_name in type_names:\n        ...     is_vaid_spark_type(type_name)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'np.ndarray' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'pd.DataFrame' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'dict' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        !!! failure \"Conclusion: All of these types are invalid.\"\n        &lt;/div&gt;\n    \"\"\"\n    if datatype not in VALID_PYSPARK_TYPE_NAMES:\n        raise AttributeError(\n            f\"DataType '{datatype}' is not valid.\\n\"\n            f\"Must be one of: {VALID_PYSPARK_TYPE_NAMES}\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.table_exists","title":"table_exists","text":"<pre><code>table_exists(\n    name: str,\n    path: str,\n    data_format: str,\n    spark_session: SparkSession,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Will try to read <code>table</code> from <code>path</code> using <code>format</code>, and if successful will return <code>True</code> otherwise <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to check exists.</p> required <code>path</code> <code>str</code> <p>The directory where the table should be existing.</p> required <code>data_format</code> <code>str</code> <p>The format of the table to try checking.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The <code>spark</code> session to use for the importing.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Returns <code>True</code> if the table exists, <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Constants\n&gt;&gt;&gt; write_name = \"test_df\"\n&gt;&gt;&gt; write_path = f\"./test\"\n&gt;&gt;&gt; write_format = \"parquet\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write data\n&gt;&gt;&gt; write_to_path(df, f\"{write_name}.{write_format}\", write_path)\n</code></pre> <p>Example 1: Table exists<pre><code>&gt;&gt;&gt; table_exists(\"test_df.parquet\", \"./test\", \"parquet\", spark)\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Table exists.</p> <p>Example 2: Table does not exist<pre><code>&gt;&gt;&gt; table_exists(\"bad_table_name.parquet\", \"./test\", \"parquet\", spark)\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Table does not exist.</p> See Also <ul> <li><code>toolbox_pyspark.io.read_from_path()</code></li> </ul> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef table_exists(\n    name: str,\n    path: str,\n    data_format: str,\n    spark_session: SparkSession,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Will try to read `#!py table` from `#!py path` using `#!py format`, and if successful will return `#!py True` otherwise `#!py False`.\n\n    Params:\n        name (str):\n            The name of the table to check exists.\n        path (str):\n            The directory where the table should be existing.\n        data_format (str):\n            The format of the table to try checking.\n        spark_session (SparkSession):\n            The `#!py spark` session to use for the importing.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            Returns `#!py True` if the table exists, `False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Constants\n        &gt;&gt;&gt; write_name = \"test_df\"\n        &gt;&gt;&gt; write_path = f\"./test\"\n        &gt;&gt;&gt; write_format = \"parquet\"\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Write data\n        &gt;&gt;&gt; write_to_path(df, f\"{write_name}.{write_format}\", write_path)\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Table exists\"}\n        &gt;&gt;&gt; table_exists(\"test_df.parquet\", \"./test\", \"parquet\", spark)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Table exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Table does not exist\"}\n        &gt;&gt;&gt; table_exists(\"bad_table_name.parquet\", \"./test\", \"parquet\", spark)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Table does not exist.\"\n        &lt;/div&gt;\n\n    ???+ tip \"See Also\"\n        - [`toolbox_pyspark.io.read_from_path()`][toolbox_pyspark.io.read_from_path]\n    \"\"\"\n    try:\n        _ = read_from_path(\n            name=name,\n            path=path,\n            data_format=data_format,\n            spark_session=spark_session,\n        )\n    except Exception:\n        return False\n    return True\n</code></pre>"},{"location":"code/io/","title":"IO","text":""},{"location":"code/io/#toolbox_pyspark.io","title":"toolbox_pyspark.io","text":"<p>Summary</p> <p>The <code>io</code> module is used for reading and writing tables to/from directories.</p>"},{"location":"code/io/#toolbox_pyspark.io.read_from_path","title":"read_from_path","text":"<pre><code>read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Read an object from a given <code>path</code> in to memory as a <code>pyspark</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to read in.</p> required <code>path</code> <code>str</code> <p>The path from which it will be read.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The Spark session to use for the reading.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format of the object at location <code>path</code>. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>read_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. Like, for example:</p> <ul> <li>If the object is a CSV, you may want to define that it has a header row: <code>{\"header\": \"true\"}</code>.</li> <li>If the object is a Delta table, you may want to query a specific version: <code>{versionOf\": \"0\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The loaded dataframe.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         \"c\": [1, 1, 1, 1],\n...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...     }\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write data\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> Terminal<pre><code>[\"table.csv\", \"table.parquet\"]\n</code></pre> </p> <p>Example 1: Read CSV<pre><code>&gt;&gt;&gt; df_csv = read_from_path(\n...     name=\"table.csv\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"csv\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_csv.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully read CSV.</p> <p>Example 2: Read Parquet<pre><code>&gt;&gt;&gt; df_parquet = read_from_path(\n...     name=\"table.parquet\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"parquet\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_parquet.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully read Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Read an object from a given `path` in to memory as a `pyspark` dataframe.\n\n    Params:\n        name (str):\n            The name of the table to read in.\n        path (str):\n            The path from which it will be read.\n        spark_session (SparkSession):\n            The Spark session to use for the reading.\n        data_format (Optional[str], optional):\n            The format of the object at location `path`.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        read_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            Like, for example:&lt;br&gt;\n\n            - If the object is a CSV, you may want to define that it has a header row: `#!py {\"header\": \"true\"}`.\n            - If the object is a Delta table, you may want to query a specific version: `#!py {versionOf\": \"0\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The loaded dataframe.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4],\n        ...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         \"c\": [1, 1, 1, 1],\n        ...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Write data\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"table.csv\", \"table.parquet\"]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Read CSV\"}\n        &gt;&gt;&gt; df_csv = read_from_path(\n        ...     name=\"table.csv\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"csv\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df_csv.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully read CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Read Parquet\"}\n        &gt;&gt;&gt; df_parquet = read_from_path(\n        ...     name=\"table.parquet\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"parquet\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df_parquet.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully read Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    data_format: str = data_format or \"parquet\"\n    reader: DataFrameReader = spark_session.read.format(data_format)\n    if read_options:\n        reader.options(**read_options)\n    return reader.load(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.write_to_path","title":"write_to_path","text":"<pre><code>write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>For a given <code>table</code>, write it out to a specified <code>path</code> with name <code>name</code> and format <code>format</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame</code> <p>The table to be written. Must be a valid <code>pyspark</code> DataFrame (<code>pyspark.sql.DataFrame</code>).</p> required <code>name</code> <code>str</code> <p>The name of the table where it will be written.</p> required <code>path</code> <code>str</code> <p>The path location for where to save the table.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format that the <code>table</code> will be written to. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>write_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. Like, for example:</p> <ul> <li>If you are writing to a Delta object, and wanted to overwrite the schema: <code>{\"overwriteSchema\": \"true\"}</code>.</li> <li>If you\"re writing to a CSV file, and wanted to specify the header row: <code>{\"header\": \"true\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Note <p>You know that this function is successful if the table exists at the specified location, and there are no errors thrown.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Write to CSV<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     mode=\"overwrite\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully written to CSV.</p> <p>Example 2: Write to Parquet<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     mode=\"overwrite\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully written to Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `table`, write it out to a specified `path` with name `name` and format `format`.\n\n    Params:\n        table (psDataFrame):\n            The table to be written. Must be a valid `pyspark` DataFrame ([`pyspark.sql.DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)).\n        name (str):\n            The name of the table where it will be written.\n        path (str):\n            The path location for where to save the table.\n        data_format (Optional[str], optional):\n            The format that the `table` will be written to.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        write_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            Like, for example:\n\n            - If you are writing to a Delta object, and wanted to overwrite the schema: `#!py {\"overwriteSchema\": \"true\"}`.\n            - If you\"re writing to a CSV file, and wanted to specify the header row: `#!py {\"header\": \"true\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n        partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ tip \"Note\"\n        You know that this function is successful if the table exists at the specified location, and there are no errors thrown.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Write to CSV\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     mode=\"overwrite\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully written to CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Write to Parquet\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     mode=\"overwrite\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully written to Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    write_options: str_dict = write_options or dict()\n    data_format: str = data_format or \"parquet\"\n    writer: DataFrameWriter = table.write.mode(mode).format(data_format)\n    if write_options:\n        writer.options(**write_options)\n    if partition_cols is not None:\n        partition_cols = (\n            [partition_cols] if is_type(partition_cols, str) else partition_cols\n        )\n        writer = writer.partitionBy(list(partition_cols))\n    writer.save(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.transfer_table","title":"transfer_table","text":"<pre><code>transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[\n        str_collection\n    ] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Copy a table from one location to another.</p> Details <p>This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.</p> <p>Parameters:</p> Name Type Description Default <code>spark_session</code> <code>SparkSession</code> <p>The spark session to use for the transfer. Necessary in order to instantiate the reading process.</p> required <code>from_table_path</code> <code>str</code> <p>The path from which the table will be read.</p> required <code>from_table_name</code> <code>str</code> <p>The name of the table to be read.</p> required <code>from_table_format</code> <code>str</code> <p>The format of the data at the reading location.</p> required <code>to_table_path</code> <code>str</code> <p>The location where to save the table to.</p> required <code>to_table_name</code> <code>str</code> <p>The name of the table where it will be saved.</p> required <code>to_table_format</code> <code>str</code> <p>The format of the saved table.</p> required <code>from_table_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>to_table_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Note <p>You know that this function is successful if the table exists at the specified location, and there are no errors thrown.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         \"c\": [1, 1, 1 1],\n...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...     }\n... )\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> Terminal<pre><code>[\"table.csv\", \"table.parquet\"]\n</code></pre> </p> <p>Example 1: Transfer CSV<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.csv\",\n...     to_table_format=\"csv\",\n...     from_table_options={\"header\": \"true\"},\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./other\",\n...     data_format=\"csv\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred CSV to CSV.</p> <p>Example 2: Transfer Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.parquet\",\n...     from_table_format=\"parquet\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred Parquet to Parquet.</p> <p>Example 3: Transfer CSV to Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred CSV to Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Copy a table from one location to another.\n\n    ???+ abstract \"Details\"\n        This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.\n\n    Params:\n        spark_session (SparkSession):\n            The spark session to use for the transfer. Necessary in order to instantiate the reading process.\n        from_table_path (str):\n            The path from which the table will be read.\n        from_table_name (str):\n            The name of the table to be read.\n        from_table_format (str):\n            The format of the data at the reading location.\n        to_table_path (str):\n            The location where to save the table to.\n        to_table_name (str):\n            The name of the table where it will be saved.\n        to_table_format (str):\n            The format of the saved table.\n        from_table_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        to_table_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ tip \"Note\"\n        You know that this function is successful if the table exists at the specified location, and there are no errors thrown.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4],\n        ...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         \"c\": [1, 1, 1 1],\n        ...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"table.csv\", \"table.parquet\"]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Transfer CSV\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.csv\",\n        ...     to_table_format=\"csv\",\n        ...     from_table_options={\"header\": \"true\"},\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./other\",\n        ...     data_format=\"csv\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred CSV to CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Transfer Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.parquet\",\n        ...     from_table_format=\"parquet\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred Parquet to Parquet.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Transfer CSV to Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred CSV to Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    from_table_options: str_dict = from_table_options or dict()\n    to_table_options: str_dict = to_table_options or dict()\n    from_table: psDataFrame = read_from_path(\n        name=from_table_name,\n        path=from_table_path,\n        spark_session=spark_session,\n        data_format=from_table_format,\n        read_options=from_table_options,\n    )\n    write_to_path(\n        table=from_table,\n        name=to_table_name,\n        path=to_table_path,\n        data_format=to_table_format,\n        mode=to_table_mode,\n        write_options=to_table_options,\n        partition_cols=to_table_partition_cols,\n    )\n</code></pre>"},{"location":"usage/overview/","title":"Overview<code>toolbox-pyspark</code>","text":""},{"location":"usage/overview/#introduction","title":"Introduction","text":"<p>The purpose of this package is to provide some helper files/functions/classes for generic PySpark processes.</p>"},{"location":"usage/overview/#key-urls","title":"Key URLs","text":"<p>For reference, these URL's are used:</p> Type Source URL Git Repo GitHub https://github.com/data-science-extensions/toolbox-pyspark Python Package PyPI https://pypi.org/project/toolbox-pyspark Package Docs Pages https://data-science-extensions.com/toolbox-pyspark/"},{"location":"usage/overview/#installation","title":"Installation","text":"<p>You can install and use this package multiple ways by using <code>pip</code>, <code>pipenv</code>, or <code>poetry</code>.</p>"},{"location":"usage/overview/#using-pip","title":"Using <code>pip</code>:","text":"<ol> <li> <p>In your terminal, run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install --requirement=requirements.txt\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-pipenv","title":"Using <code>pipenv</code>:","text":"<ol> <li> <p>Install using environment variables:</p> <p>In your <code>Pipfile</code> file, add:</p> <pre><code>[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = false\nname = \"pypi\"\n\n[packages]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install pipenv\npython3 -m pipenv install --verbose --skip-lock --categories=root index=pypi toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m run pipenv install --verbose --skip-lock --requirements=requirements.txt\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>python3 -m pipenv install --verbose --skip-lock toolbox-pyspark\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-poetry","title":"Using <code>poetry</code>:","text":"<ol> <li> <p>In your <code>pyproject.toml</code> file, add:</p> <pre><code>[tool.poetry.dependencies]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>poetry add toolbox-pyspark\npoetry install\npoetry sync\n</code></pre> </li> </ol>"},{"location":"usage/overview/#contribution","title":"Contribution","text":"<p>Contribution is always welcome.</p> <ol> <li> <p>First, either fork or branch the main repo.</p> </li> <li> <p>Clone your forked/branched repo.</p> </li> <li> <p>Build your environment:</p> <ol> <li> <p>With <code>pipenv</code> on Windows:</p> <pre><code>if (-not (Test-Path .venv)) {mkdir .venv}\npython -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>pipenv</code> on Linux:</p> <pre><code>mkdir .venv\npython3 -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Windows:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install poetry\npython -m poetry init\npython -m poetry add $(cat requirements/root.txt)\npython -m poetry add --group=dev $(cat requirements/dev.txt)\npython -m poetry add --group=test $(cat requirements/test.txt)\npython -m poetry add --group=docs $(cat requirements/docs.txt)\npython -m poetry install\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Linux:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install poetry\npython3 -m poetry init\npython3 -m poetry add $(cat requirements/root.txt)\npython3 -m poetry add --group=dev $(cat requirements/dev.txt)\npython3 -m poetry add --group=test $(cat requirements/test.txt)\npython3 -m poetry add --group=docs $(cat requirements/docs.txt)\npython3 -m poetry install\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> </ol> </li> <li> <p>Start contributing.</p> </li> <li> <p>When you're happy with the changes, raise a Pull Request to merge with the main branch again.</p> </li> </ol>"},{"location":"usage/overview/#build-and-test","title":"Build and Test","text":"<p>To ensure that the package is working as expected, please ensure that:</p> <ol> <li>You write your code as per PEP8 requirements.</li> <li>You write a UnitTest for each function/feature you include.</li> <li>The CodeCoverage is 100%.</li> <li>All UnitTests are passing.</li> <li>MyPy is passing 100%.</li> </ol>"},{"location":"usage/overview/#testing","title":"Testing","text":"<ul> <li> <p>Run them all together</p> <pre><code>poetry run make check\n</code></pre> </li> <li> <p>Or run them individually:</p> <ul> <li> <p>Black <pre><code>poetry run make check-black\n</code></pre></p> </li> <li> <p>PyTests:     <pre><code>poetry run make ckeck-pytest\n</code></pre></p> </li> <li> <p>MyPy:     <pre><code>poetry run make check-mypy\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"usage/roadmap/","title":"Roadmap","text":"Module Completed Issue Milestone <code>io</code> \u2705 <code>checks</code> \u2705 <code>cleaning</code> \u2b1c <code>columns</code> \u2b1c <code>datetime</code> \u2b1c <code>delta</code> \u2b1c <code>dimensions</code> \u2b1c <code>duplication</code> \u2b1c <code>keys</code> \u2b1c <code>scale</code> \u2b1c <code>schema</code> \u2b1c <code>types</code> \u2b1c"}]}