{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"code/","title":"Modules","text":""},{"location":"code/#overview","title":"Overview","text":"<p>There are 12 modules used in this package, which covers 41 functions</p>"},{"location":"code/#module-descriptions","title":"Module Descriptions","text":"Module Description <code>io</code> The <code>io</code> module is used for reading and writing tables to/from directories. <code>checks</code> The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe. <code>types</code> The <code>types</code> module is used to get, check, and change a datafames column data types. <code>keys</code> The <code>keys</code> module is used for creating new columns to act as keys (primary and foreign), to be used for joins with other tables, or to create relationships within downstream applications, like PowerBI. <code>scale</code> The <code>scale</code> module is used for rounding a column (or columns) to a given rounding accuracy. <code>dimensions</code> The <code>dimensions</code> module is used for checking the dimensions of <code>pyspark</code> <code>dataframe</code>'s. <code>columns</code> The <code>columns</code> module is used to fetch columns from a given DataFrame using convenient syntax. <code>datetime</code> The <code>datetime</code> module is used for fixing column names that contain datetime data, adding conversions to local datetimes, and for splitting a column in to their date and time components."},{"location":"code/#functions-by-module","title":"Functions by Module","text":"Module Function <code>io</code> <code>read_from_path()</code> <code>write_to_path()</code> <code>transfer_table()</code> <code>checks</code> <code>column_exists()</code> <code>columns_exists()</code> <code>is_vaid_spark_type()</code> <code>table_exists()</code> <code>types</code> <code>get_column_types()</code> <code>cast_column_to_type()</code> <code>cast_columns_to_type()</code> <code>map_cast_columns_to_type()</code> <code>keys</code> <code>add_keys_from_columns()</code> <code>add_key_from_columns()</code> <code>scale</code> <code>round_column()</code> <code>round_columns()</code> <code>dimensions</code> <code>get_dims()</code> <code>get_dims_of_tables()</code> <code>columns</code> <code>get_columns()</code> <code>get_columns_by_likeness()</code> <code>rename_columns()</code> <code>reorder_columns()</code> <code>delete_columns()</code> <code>datetime</code> <code>rename_datetime_columns()</code> <code>rename_datetime_column()</code> <code>add_local_datetime_columns()</code> <code>add_local_datetime_column()</code> <code>split_datetime_column()</code> <code>split_datetime_columns()</code> <code>cleaning</code> <code>create_empty_dataframe()</code> <code>keep_first_record_by_columns()</code> <code>convert_dataframe()</code> <code>get_column_values()</code> <code>update_nullability()</code> <code>trim_spaces_from_column()</code> <code>trim_spaces_from_columns()</code> <code>apply_function_to_column()</code> <code>apply_function_to_columns()</code> <code>drop_matching_rows()</code>"},{"location":"code/#testing","title":"Testing","text":"<p>This package is fully tested against:</p> <ol> <li>Unit tests</li> <li>Lint tests</li> <li>MyPy tests</li> <li>Build tests</li> </ol>"},{"location":"code/#latest-code-coverage","title":"Latest Code Coverage","text":""},{"location":"code/checks/","title":"Checks","text":""},{"location":"code/checks/#toolbox_pyspark.checks","title":"toolbox_pyspark.checks","text":"<p>Summary</p> <p>The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe.</p>"},{"location":"code/checks/#column-existence","title":"Column Existence","text":"<p>Summary</p> <p>The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe.</p>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnExistsResult","title":"ColumnExistsResult  <code>dataclass</code>","text":"Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@dataclass\nclass ColumnExistsResult:\n    result: bool\n    missing_cols: str_list\n\n    def __iter__(self):\n        for field in fields(self):\n            yield getattr(self, field.name)\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnExistsResult.__init__","title":"__init__","text":"<pre><code>__init__(result: bool, missing_cols: str_list) -&gt; None\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnExistsResult.result","title":"result  <code>instance-attribute</code>","text":"<pre><code>result: bool\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnExistsResult.missing_cols","title":"missing_cols  <code>instance-attribute</code>","text":"<pre><code>missing_cols: str_list\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnExistsResult.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>def __iter__(self):\n    for field in fields(self):\n        yield getattr(self, field.name)\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.column_exists","title":"column_exists","text":"<pre><code>column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>column.upper()</code>. Default: <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if exists or <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import column_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example1: Column Exists<pre><code>&gt;&gt;&gt; result = column_exists(df, \"a\")\n&gt;&gt;&gt; print(result)\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Column exists.</p> <p>Example 2: Column Missing<pre><code>&gt;&gt;&gt; result = column_exists(df, \"c\")\n&gt;&gt;&gt; print(result)\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py column.upper()`.&lt;br&gt;\n            Default: `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            `#!py True` if exists or `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import column_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example1: Column Exists\"}\n        &gt;&gt;&gt; result = column_exists(df, \"a\")\n        &gt;&gt;&gt; print(result)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Column exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Column Missing\"}\n        &gt;&gt;&gt; result = column_exists(df, \"c\")\n        &gt;&gt;&gt; print(result)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_exists(dataframe, [column], match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.columns_exists","title":"columns_exists","text":"<pre><code>columns_exists(\n    dataframe: psDataFrame,\n    columns: str_collection,\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check</p> required <code>columns</code> <code>Union[str_list, str_tuple, str_set]</code> <p>The columns to check</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>[col.upper() for col in columns]</code>. Default: <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all columns exist or <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import columns_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: Columns exist<pre><code>&gt;&gt;&gt; columns_exists(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: All columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; columns_exists(df, [\"b\", \"d\"])\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: One column is missing.</p> <p>Example 3: All columns missing<pre><code>&gt;&gt;&gt; columns_exists(df, [\"c\", \"d\"])\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: All columns are missing.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef columns_exists(\n    dataframe: psDataFrame,\n    columns: str_collection,\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check\n        columns (Union[str_list, str_tuple, str_set]):\n            The columns to check\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py [col.upper() for col in columns]`.&lt;br&gt;\n            Default: `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            `#!py True` if all columns exist or `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import columns_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Columns exist\"}\n        &gt;&gt;&gt; columns_exists(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: All columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; columns_exists(df, [\"b\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: One column is missing.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: All columns missing\"}\n        &gt;&gt;&gt; columns_exists(df, [\"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: All columns are missing.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_exists(dataframe, columns, match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_column_exists","title":"assert_column_exists","text":"<pre><code>assert_column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>column.upper()</code>. Default: <code>True</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If the <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>ColumnDoesNotExistError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1,2,3,4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_column_exists(df, \"a\")\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Column exists.</p> <p>Example 2: Error raised<pre><code>&gt;&gt;&gt; assert_column_exists(df, \"c\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column \"c\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py column.upper()`.&lt;br&gt;\n            Default: `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If the `#!py column` does not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py ColumnDoesNotExistError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1,2,3,4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_column_exists(df, \"a\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Column exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Error raised\"}\n        &gt;&gt;&gt; assert_column_exists(df, \"c\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column \"c\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    if not column_exists(dataframe, column, match_case):\n        raise ColumnDoesNotExistError(\n            f\"Column '{column}' does not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}.\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_columns_exists","title":"assert_columns_exists","text":"<pre><code>assert_columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check</p> required <code>columns</code> <code>Union[str_list, str_tuple, str_set]</code> <p>The columns to check</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>[col.upper() for col in columns]</code>. Default: <code>True</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>ColumnDoesNotExistError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\"])\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns [\"c\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column \"c\" does not exist.</p> <p>Example 3: Multiple columns missing<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\", \"d\"])\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns [\"c\", \"d\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Columns \"c\" and \"d\" does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check\n        columns (Union[str_list, str_tuple, str_set]):\n            The columns to check\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py [col.upper() for col in columns]`.&lt;br&gt;\n            Default: `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py ColumnDoesNotExistError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns [\"c\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column \"c\" does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Multiple columns missing\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns [\"c\", \"d\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Columns \"c\" and \"d\" does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    columns = [columns] if is_type(columns, str) else columns\n    (exist, missing_cols) = _columns_exists(dataframe, columns, match_case)\n    if not exist:\n        raise ColumnDoesNotExistError(\n            f\"Columns {missing_cols} do not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.warn_column_missing","title":"warn_column_missing","text":"<pre><code>warn_column_missing(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>ColumnDoesNotExistWarning</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import warn_column_missing\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; warn_column_missing(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns exist.</p> <p>Example 2: Warning raised<pre><code>&gt;&gt;&gt; warn_column_missing(df, \"c\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistWarning: Column \"c\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef warn_column_missing(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! summary \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            Defaults to `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py ColumnDoesNotExistWarning` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import warn_column_missing\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; warn_column_missing(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Warning raised\"}\n        &gt;&gt;&gt; warn_column_missing(df, \"c\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistWarning: Column \"c\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    if not column_exists(dataframe, column, match_case):\n        warn(\n            f\"Column '{column}' does not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}.\",\n            ColumnDoesNotExistWarning,\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.warn_columns_missing","title":"warn_columns_missing","text":"<pre><code>warn_columns_missing(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The columns to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>ColumnDoesNotExistWarning</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import warn_columns_missing\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; warn_columns_missing(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; warn_columns_missing(df, [\"b\", \"c\"])\n</code></pre> Terminal<pre><code>ColumnDoesNotExistWarning: Columns [\"c\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column \"c\" does not exist.</p> <p>Example 3: Multiple columns missing<pre><code>&gt;&gt;&gt; warn_columns_missing(df, [\"b\", \"c\", \"d\"])\n</code></pre> Terminal<pre><code>ColumnDoesNotExistWarning: Columns [\"c\", \"d\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Columns \"c\" and \"d\" does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef warn_columns_missing(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! summary \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        columns (Union[str, str_collection]):\n            The columns to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            Defaults to `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py ColumnDoesNotExistWarning` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import warn_columns_missing\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; warn_columns_missing(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; warn_columns_missing(df, [\"b\", \"c\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistWarning: Columns [\"c\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column \"c\" does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Multiple columns missing\"}\n        &gt;&gt;&gt; warn_columns_missing(df, [\"b\", \"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistWarning: Columns [\"c\", \"d\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Columns \"c\" and \"d\" does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    columns = [columns] if is_type(columns, str) else columns\n    (exist, missing_cols) = _columns_exists(dataframe, columns, match_case)\n    if not exist:\n        warn(\n            f\"Columns {missing_cols} do not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}\",\n            ColumnDoesNotExistWarning,\n        )\n</code></pre>"},{"location":"code/checks/#type-checks","title":"Type Checks","text":"<p>Summary</p> <p>The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe.</p>"},{"location":"code/checks/#toolbox_pyspark.checks.is_vaid_spark_type","title":"is_vaid_spark_type","text":"<pre><code>is_vaid_spark_type(datatype: str) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether a given <code>datatype</code> is a correct and valid <code>pyspark</code> data type.</p> <p>Parameters:</p> Name Type Description Default <code>datatype</code> <code>str</code> <p>The name of the data type to check.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>InvalidPySparkDataTypeError</code> <p>If the given <code>datatype</code> is not a valid <code>pyspark</code> data type.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>InvalidPySparkDataTypeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; from toolbox_pyspark.checks import is_vaid_spark_type\n</code></pre> <p>Loop through all valid types<pre><code>&gt;&gt;&gt; type_names = [\"string\", \"char\", \"varchar\", \"binary\", \"boolean\", \"decimal\", \"float\", \"double\", \"byte\", \"short\", \"integer\", \"long\", \"date\", \"timestamp\", \"timestamp_ntz\", \"void\"]\n&gt;&gt;&gt; for type_name in type_names:\n...     is_vaid_spark_type(type_name)\n</code></pre>  Nothing is returned each time. Because they're all valid. <p>Conclusion: They're all valid.</p> <p>Check some invalid types<pre><code>&gt;&gt;&gt; type_names = [\"np.ndarray\", \"pd.DataFrame\", \"dict\"]\n&gt;&gt;&gt; for type_name in type_names:\n...     is_vaid_spark_type(type_name)\n</code></pre> Terminal<pre><code>InvalidPySparkDataTypeError: DataType 'np.ndarray' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> Terminal<pre><code>InvalidPySparkDataTypeError: DataType 'pd.DataFrame' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> Terminal<pre><code>InvalidPySparkDataTypeError: DataType 'dict' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> <p>Conclusion: All of these types are invalid.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef is_vaid_spark_type(datatype: str) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py datatype` is a correct and valid `#!py pyspark` data type.\n\n    Params:\n        datatype (str):\n            The name of the data type to check.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        InvalidPySparkDataTypeError:\n            If the given `#!py datatype` is not a valid `#!py pyspark` data type.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py InvalidPySparkDataTypeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; from toolbox_pyspark.checks import is_vaid_spark_type\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Loop through all valid types\"}\n        &gt;&gt;&gt; type_names = [\"string\", \"char\", \"varchar\", \"binary\", \"boolean\", \"decimal\", \"float\", \"double\", \"byte\", \"short\", \"integer\", \"long\", \"date\", \"timestamp\", \"timestamp_ntz\", \"void\"]\n        &gt;&gt;&gt; for type_name in type_names:\n        ...     is_vaid_spark_type(type_name)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        Nothing is returned each time. Because they're all valid.\n        !!! success \"Conclusion: They're all valid.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Check some invalid types\"}\n        &gt;&gt;&gt; type_names = [\"np.ndarray\", \"pd.DataFrame\", \"dict\"]\n        &gt;&gt;&gt; for type_name in type_names:\n        ...     is_vaid_spark_type(type_name)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        InvalidPySparkDataTypeError: DataType 'np.ndarray' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        InvalidPySparkDataTypeError: DataType 'pd.DataFrame' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        InvalidPySparkDataTypeError: DataType 'dict' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        !!! failure \"Conclusion: All of these types are invalid.\"\n        &lt;/div&gt;\n    \"\"\"\n    return datatype in VALID_PYSPARK_TYPE_NAMES\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_valid_spark_type","title":"assert_valid_spark_type","text":"<pre><code>assert_valid_spark_type(datatype: str) -&gt; None\n</code></pre> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_valid_spark_type(datatype: str) -&gt; None:\n    if not is_vaid_spark_type(datatype):\n        raise InvalidPySparkDataTypeError(\n            f\"DataType '{datatype}' is not valid.\\n\"\n            f\"Must be one of: {VALID_PYSPARK_TYPE_NAMES}\"\n        )\n</code></pre>"},{"location":"code/checks/#column-types","title":"Column Types","text":"<p>Summary</p> <p>The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe.</p>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnsAreTypeResult","title":"ColumnsAreTypeResult  <code>dataclass</code>","text":"Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@dataclass\nclass ColumnsAreTypeResult:\n    result: bool\n    invalid_types: list[tuple[str, str]]\n\n    def __iter__(self):\n        for field in fields(self):\n            yield getattr(self, field.name)\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnsAreTypeResult.__init__","title":"__init__","text":"<pre><code>__init__(\n    result: bool, invalid_types: list[tuple[str, str]]\n) -&gt; None\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnsAreTypeResult.result","title":"result  <code>instance-attribute</code>","text":"<pre><code>result: bool\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnsAreTypeResult.invalid_types","title":"invalid_types  <code>instance-attribute</code>","text":"<pre><code>invalid_types: list[tuple[str, str]]\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.ColumnsAreTypeResult.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>def __iter__(self):\n    for field in fields(self):\n        yield getattr(self, field.name)\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.column_is_type","title":"column_is_type","text":"<pre><code>column_is_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: str,\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> is of a given <code>datatype</code> in <code>dataframe</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>datatype</code> <code>str</code> <p>The data type to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If the <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <code>InvalidPySparkDataTypeError</code> <p>If the <code>datatype</code> is not a valid <code>pyspark</code> data type.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the column is of the given <code>datatype</code>, <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import column_is_type\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: Column is of type<pre><code>&gt;&gt;&gt; column_is_type(df, \"a\", \"integer\")\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Column is the correct type.</p> <p>Example 2: Column is not of type<pre><code>&gt;&gt;&gt; column_is_type(df, \"b\", \"integer\")\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Column is not the correct type.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef column_is_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: str,\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` is of a given `#!py datatype` in `#!py dataframe`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        datatype (str):\n            The data type to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            Defaults to `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If the `#!py column` does not exist within `#!py dataframe.columns`.\n        InvalidPySparkDataTypeError:\n            If the `#!py datatype` is not a valid `#!py pyspark` data type.\n\n    Returns:\n        (bool):\n            `#!py True` if the column is of the given `#!py datatype`, `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import column_is_type\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Column is of type\"}\n        &gt;&gt;&gt; column_is_type(df, \"a\", \"integer\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Column is the correct type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Column is not of type\"}\n        &gt;&gt;&gt; column_is_type(df, \"b\", \"integer\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Column is not the correct type.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_are_type(dataframe, column, datatype, match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.columns_are_type","title":"columns_are_type","text":"<pre><code>columns_are_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    datatype: str,\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether the given <code>columns</code> are of a given <code>datatype</code> in <code>dataframe</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The columns to check.</p> required <code>datatype</code> <code>str</code> <p>The data type to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <code>InvalidPySparkDataTypeError</code> <p>If the <code>datatype</code> is not a valid <code>pyspark</code> data type.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all the columns are of the given <code>datatype</code>, <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import columns_are_type\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1.1, 2.2, 3.3, 4.4],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: Columns are of type<pre><code>&gt;&gt;&gt; columns_are_type(df, [\"a\", \"c\"], \"double\")\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Columns are the correct type.</p> <p>Example 2: Columns are not of type<pre><code>&gt;&gt;&gt; columns_are_type(df, [\"a\", \"b\"], \"double\")\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Columns are not the correct type.</p> <p>Example 3: Single column is of type<pre><code>&gt;&gt;&gt; columns_are_type(df, \"a\", \"integer\")\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Column is the correct type.</p> <p>Example 4: Single column is not of type<pre><code>&gt;&gt;&gt; columns_are_type(df, \"b\", \"integer\")\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Column is not the correct type.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef columns_are_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    datatype: str,\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether the given `#!py columns` are of a given `#!py datatype` in `#!py dataframe`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        columns (Union[str, str_collection]):\n            The columns to check.\n        datatype (str):\n            The data type to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            Defaults to `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n        InvalidPySparkDataTypeError:\n            If the `#!py datatype` is not a valid `#!py pyspark` data type.\n\n    Returns:\n        (bool):\n            `#!py True` if all the columns are of the given `#!py datatype`, `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import columns_are_type\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1.1, 2.2, 3.3, 4.4],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Columns are of type\"}\n        &gt;&gt;&gt; columns_are_type(df, [\"a\", \"c\"], \"double\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Columns are the correct type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Columns are not of type\"}\n        &gt;&gt;&gt; columns_are_type(df, [\"a\", \"b\"], \"double\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Columns are not the correct type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Single column is of type\"}\n        &gt;&gt;&gt; columns_are_type(df, \"a\", \"integer\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Column is the correct type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Single column is not of type\"}\n        &gt;&gt;&gt; columns_are_type(df, \"b\", \"integer\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Column is not the correct type.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_are_type(dataframe, columns, datatype, match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_column_is_type","title":"assert_column_is_type","text":"<pre><code>assert_column_is_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> is of a given <code>datatype</code> in <code>dataframe</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>datatype</code> <code>str</code> <p>The data type to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If the <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <code>InvalidPySparkDataTypeError</code> <p>If the given <code>column</code> is not of the given <code>datatype</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>InvalidPySparkDataTypeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_is_type\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_column_is_type(df, \"a\", \"integer\")\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Column is of type.</p> <p>Example 2: Error raised<pre><code>&gt;&gt;&gt; assert_column_is_type(df, \"b\", \"integer\")\n</code></pre> Terminal<pre><code>InvalidPySparkDataTypeError: Column 'b' is not of type 'integer'.\n</code></pre> <p>Conclusion: Column is not of type.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_column_is_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: str,\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` is of a given `#!py datatype` in `#!py dataframe`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        datatype (str):\n            The data type to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            Defaults to `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If the `#!py column` does not exist within `#!py dataframe.columns`.\n        InvalidPySparkDataTypeError:\n            If the given `#!py column` is not of the given `#!py datatype`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py InvalidPySparkDataTypeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_is_type\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_column_is_type(df, \"a\", \"integer\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Column is of type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Error raised\"}\n        &gt;&gt;&gt; assert_column_is_type(df, \"b\", \"integer\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        InvalidPySparkDataTypeError: Column 'b' is not of type 'integer'.\n        ```\n        !!! failure \"Conclusion: Column is not of type.\"\n        &lt;/div&gt;\n    \"\"\"\n    result, invalid_types = _columns_are_type(dataframe, column, datatype, match_case)\n    if not result:\n        raise InvalidPySparkDataTypeError(\n            f\"Column '{column}' is type '{invalid_types[0][1]}', \"\n            f\"which is not the required type: '{datatype}'.\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_columns_are_type","title":"assert_columns_are_type","text":"<pre><code>assert_columns_are_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    datatype: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether the given <code>columns</code> are of a given <code>datatype</code> in <code>dataframe</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The columns to check.</p> required <code>datatype</code> <code>str</code> <p>The data type to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. Defaults to <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <code>InvalidPySparkDataTypeError</code> <p>If any of the given <code>columns</code> are not of the given <code>datatype</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>InvalidPySparkDataTypeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_are_type\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1.1, 2.2, 3.3, 4.4],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_columns_are_type(df, [\"a\", \"c\"], \"double\")\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns are of type.</p> <p>Example 2: Error raised<pre><code>&gt;&gt;&gt; assert_columns_are_type(df, [\"a\", \"b\"], \"double\")\n</code></pre> Terminal<pre><code>InvalidPySparkDataTypeError: Columns ['a', 'b'] are types ['int', 'string'], which are not the required type: 'double'.\n</code></pre> <p>Conclusion: Columns are not of type.</p> <p>Example 3: Single column is of type<pre><code>&gt;&gt;&gt; assert_columns_are_type(df, \"a\", \"integer\")\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Column is of type.</p> <p>Example 4: Single column is not of type<pre><code>&gt;&gt;&gt; assert_columns_are_type(df, \"b\", \"integer\")\n</code></pre> Terminal<pre><code>InvalidPySparkDataTypeError: Columns ['b'] are types ['string'], which are not the required type: 'integer'.\n</code></pre> <p>Conclusion: Column is not of type.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_columns_are_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    datatype: str,\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether the given `#!py columns` are of a given `#!py datatype` in `#!py dataframe`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        columns (Union[str, str_collection]):\n            The columns to check.\n        datatype (str):\n            The data type to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            Defaults to `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n        InvalidPySparkDataTypeError:\n            If any of the given `#!py columns` are not of the given `#!py datatype`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py InvalidPySparkDataTypeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_are_type\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1.1, 2.2, 3.3, 4.4],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_columns_are_type(df, [\"a\", \"c\"], \"double\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns are of type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Error raised\"}\n        &gt;&gt;&gt; assert_columns_are_type(df, [\"a\", \"b\"], \"double\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        InvalidPySparkDataTypeError: Columns ['a', 'b'] are types ['int', 'string'], which are not the required type: 'double'.\n        ```\n        !!! failure \"Conclusion: Columns are not of type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Single column is of type\"}\n        &gt;&gt;&gt; assert_columns_are_type(df, \"a\", \"integer\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Column is of type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Single column is not of type\"}\n        &gt;&gt;&gt; assert_columns_are_type(df, \"b\", \"integer\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        InvalidPySparkDataTypeError: Columns ['b'] are types ['string'], which are not the required type: 'integer'.\n        ```\n        !!! failure \"Conclusion: Column is not of type.\"\n        &lt;/div&gt;\n    \"\"\"\n    result, invalid_types = _columns_are_type(dataframe, columns, datatype, match_case)\n    if not result:\n        raise InvalidPySparkDataTypeError(\n            f\"Columns {[col for col, _ in invalid_types]} are types {[typ for _, typ in invalid_types]}, \"\n            f\"which are not the required type: '{datatype}'.\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.warn_column_invalid_type","title":"warn_column_invalid_type","text":"<pre><code>warn_column_invalid_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef warn_column_invalid_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: str,\n    match_case: bool = False,\n) -&gt; None:\n    result, invalid_types = _columns_are_type(dataframe, column, datatype, match_case)\n    if not result:\n        warn(\n            f\"Column '{column}' is type '{invalid_types[0][1]}', \"\n            f\"which is not the required type: '{datatype}'.\",\n            InvalidPySparkDataTypeWarning,\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.warn_columns_invalid_type","title":"warn_columns_invalid_type","text":"<pre><code>warn_columns_invalid_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    datatype: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef warn_columns_invalid_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    datatype: str,\n    match_case: bool = False,\n) -&gt; None:\n    result, invalid_types = _columns_are_type(dataframe, columns, datatype, match_case)\n    if not result:\n        warn(\n            f\"Columns {[col for col, _ in invalid_types]} are types {[typ for _, typ in invalid_types]}, \"\n            f\"which are not the required type: '{datatype}'.\",\n            InvalidPySparkDataTypeWarning,\n        )\n</code></pre>"},{"location":"code/checks/#table-existence","title":"Table Existence","text":"<p>Summary</p> <p>The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe.</p>"},{"location":"code/checks/#toolbox_pyspark.checks.table_exists","title":"table_exists","text":"<pre><code>table_exists(\n    name: str,\n    path: str,\n    data_format: str,\n    spark_session: SparkSession,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Will try to read <code>table</code> from <code>path</code> using <code>format</code>, and if successful will return <code>True</code> otherwise <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to check exists.</p> required <code>path</code> <code>str</code> <p>The directory where the table should be existing.</p> required <code>data_format</code> <code>str</code> <p>The format of the table to try checking.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The <code>spark</code> session to use for the importing.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Returns <code>True</code> if the table exists, <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Constants\n&gt;&gt;&gt; write_name = \"test_df\"\n&gt;&gt;&gt; write_path = f\"./test\"\n&gt;&gt;&gt; write_format = \"parquet\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write data\n&gt;&gt;&gt; write_to_path(df, f\"{write_name}.{write_format}\", write_path)\n</code></pre> <p>Example 1: Table exists<pre><code>&gt;&gt;&gt; table_exists(\"test_df.parquet\", \"./test\", \"parquet\", spark)\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Table exists.</p> <p>Example 2: Table does not exist<pre><code>&gt;&gt;&gt; table_exists(\"bad_table_name.parquet\", \"./test\", \"parquet\", spark)\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Table does not exist.</p> See Also <ul> <li><code>toolbox_pyspark.io.read_from_path()</code></li> </ul> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef table_exists(\n    name: str,\n    path: str,\n    data_format: str,\n    spark_session: SparkSession,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Will try to read `#!py table` from `#!py path` using `#!py format`, and if successful will return `#!py True` otherwise `#!py False`.\n\n    Params:\n        name (str):\n            The name of the table to check exists.\n        path (str):\n            The directory where the table should be existing.\n        data_format (str):\n            The format of the table to try checking.\n        spark_session (SparkSession):\n            The `#!py spark` session to use for the importing.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            Returns `#!py True` if the table exists, `False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Constants\n        &gt;&gt;&gt; write_name = \"test_df\"\n        &gt;&gt;&gt; write_path = f\"./test\"\n        &gt;&gt;&gt; write_format = \"parquet\"\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Write data\n        &gt;&gt;&gt; write_to_path(df, f\"{write_name}.{write_format}\", write_path)\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Table exists\"}\n        &gt;&gt;&gt; table_exists(\"test_df.parquet\", \"./test\", \"parquet\", spark)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Table exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Table does not exist\"}\n        &gt;&gt;&gt; table_exists(\"bad_table_name.parquet\", \"./test\", \"parquet\", spark)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Table does not exist.\"\n        &lt;/div&gt;\n\n    ???+ tip \"See Also\"\n        - [`toolbox_pyspark.io.read_from_path()`][toolbox_pyspark.io.read_from_path]\n    \"\"\"\n    try:\n        _ = read_from_path(\n            name=name,\n            path=path,\n            data_format=data_format,\n            spark_session=spark_session,\n        )\n    except Exception:\n        return False\n    return True\n</code></pre>"},{"location":"code/cleaning/","title":"Cleaning","text":""},{"location":"code/cleaning/#toolbox_pyspark.cleaning","title":"toolbox_pyspark.cleaning","text":"<p>Summary</p> <p>The <code>cleaning</code> module is used to clean, fix, and fetch various aspects on a given DataFrame.</p>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.create_empty_dataframe","title":"create_empty_dataframe","text":"<pre><code>create_empty_dataframe(\n    spark_session: SparkSession,\n) -&gt; psDataFrame\n</code></pre> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef create_empty_dataframe(spark_session: SparkSession) -&gt; psDataFrame:\n    return spark_session.createDataFrame([], T.StructType([]))\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.keep_first_record_by_columns","title":"keep_first_record_by_columns","text":"<pre><code>keep_first_record_by_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given Spark <code>DataFrame</code>, keep the first record given by the column(s) specified in <code>columns</code>.</p> Details <p>The necessity for this function arose when we needed to perform a <code>distinct()</code> function for a given <code>DataFrame</code>; however, we still wanted to retain data provided in the other columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame that you want to filter.</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The single or multiple columns by which you want to extract the distinct values from.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated dataframe, retaining only the first unique set of records from the columns specified in <code>columns</code>.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import keep_first_record_by_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 2, 2],\n...             \"d\": [1, 2, 2, 2],\n...             \"e\": [1, 1, 2, 3],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 1 | 1 |\n| 2 | b | 1 | 2 | 1 |\n| 3 | c | 2 | 2 | 2 |\n| 4 | d | 2 | 2 | 3 |\n+---+---+---+---+---+\n</code></pre> </p> <p>Example 1: Distinct by the `c` column<pre><code>&gt;&gt;&gt; keep_first_record_by_columns(df, \"c\").show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 1 | 1 |\n| 3 | c | 2 | 2 | 2 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully kept first records by the <code>c</code> column.</p> <p>Example 2: Distinct by the `d` column<pre><code>&gt;&gt;&gt; keep_first_record_by_columns(df, \"d\").show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 1 | 1 |\n| 2 | b | 1 | 2 | 1 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully kept first records by the <code>d</code> column.</p> <p>Example 3: Distinct by the `e` column<pre><code>&gt;&gt;&gt; keep_first_record_by_columns(df, \"e\").show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 1 | 1 |\n| 3 | c | 2 | 2 | 2 |\n| 4 | d | 2 | 2 | 3 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully kept first records by the <code>e</code> column.</p> <p>Example 4: Distinct by the `c` &amp; `d` columns<pre><code>&gt;&gt;&gt; keep_first_record_by_columns(df, [\"c\", \"d\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 1 | 1 |\n| 2 | b | 1 | 2 | 1 |\n| 3 | c | 2 | 2 | 2 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully kept first records by the <code>c</code> &amp; <code>d</code> columns.</p> <p>Example 5: Distinct by the `c` &amp; `e` columns<pre><code>&gt;&gt;&gt; keep_first_record_by_columns(df, [\"c\", \"e\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 1 | 1 |\n| 3 | c | 2 | 2 | 2 |\n| 4 | d | 2 | 2 | 3 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully kept first records by the <code>c</code> &amp; <code>e</code> columns.</p> <p>Example 6: Distinct by the `d` &amp; `e` columns<pre><code>&gt;&gt;&gt; keep_first_record_by_columns(df, [\"d\", \"e\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 1 | 1 |\n| 2 | b | 1 | 2 | 1 |\n| 3 | c | 2 | 2 | 2 |\n| 4 | d | 2 | 2 | 3 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully kept first records by the <code>d</code> &amp; <code>e</code> columns.</p> <p>Example 7: Distinct by the `c`, `d` &amp; `e` columns<pre><code>&gt;&gt;&gt; keep_first_record_by_columns(df, [\"c\", \"d\", \"e\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 1 | 1 |\n| 2 | b | 1 | 2 | 1 |\n| 3 | c | 2 | 2 | 2 |\n| 4 | d | 2 | 2 | 3 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully kept first records by the <code>c</code>, <code>d</code> &amp; <code>e</code> columns.</p> <p>Conclusion: Failure.</p> <p>Example 8: Column missing<pre><code>&gt;&gt;&gt; keep_first_record_by_columns(df, \"f\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column 'f' does not exist in the DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\", \"e\"]\n</code></pre> <p>Conclusion: Column missing.</p> Notes <p>The way this process will retain only the first record in the given <code>columns</code> is by:</p> <ol> <li>Add a new column called <code>RowNum</code><ol> <li>This <code>RowNum</code> column uses the SparkSQL function <code>ROW_NUMBER()</code></li> <li>The window-function <code>OVER</code> clause will then:<ol> <li><code>PARTITION BY</code> the <code>columns</code>,</li> <li><code>ORDER BY</code> the <code>columns</code>.</li> </ol> </li> </ol> </li> <li>Filter so that <code>RowNum=1</code>.</li> <li>Drop the <code>RowNum</code> column.</li> </ol> See Also <ul> <li><code>toolbox_pyspark.checks.assert_columns_exists()</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef keep_first_record_by_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given Spark `#!py DataFrame`, keep the first record given by the column(s) specified in `#!py columns`.\n\n    ???+ abstract \"Details\"\n        The necessity for this function arose when we needed to perform a `#!py distinct()` function for a given `#!py DataFrame`; however, we still wanted to retain data provided in the other columns.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame that you want to filter.\n        columns (Union[str, str_collection]):\n            The single or multiple columns by which you want to extract the distinct values from.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated dataframe, retaining only the first unique set of records from the columns specified in `#!py columns`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import keep_first_record_by_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 2, 2],\n        ...             \"d\": [1, 2, 2, 2],\n        ...             \"e\": [1, 1, 2, 3],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 1 | 1 |\n        | 2 | b | 1 | 2 | 1 |\n        | 3 | c | 2 | 2 | 2 |\n        | 4 | d | 2 | 2 | 3 |\n        +---+---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Distinct by the `c` column\"}\n        &gt;&gt;&gt; keep_first_record_by_columns(df, \"c\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 1 | 1 |\n        | 3 | c | 2 | 2 | 2 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully kept first records by the `c` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Distinct by the `d` column\"}\n        &gt;&gt;&gt; keep_first_record_by_columns(df, \"d\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 1 | 1 |\n        | 2 | b | 1 | 2 | 1 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully kept first records by the `d` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Distinct by the `e` column\"}\n        &gt;&gt;&gt; keep_first_record_by_columns(df, \"e\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 1 | 1 |\n        | 3 | c | 2 | 2 | 2 |\n        | 4 | d | 2 | 2 | 3 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully kept first records by the `e` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Distinct by the `c` &amp; `d` columns\"}\n        &gt;&gt;&gt; keep_first_record_by_columns(df, [\"c\", \"d\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 1 | 1 |\n        | 2 | b | 1 | 2 | 1 |\n        | 3 | c | 2 | 2 | 2 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully kept first records by the `c` &amp; `d` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Distinct by the `c` &amp; `e` columns\"}\n        &gt;&gt;&gt; keep_first_record_by_columns(df, [\"c\", \"e\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 1 | 1 |\n        | 3 | c | 2 | 2 | 2 |\n        | 4 | d | 2 | 2 | 3 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully kept first records by the `c` &amp; `e` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Distinct by the `d` &amp; `e` columns\"}\n        &gt;&gt;&gt; keep_first_record_by_columns(df, [\"d\", \"e\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 1 | 1 |\n        | 2 | b | 1 | 2 | 1 |\n        | 3 | c | 2 | 2 | 2 |\n        | 4 | d | 2 | 2 | 3 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully kept first records by the `d` &amp; `e` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: Distinct by the `c`, `d` &amp; `e` columns\"}\n        &gt;&gt;&gt; keep_first_record_by_columns(df, [\"c\", \"d\", \"e\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 1 | 1 |\n        | 2 | b | 1 | 2 | 1 |\n        | 3 | c | 2 | 2 | 2 |\n        | 4 | d | 2 | 2 | 3 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully kept first records by the `c`, `d` &amp; `e` columns.\"\n        !!! failure \"Conclusion: Failure.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: Column missing\"}\n        &gt;&gt;&gt; keep_first_record_by_columns(df, \"f\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column 'f' does not exist in the DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\", \"e\"]\n        ```\n        !!! failure \"Conclusion: Column missing.\"\n        &lt;/div&gt;\n\n    ??? info \"Notes\"\n        The way this process will retain only the first record in the given `#!py columns` is by:\n\n        1. Add a new column called `RowNum`\n            1. This `RowNum` column uses the SparkSQL function `#!sql ROW_NUMBER()`\n            1. The window-function `#!sql OVER` clause will then:\n                1. `#!sql PARTITION BY` the `#!py columns`,\n                1. `#!sql ORDER BY` the `#!py columns`.\n        1. Filter so that `#!sql RowNum=1`.\n        1. Drop the `#!py RowNum` column.\n\n    ???+ tip \"See Also\"\n        - [`toolbox_pyspark.checks.assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n    \"\"\"\n    columns = [columns] if is_type(columns, str) else columns\n    assert_columns_exists(dataframe, columns)\n    return (\n        dataframe.withColumn(\n            colName=\"RowNum\",\n            col=F.expr(\n                f\"\"\"\n                ROW_NUMBER()\n                OVER\n                (\n                    PARTITION BY {','.join(columns)}\n                    ORDER BY {','.join(columns)}\n                )\n                \"\"\"\n            ),\n        )\n        .where(\"RowNum=1\")\n        .drop(\"RowNum\")\n    )\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.convert_dataframe","title":"convert_dataframe","text":"<pre><code>convert_dataframe(\n    dataframe: psDataFrame,\n    return_type: Union[\n        LITERAL_PYSPARK_DATAFRAME_NAMES,\n        LITERAL_PANDAS_DATAFRAME_NAMES,\n        LITERAL_NUMPY_ARRAY_NAMES,\n        LITERAL_LIST_OBJECT_NAMES,\n        str,\n    ] = \"pd\",\n) -&gt; Optional[\n    Union[psDataFrame, pdDataFrame, npArray, list]\n]\n</code></pre> <p>Summary</p> <p>Convert a PySpark DataFrame to the desired return type.</p> Details <p>This function converts a PySpark DataFrame to one of the supported return types, including:</p> <p>PySpark DataFrame:</p> <ul> <li><code>\"spark.DataFrame\"</code></li> <li><code>\"pyspark.DataFrame\"</code></li> <li><code>\"pyspark\"</code></li> <li><code>\"spark\"</code></li> <li><code>\"ps.DataFrame\"</code></li> <li><code>\"ps.df\"</code></li> <li><code>\"psdf\"</code></li> <li><code>\"psDataFrame\"</code></li> <li><code>\"psDF\"</code></li> <li><code>\"ps\"</code></li> </ul> <p>Pandas DataFrame:</p> <ul> <li><code>\"pandas.DataFrame\"</code></li> <li><code>\"pandas\"</code></li> <li><code>\"pd.DataFrame\"</code></li> <li><code>\"pd.df\"</code></li> <li><code>\"pddf\"</code></li> <li><code>\"pdDataFrame\"</code></li> <li><code>\"pdDF\"</code></li> <li><code>\"pd\"</code></li> </ul> <p>NumPy array:</p> <ul> <li><code>\"numpy.array\"</code></li> <li><code>\"np.array\"</code></li> <li><code>\"np\"</code></li> <li><code>\"numpy\"</code></li> <li><code>\"nparr\"</code></li> <li><code>\"npa\"</code></li> <li><code>\"np.arr\"</code></li> <li><code>\"np.a\"</code></li> </ul> <p>Python list:</p> <ul> <li><code>\"list\"</code></li> <li><code>\"lst\"</code></li> <li><code>\"l\"</code></li> <li><code>\"flat_list\"</code></li> <li><code>\"flatten_list\"</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The PySpark DataFrame to be converted.</p> required <code>return_type</code> <code>Union[LITERAL_LIST_OBJECT_NAMES, LITERAL_PANDAS_DATAFRAME_NAMES, LITERAL_PYSPARK_DATAFRAME_NAMES, LITERAL_NUMPY_ARRAY_NAMES, str]</code> <p>The desired return type. Options:</p> <ul> <li><code>\"ps\"</code>: Return the PySpark DataFrame.</li> <li><code>\"pd\"</code>: Return a Pandas DataFrame.</li> <li><code>\"np\"</code>: Return a NumPy array.</li> <li><code>\"list\"</code>: Return a Python list.</li> <li><code>\"list_flat\"</code>: Return a flat Python list (1D).</li> </ul> <p>Defaults to <code>\"pd\"</code> (Pandas DataFrame).</p> <code>'pd'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ValueError</code> <p>If any of the values parsed to <code>return_type</code> are not valid options.</p> <p>Returns:</p> Type Description <code>Optional[Union[DataFrame, DataFrame, ndarray, list]]</code> <p>The converted data in the specified return type.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import convert_dataframe\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pdDataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+\n| a | b |\n+---+---+\n| 0 | a |\n| 1 | b |\n| 2 | c |\n| 3 | d |\n+---+---+\n</code></pre> <p>Example 1: Convert to PySpark<pre><code>&gt;&gt;&gt; new_df = convert_dataframe(df, \"ps\")\n&gt;&gt;&gt; print(type(new_df))\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>&lt;class 'pyspark.sql.dataframe.DataFrame'&gt;\n</code></pre> Terminal<pre><code>+---+---+\n| a | b |\n+---+---+\n| 0 | a |\n| 1 | b |\n| 2 | c |\n| 3 | d |\n+---+---+\n</code></pre> <p>Conclusion: Successfully converted to PySpark.</p> <p>Example 2: Convert to Pandas<pre><code>&gt;&gt;&gt; new_df = convert_dataframe(df, \"pd\")\n&gt;&gt;&gt; print(type(new_df))\n&gt;&gt;&gt; print(new_df)\n</code></pre> Terminal<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> Terminal<pre><code>   a  b\n0  0  a\n1  1  b\n2  2  c\n3  3  d\n</code></pre> <p>Conclusion: Successfully converted to Pandas.</p> <p>Example 3: Convert to Numpy<pre><code>&gt;&gt;&gt; new_df = convert_dataframe(df, \"np\")\n&gt;&gt;&gt; print(type(new_df))\n&gt;&gt;&gt; print(new_df)\n</code></pre> Terminal<pre><code>&lt;class 'numpy.ndarray'&gt;\n</code></pre> Terminal<pre><code>[[0 \"a\"]\n [1 \"b\"]\n [2 \"c\"]\n [3 \"d\"]]\n</code></pre> <p>Conclusion: Successfully converted to Numpy.</p> <p>Example 4: List<pre><code>&gt;&gt;&gt; new_df = convert_dataframe(df, \"list\")\n&gt;&gt;&gt; print(type(new_df))\n&gt;&gt;&gt; print(new_df)\n</code></pre> Terminal<pre><code>&lt;class 'list'&gt;\n</code></pre> Terminal<pre><code>[\n    [0, \"a\"],\n    [1, \"b\"],\n    [2, \"c\"],\n    [3, \"d\"],\n]\n</code></pre> <p>Conclusion: Successfully converted to List.</p> <p>Example 5: Convert to single column as list<pre><code>&gt;&gt;&gt; new_df = convert_dataframe(df.select(\"b\"), \"flat_list\")\n&gt;&gt;&gt; print(type(new_df))\n&gt;&gt;&gt; print(new_df)\n</code></pre> Terminal<pre><code>&lt;class 'list'&gt;\n</code></pre> Terminal<pre><code>[\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Successfully converted to flat List.</p> <p>Example 6: Invalid return type<pre><code>&gt;&gt;&gt; convert_dataframe(df, \"invalid\")\n</code></pre> Terminal<pre><code>ValueError: Unknown return type: 'invalid'.\nMust be one of: ['pd', 'ps', 'np', 'list'].\nFor more info, check the `constants` module.\n</code></pre> <p>Conclusion: Invalid return type.</p> See Also <ul> <li><code>toolbox_pyspark.constants</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef convert_dataframe(\n    dataframe: psDataFrame,\n    return_type: Union[\n        LITERAL_PYSPARK_DATAFRAME_NAMES,\n        LITERAL_PANDAS_DATAFRAME_NAMES,\n        LITERAL_NUMPY_ARRAY_NAMES,\n        LITERAL_LIST_OBJECT_NAMES,\n        str,\n    ] = \"pd\",\n) -&gt; Optional[Union[psDataFrame, pdDataFrame, npArray, list]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Convert a PySpark DataFrame to the desired return type.\n\n    ???+ abstract \"Details\"\n        This function converts a PySpark DataFrame to one of the supported return types, including:\n\n        PySpark DataFrame:\n\n        &lt;div class=\"mdx-four-columns\" markdown&gt;\n\n        - `#!py \"spark.DataFrame\"`\n        - `#!py \"pyspark.DataFrame\"`\n        - `#!py \"pyspark\"`\n        - `#!py \"spark\"`\n        - `#!py \"ps.DataFrame\"`\n        - `#!py \"ps.df\"`\n        - `#!py \"psdf\"`\n        - `#!py \"psDataFrame\"`\n        - `#!py \"psDF\"`\n        - `#!py \"ps\"`\n\n        &lt;/div&gt;\n\n        Pandas DataFrame:\n\n        &lt;div class=\"mdx-four-columns\" markdown&gt;\n\n        - `#!py \"pandas.DataFrame\"`\n        - `#!py \"pandas\"`\n        - `#!py \"pd.DataFrame\"`\n        - `#!py \"pd.df\"`\n        - `#!py \"pddf\"`\n        - `#!py \"pdDataFrame\"`\n        - `#!py \"pdDF\"`\n        - `#!py \"pd\"`\n\n        &lt;/div&gt;\n\n        NumPy array:\n\n        &lt;div class=\"mdx-four-columns\" markdown&gt;\n\n        - `#!py \"numpy.array\"`\n        - `#!py \"np.array\"`\n        - `#!py \"np\"`\n        - `#!py \"numpy\"`\n        - `#!py \"nparr\"`\n        - `#!py \"npa\"`\n        - `#!py \"np.arr\"`\n        - `#!py \"np.a\"`\n\n        &lt;/div&gt;\n\n        Python list:\n\n        &lt;div class=\"mdx-four-columns\" markdown&gt;\n\n        - `#!py \"list\"`\n        - `#!py \"lst\"`\n        - `#!py \"l\"`\n        - `#!py \"flat_list\"`\n        - `#!py \"flatten_list\"`\n\n        &lt;/div&gt;\n\n    Params:\n        dataframe (psDataFrame):\n            The PySpark DataFrame to be converted.\n        return_type (Union[LITERAL_LIST_OBJECT_NAMES, LITERAL_PANDAS_DATAFRAME_NAMES, LITERAL_PYSPARK_DATAFRAME_NAMES, LITERAL_NUMPY_ARRAY_NAMES, str], optional):\n            The desired return type.&lt;br&gt;\n            Options:\n\n            - `#!py \"ps\"`: Return the PySpark DataFrame.\n            - `#!py \"pd\"`: Return a Pandas DataFrame.\n            - `#!py \"np\"`: Return a NumPy array.\n            - `#!py \"list\"`: Return a Python list.\n            - `#!py \"list_flat\"`: Return a flat Python list (1D).\n\n            Defaults to `#!py \"pd\"` (Pandas DataFrame).\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ValueError:\n            If any of the values parsed to `return_type` are not valid options.\n\n    Returns:\n        (Optional[Union[psDataFrame, pdDataFrame, npArray, list]]):\n            The converted data in the specified return type.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import convert_dataframe\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pdDataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+\n        | a | b |\n        +---+---+\n        | 0 | a |\n        | 1 | b |\n        | 2 | c |\n        | 3 | d |\n        +---+---+\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Convert to PySpark\"}\n        &gt;&gt;&gt; new_df = convert_dataframe(df, \"ps\")\n        &gt;&gt;&gt; print(type(new_df))\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        &lt;class 'pyspark.sql.dataframe.DataFrame'&gt;\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+\n        | a | b |\n        +---+---+\n        | 0 | a |\n        | 1 | b |\n        | 2 | c |\n        | 3 | d |\n        +---+---+\n        ```\n        !!! success \"Conclusion: Successfully converted to PySpark.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Convert to Pandas\"}\n        &gt;&gt;&gt; new_df = convert_dataframe(df, \"pd\")\n        &gt;&gt;&gt; print(type(new_df))\n        &gt;&gt;&gt; print(new_df)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        &lt;class 'pandas.core.frame.DataFrame'&gt;\n        ```\n        ```{.txt .text title=\"Terminal\"}\n           a  b\n        0  0  a\n        1  1  b\n        2  2  c\n        3  3  d\n        ```\n        !!! success \"Conclusion: Successfully converted to Pandas.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Convert to Numpy\"}\n        &gt;&gt;&gt; new_df = convert_dataframe(df, \"np\")\n        &gt;&gt;&gt; print(type(new_df))\n        &gt;&gt;&gt; print(new_df)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        &lt;class 'numpy.ndarray'&gt;\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        [[0 \"a\"]\n         [1 \"b\"]\n         [2 \"c\"]\n         [3 \"d\"]]\n        ```\n        !!! success \"Conclusion: Successfully converted to Numpy.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: List\"}\n        &gt;&gt;&gt; new_df = convert_dataframe(df, \"list\")\n        &gt;&gt;&gt; print(type(new_df))\n        &gt;&gt;&gt; print(new_df)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        &lt;class 'list'&gt;\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        [\n            [0, \"a\"],\n            [1, \"b\"],\n            [2, \"c\"],\n            [3, \"d\"],\n        ]\n        ```\n        !!! success \"Conclusion: Successfully converted to List.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Convert to single column as list\"}\n        &gt;&gt;&gt; new_df = convert_dataframe(df.select(\"b\"), \"flat_list\")\n        &gt;&gt;&gt; print(type(new_df))\n        &gt;&gt;&gt; print(new_df)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        &lt;class 'list'&gt;\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Successfully converted to flat List.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Invalid return type\"}\n        &gt;&gt;&gt; convert_dataframe(df, \"invalid\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ValueError: Unknown return type: 'invalid'.\n        Must be one of: ['pd', 'ps', 'np', 'list'].\n        For more info, check the `constants` module.\n        ```\n        !!! failure \"Conclusion: Invalid return type.\"\n        &lt;/div&gt;\n\n    ???+ tip \"See Also\"\n        - [`toolbox_pyspark.constants`][toolbox_pyspark.constants]\n    \"\"\"\n    if return_type in VALID_PYSPARK_DATAFRAME_NAMES:\n        return dataframe\n    elif return_type in VALID_PANDAS_DATAFRAME_NAMES:\n        return dataframe.toPandas()\n    elif return_type in VALID_NUMPY_ARRAY_NAMES:\n        return dataframe.toPandas().values  # type:ignore\n    elif return_type in VALID_LIST_OBJECT_NAMES:\n        if \"flat\" in return_type:\n            return flatten(dataframe.toPandas().values.tolist())  # type:ignore\n        else:\n            return dataframe.toPandas().values.tolist()  # type:ignore\n    else:\n        raise ValueError(\n            f\"Unknown return type: '{return_type}'.\\n\"\n            f\"Must be one of: {['pd', 'ps', 'np', 'list']}.\\n\"\n            f\"For more info, check the `constants` module.\"\n        )\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.get_column_values","title":"get_column_values","text":"<pre><code>get_column_values(\n    dataframe: psDataFrame,\n    column: str,\n    distinct: bool = True,\n    return_type: Union[\n        LITERAL_PYSPARK_DATAFRAME_NAMES,\n        LITERAL_PANDAS_DATAFRAME_NAMES,\n        LITERAL_NUMPY_ARRAY_NAMES,\n        LITERAL_LIST_OBJECT_NAMES,\n        str,\n    ] = \"pd\",\n) -&gt; Optional[\n    Union[psDataFrame, pdDataFrame, npArray, list]\n]\n</code></pre> <p>Summary</p> <p>Extract and return unique values from a specified column of a PySpark DataFrame.</p> Details <p>This function retrieves the unique values from a specified column of a PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>column</code> <code>str</code> <p>The name of the column from which to extract unique values.</p> required <code>distinct</code> <code>bool</code> <p>If <code>True</code>, return distinct (unique) values. Defaults to <code>True</code>.</p> <code>True</code> <code>return_type</code> <code>Union[LITERAL_LIST_OBJECT_NAMES, LITERAL_PANDAS_DATAFRAME_NAMES, LITERAL_PYSPARK_DATAFRAME_NAMES, LITERAL_NUMPY_ARRAY_NAMES, str]</code> <p>The desired return type. Options:</p> <ul> <li><code>\"ps\"</code>: Return the result as a PySpark DataFrame.</li> <li><code>\"pd\"</code>: Return the result as a Pandas DataFrame.</li> <li><code>\"np\"</code>: Return the result as a NumPy array.</li> <li><code>\"list\"</code>: Return the result as a Python list.</li> </ul> <p>Defaults to <code>\"pd\"</code>.</p> <code>'pd'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ValueError</code> <p>If any of the values parsed to <code>return_type</code> are not valid options.</p> <code>ColumnDoesNotExistError</code> <p>If the <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>Optional[Union[DataFrame, DataFrame, ndarray, list]]</code> <p>The values from the given column, in the desired type.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import get_column_values\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Default params<pre><code>&gt;&gt;&gt; values = get_column_values(df, \"c\")\n&gt;&gt;&gt; print(type(values))\n&gt;&gt;&gt; print(values)\n</code></pre> Terminal<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> Terminal<pre><code>  c\n0 c\n</code></pre> <p>Conclusion: Successfully extracted distinct values from the <code>c</code> column, and converted to Pandas.</p> <p>Example 2: Not distinct<pre><code>&gt;&gt;&gt; values = get_column_values(df, \"c\", False)\n&gt;&gt;&gt; print(type(values))\n&gt;&gt;&gt; print(values)\n</code></pre> Terminal<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> Terminal<pre><code>  c\n0 c\n1 c\n2 c\n3 c\n</code></pre> <p>Conclusion: Successfully extracted values from the <code>c</code> column, and converted to Pandas.</p> <p>Example 3: Flat list<pre><code>&gt;&gt;&gt; values = get_column_values(df, \"c\", False, \"flat_list\")\n&gt;&gt;&gt; print(type(values))\n&gt;&gt;&gt; print(values)\n</code></pre> Terminal<pre><code>&lt;class 'list'&gt;\n</code></pre> Terminal<pre><code>[\"c\", \"c\", \"c\", \"c\"]\n</code></pre> <p>Conclusion: Successfully extracted values from the <code>c</code> column, and converted to flat List.</p> <p>Example 4: Invalid return type<pre><code>&gt;&gt;&gt; get_column_values(df, \"c\", return_type=\"invalid\")\n</code></pre> Terminal<pre><code>ValueError: Unknown return type: 'invalid'.\nMust be one of: ['pd', 'ps', 'np', 'list'].\nFor more info, check the `constants` module.\n</code></pre> <p>Conclusion: Invalid return type.</p> See Also <ul> <li><code>toolbox_pyspark.cleaning.convert_dataframe()</code></li> <li><code>toolbox_pyspark.constants</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef get_column_values(\n    dataframe: psDataFrame,\n    column: str,\n    distinct: bool = True,\n    return_type: Union[\n        LITERAL_PYSPARK_DATAFRAME_NAMES,\n        LITERAL_PANDAS_DATAFRAME_NAMES,\n        LITERAL_NUMPY_ARRAY_NAMES,\n        LITERAL_LIST_OBJECT_NAMES,\n        str,\n    ] = \"pd\",\n) -&gt; Optional[Union[psDataFrame, pdDataFrame, npArray, list]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Extract and return unique values from a specified column of a PySpark DataFrame.\n\n    ???+ abstract \"Details\"\n        This function retrieves the unique values from a specified column of a PySpark DataFrame.\n\n    Params:\n        dataframe (psDataFrame):\n            The input PySpark DataFrame.\n        column (str):\n            The name of the column from which to extract unique values.\n        distinct (bool, optional):\n            If `#!py True`, return distinct (unique) values.&lt;br&gt;\n            Defaults to `#!py True`.\n        return_type (Union[LITERAL_LIST_OBJECT_NAMES, LITERAL_PANDAS_DATAFRAME_NAMES, LITERAL_PYSPARK_DATAFRAME_NAMES, LITERAL_NUMPY_ARRAY_NAMES, str], optional):\n            The desired return type.&lt;br&gt;\n            Options:\n\n            - `#!py \"ps\"`: Return the result as a PySpark DataFrame.\n            - `#!py \"pd\"`: Return the result as a Pandas DataFrame.\n            - `#!py \"np\"`: Return the result as a NumPy array.\n            - `#!py \"list\"`: Return the result as a Python list.\n\n            Defaults to `#!py \"pd\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ValueError:\n            If any of the values parsed to `return_type` are not valid options.\n        ColumnDoesNotExistError:\n            If the `#!py column` does not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (Optional[Union[psDataFrame, pdDataFrame, npArray, list]]):\n            The values from the given column, in the desired type.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import get_column_values\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n        ...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default params\"}\n        &gt;&gt;&gt; values = get_column_values(df, \"c\")\n        &gt;&gt;&gt; print(type(values))\n        &gt;&gt;&gt; print(values)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        &lt;class 'pandas.core.frame.DataFrame'&gt;\n        ```\n        ```{.txt .text title=\"Terminal\"}\n          c\n        0 c\n        ```\n        !!! success \"Conclusion: Successfully extracted distinct values from the `c` column, and converted to Pandas.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Not distinct\"}\n        &gt;&gt;&gt; values = get_column_values(df, \"c\", False)\n        &gt;&gt;&gt; print(type(values))\n        &gt;&gt;&gt; print(values)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        &lt;class 'pandas.core.frame.DataFrame'&gt;\n        ```\n        ```{.txt .text title=\"Terminal\"}\n          c\n        0 c\n        1 c\n        2 c\n        3 c\n        ```\n        !!! success \"Conclusion: Successfully extracted values from the `c` column, and converted to Pandas.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Flat list\"}\n        &gt;&gt;&gt; values = get_column_values(df, \"c\", False, \"flat_list\")\n        &gt;&gt;&gt; print(type(values))\n        &gt;&gt;&gt; print(values)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        &lt;class 'list'&gt;\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        [\"c\", \"c\", \"c\", \"c\"]\n        ```\n        !!! success \"Conclusion: Successfully extracted values from the `c` column, and converted to flat List.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Invalid return type\"}\n        &gt;&gt;&gt; get_column_values(df, \"c\", return_type=\"invalid\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ValueError: Unknown return type: 'invalid'.\n        Must be one of: ['pd', 'ps', 'np', 'list'].\n        For more info, check the `constants` module.\n        ```\n        !!! failure \"Conclusion: Invalid return type.\"\n        &lt;/div&gt;\n\n    ???+ tip \"See Also\"\n        - [`toolbox_pyspark.cleaning.convert_dataframe()`][toolbox_pyspark.cleaning.convert_dataframe]\n        - [`toolbox_pyspark.constants`][toolbox_pyspark.constants]\n    \"\"\"\n    df: psDataFrame = dataframe.select(column).filter(\n        f\"{column} is not null and {column} &lt;&gt; ''\"\n    )\n    df = df.distinct() if distinct else df\n    return convert_dataframe(dataframe=df, return_type=return_type)\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.update_nullability","title":"update_nullability","text":"<pre><code>update_nullability(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n    nullable: bool = True,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Update the nullability of specified columns in a PySpark DataFrame.</p> Details <p>This function updates the nullability of the specified columns in a PySpark DataFrame. If no columns are specified, it updates the nullability of all columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The input PySpark DataFrame.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>The columns for which to update nullability. If not provided, all columns will be updated. Defaults to <code>None</code>.</p> <code>None</code> <code>nullable</code> <code>bool</code> <p>Whether to set the columns as nullable or not. Defaults to <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame with the specified columns' nullability updated.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import update_nullability\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1.1, 2.2, 3.3, 4.4],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n&gt;&gt;&gt; print(df.schema)\n&gt;&gt;&gt; df.printSchema()\n</code></pre> Terminal<pre><code>+---+---+-----+\n| a | b |   c |\n+---+---+-----+\n| 1 | a | 1.1 |\n| 2 | b | 2.2 |\n| 3 | c | 3.3 |\n| 4 | d | 4.4 |\n+---+---+-----+\n</code></pre> Terminal<pre><code>StructType(\n    [\n        StructField(\"a\", LongType(), True),\n        StructField(\"b\", StringType(), True),\n        StructField(\"c\", DoubleType(), True),\n    ]\n)\n</code></pre> Terminal<pre><code>root\n |-- a: long (nullable = true)\n |-- b: string (nullable = true)\n |-- c: double (nullable = true)\n</code></pre> </p> <p>Example 1: Update nullability of all columns<pre><code>&gt;&gt;&gt; new_df = update_nullability(df, nullable=False)\n&gt;&gt;&gt; new_df.printSchema()\n</code></pre> Terminal<pre><code>root\n |-- a: long (nullable = false)\n |-- b: string (nullable = false)\n |-- c: double (nullable = false)\n</code></pre> <p>Conclusion: Successfully updated nullability of all columns.</p> <p>Example 2: Update nullability of specific columns<pre><code>&gt;&gt;&gt; new_df = update_nullability(df, columns=[\"a\", \"c\"], nullable=False)\n&gt;&gt;&gt; new_df.printSchema()\n</code></pre> Terminal<pre><code>root\n |-- a: long (nullable = false)\n |-- b: string (nullable = true)\n |-- c: double (nullable = false)\n</code></pre> <p>Conclusion: Successfully updated nullability of specific columns.</p> <p>Example 3: Column does not exist<pre><code>&gt;&gt;&gt; update_nullability(df, columns=\"d\", nullable=False)\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column 'd' does not exist in the DataFrame.\nTry one of: [\"a\", \"b\", \"c\"]\n</code></pre> <p>Conclusion: Column does not exist.</p> Credit <p>All credit goes to: https://stackoverflow.com/questions/46072411/can-i-change-the-nullability-of-a-column-in-my-spark-dataframe#answer-51821437.</p> See Also <ul> <li><code>toolbox_pyspark.checks.assert_columns_exists()</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef update_nullability(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n    nullable: bool = True,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Update the nullability of specified columns in a PySpark DataFrame.\n\n    ???+ abstract \"Details\"\n        This function updates the nullability of the specified columns in a PySpark DataFrame. If no columns are specified, it updates the nullability of all columns.\n\n    Params:\n        dataframe (psDataFrame):\n            The input PySpark DataFrame.\n        columns (Optional[Union[str, str_collection]], optional):\n            The columns for which to update nullability. If not provided, all columns will be updated.&lt;br&gt;\n            Defaults to `#!py None`.\n        nullable (bool, optional):\n            Whether to set the columns as nullable or not.&lt;br&gt;\n            Defaults to `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame with the specified columns' nullability updated.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import update_nullability\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1.1, 2.2, 3.3, 4.4],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        &gt;&gt;&gt; print(df.schema)\n        &gt;&gt;&gt; df.printSchema()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+-----+\n        | a | b |   c |\n        +---+---+-----+\n        | 1 | a | 1.1 |\n        | 2 | b | 2.2 |\n        | 3 | c | 3.3 |\n        | 4 | d | 4.4 |\n        +---+---+-----+\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        StructType(\n            [\n                StructField(\"a\", LongType(), True),\n                StructField(\"b\", StringType(), True),\n                StructField(\"c\", DoubleType(), True),\n            ]\n        )\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        root\n         |-- a: long (nullable = true)\n         |-- b: string (nullable = true)\n         |-- c: double (nullable = true)\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Update nullability of all columns\"}\n        &gt;&gt;&gt; new_df = update_nullability(df, nullable=False)\n        &gt;&gt;&gt; new_df.printSchema()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        root\n         |-- a: long (nullable = false)\n         |-- b: string (nullable = false)\n         |-- c: double (nullable = false)\n        ```\n        !!! success \"Conclusion: Successfully updated nullability of all columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Update nullability of specific columns\"}\n        &gt;&gt;&gt; new_df = update_nullability(df, columns=[\"a\", \"c\"], nullable=False)\n        &gt;&gt;&gt; new_df.printSchema()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        root\n         |-- a: long (nullable = false)\n         |-- b: string (nullable = true)\n         |-- c: double (nullable = false)\n        ```\n        !!! success \"Conclusion: Successfully updated nullability of specific columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Column does not exist\"}\n        &gt;&gt;&gt; update_nullability(df, columns=\"d\", nullable=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column 'd' does not exist in the DataFrame.\n        Try one of: [\"a\", \"b\", \"c\"]\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n\n    ??? success \"Credit\"\n        All credit goes to: https://stackoverflow.com/questions/46072411/can-i-change-the-nullability-of-a-column-in-my-spark-dataframe#answer-51821437.\n\n    ???+ tip \"See Also\"\n        - [`toolbox_pyspark.checks.assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n    \"\"\"\n    columns = get_columns(dataframe, columns)\n    assert_columns_exists(dataframe=dataframe, columns=columns)\n    schema: T.StructType = dataframe.schema\n    for struct_field in schema:\n        if struct_field.name in columns:\n            struct_field.nullable = nullable\n    return dataframe.sparkSession.createDataFrame(\n        data=dataframe.rdd, schema=dataframe.schema\n    )\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.trim_spaces_from_column","title":"trim_spaces_from_column","text":"<pre><code>trim_spaces_from_column(\n    dataframe: psDataFrame, column: str\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given list of columns, trim all of the excess white spaces from them.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update.</p> required <code>column</code> <code>str</code> <p>The column to clean.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If the <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated Data Frame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import trim_spaces_from_column\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"1   \", \"1   \", \"1   \", \"1   \"],\n...             \"d\": [\"   2\", \"   2\", \"   2\", \"   2\"],\n...             \"e\": [\"   3   \", \"   3   \", \"   3   \", \"   3   \"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n```{.py .python linenums=\"1\" title=\"Check\"}\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+------+------+---------+\n| a | b |    c |    d |       e |\n+---+---+------+------+---------+\n| 1 | a | 1    |    2 |    3    |\n| 2 | b | 1    |    2 |    3    |\n| 3 | c | 1    |    2 |    3    |\n| 4 | d | 1    |    2 |    3    |\n+---+---+------+------+---------+\n</code></pre> </p> <p>Example 1: Trim column<pre><code>&gt;&gt;&gt; trim_spaces_from_column(df, \"c\").show()\n</code></pre> Terminal<pre><code>+---+---+---+------+--------+\n| a | b | c |    d |      e |\n+---+---+---+------+--------+\n| 1 | a | 1 |    2 |   2    |\n| 2 | b | 1 |    2 |   2    |\n| 3 | c | 1 |    2 |   2    |\n| 4 | d | 1 |    2 |   2    |\n+---+---+---+------+--------+\n</code></pre> <p>Conclusion: Successfully trimmed the <code>c</code> column.</p> <p>Example 2: Invalid column<pre><code>&gt;&gt;&gt; trim_spaces_from_column(df, \"f\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column 'f' does not exist in the DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\", \"e\"]\n</code></pre> <p>Conclusion: Column does not exist.</p> Notes Justification <ul> <li>The main reason for this function is because when the data was exported from the Legacy WMS's, there's a whole bunch of trailing spaces in the data fields. My theory is because of the data type in the source system. That is, if it's originally stored as 'char' type, then it will maintain the data length. This issues doesn't seem to be affecting the <code>varchar</code> fields. Nonetheless, this function will strip the white spaces from the data; thus reducing the total size of the data stored therein.</li> <li>The reason why it is necessary to write this out as a custom function, instead of using the <code>F.trim()</code> function from the PySpark library directly is due to the deficiencies of the Java <code>trim()</code> function. More specifically, there are 13 different whitespace characters available in our ascii character set. The Java function only cleans about 6 of these. So therefore, we define this function which iterates through all 13 whitespace characters, and formats them in to a regular expression, to then parse it to the <code>F.regexp_replace()</code> function to be replaced with an empty string (<code>\"\"</code>). Therefore, all 13 characters will be replaced, the strings will be cleaned and trimmed ready for further processing.</li> </ul> Regex definition: <code>^[...]+|[...]+$</code> <ul> <li>1st Alternative: '^[...]+'<ul> <li>'^' asserts position at start of a line</li> <li>Match a single character present in the list below '[...]'<ul> <li>'+' matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)</li> <li>matches a single character in the list '  ' (case sensitive)<ul> <li>matches the character ' ' with index 160 (A0 or 240) literally (case sensitive)</li> <li>matches the character ' ' with index 32 (20 or 40) literally (case sensitive)</li> <li>... (repeat for all whitespace characters)</li> </ul> </li> </ul> </li> </ul> </li> <li>2nd Alternative: '[...]+$'<ul> <li>Match a single character present in the list below '[...]'<ul> <li>'+' matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)</li> <li>matches a single character in the list '  ' (case sensitive)<ul> <li>matches the character ' ' with index 160 (A0 or 240) literally (case sensitive)</li> <li>matches the character ' ' with index 32 (20 or 40) literally (case sensitive)</li> <li>... (repeat for all whitespace characters)</li> </ul> </li> </ul> </li> <li>'$' asserts position at the end of a line</li> </ul> </li> </ul> See Also <ul> <li><code>trim_spaces_from_columns()</code></li> <li><code>ALL_WHITESPACE_CHARACTERS</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef trim_spaces_from_column(\n    dataframe: psDataFrame,\n    column: str,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given list of columns, trim all of the excess white spaces from them.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update.\n        column (str):\n            The column to clean.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If the `#!py column` does not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated Data Frame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import trim_spaces_from_column\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"1   \", \"1   \", \"1   \", \"1   \"],\n        ...             \"d\": [\"   2\", \"   2\", \"   2\", \"   2\"],\n        ...             \"e\": [\"   3   \", \"   3   \", \"   3   \", \"   3   \"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+------+------+---------+\n        | a | b |    c |    d |       e |\n        +---+---+------+------+---------+\n        | 1 | a | 1    |    2 |    3    |\n        | 2 | b | 1    |    2 |    3    |\n        | 3 | c | 1    |    2 |    3    |\n        | 4 | d | 1    |    2 |    3    |\n        +---+---+------+------+---------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Trim column\"}\n        &gt;&gt;&gt; trim_spaces_from_column(df, \"c\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+------+--------+\n        | a | b | c |    d |      e |\n        +---+---+---+------+--------+\n        | 1 | a | 1 |    2 |   2    |\n        | 2 | b | 1 |    2 |   2    |\n        | 3 | c | 1 |    2 |   2    |\n        | 4 | d | 1 |    2 |   2    |\n        +---+---+---+------+--------+\n        ```\n        !!! success \"Conclusion: Successfully trimmed the `c` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Invalid column\"}\n        &gt;&gt;&gt; trim_spaces_from_column(df, \"f\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column 'f' does not exist in the DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\", \"e\"]\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n\n    ??? info \"Notes\"\n\n        ???+ info \"Justification\"\n            - The main reason for this function is because when the data was exported from the Legacy WMS's, there's a _whole bunch_ of trailing spaces in the data fields. My theory is because of the data type in the source system. That is, if it's originally stored as 'char' type, then it will maintain the data length. This issues doesn't seem to be affecting the `varchar` fields. Nonetheless, this function will strip the white spaces from the data; thus reducing the total size of the data stored therein.\n            - The reason why it is necessary to write this out as a custom function, instead of using the [`F.trim()`][trim] function from the PySpark library directly is due to the deficiencies of the Java [`trim()`](https://docs.oracle.com/javase/8/docs/api/java/lang/String.html#trim) function. More specifically, there are 13 different whitespace characters available in our ascii character set. The Java function only cleans about 6 of these. So therefore, we define this function which iterates through all 13 whitespace characters, and formats them in to a regular expression, to then parse it to the [`F.regexp_replace()`][regexp_replace] function to be replaced with an empty string (`\"\"`). Therefore, all 13 characters will be replaced, the strings will be cleaned and trimmed ready for further processing.\n\n            [trim]: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.trim.html\n            [regexp_replace]: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_replace.html\n\n        ???+ info \"Regex definition: `^[...]+|[...]+$`\"\n            - 1st Alternative: '^[...]+'\n                - '^' asserts position at start of a line\n                - Match a single character present in the list below '[...]'\n                    - '+' matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)\n                    - matches a single character in the list '  ' (case sensitive)\n                        - matches the character ' ' with index 160 (A0 or 240) literally (case sensitive)\n                        - matches the character ' ' with index 32 (20 or 40) literally (case sensitive)\n                        - ... (repeat for all whitespace characters)\n            - 2nd Alternative: '[...]+$'\n                - Match a single character present in the list below '[...]'\n                    - '+' matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)\n                    - matches a single character in the list '  ' (case sensitive)\n                        - matches the character ' ' with index 160 (A0 or 240) literally (case sensitive)\n                        - matches the character ' ' with index 32 (20 or 40) literally (case sensitive)\n                        - ... (repeat for all whitespace characters)\n                - '$' asserts position at the end of a line\n\n    ??? tip \"See Also\"\n        - [`trim_spaces_from_columns()`][toolbox_pyspark.cleaning.trim_spaces_from_columns]\n        - [`ALL_WHITESPACE_CHARACTERS`][toolbox_pyspark.constants.ALL_WHITESPACE_CHARACTERS]\n    \"\"\"\n    assert_column_exists(dataframe=dataframe, column=column, match_case=True)\n    space_chars: str_list = [chr(char.ascii) for char in WHITESPACES]\n    regexp: str = f\"^[{''.join(space_chars)}]+|[{''.join(space_chars)}]+$\"\n    return dataframe.withColumn(column, F.regexp_replace(column, regexp, \"\"))\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.trim_spaces_from_columns","title":"trim_spaces_from_columns","text":"<pre><code>trim_spaces_from_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given list of columns, trim all of the excess white spaces from them.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>The list of columns to be updated. Must be valid columns on <code>dataframe</code>. If given as a string, will be executed as a single column (ie. one-element long list). If not given, will apply to all columns in <code>dataframe</code> which have the data-type <code>string</code>. It is also possible to parse the values <code>\"all\"</code> or <code>\"all_string\"</code>, which will also apply this function to all columns in <code>dataframe</code> which have the data-type <code>string</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import trim_spaces_from_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"1   \", \"1   \", \"1   \", \"1   \"],\n...             \"d\": [\"   2\", \"   2\", \"   2\", \"   2\"],\n...             \"e\": [\"   3   \", \"   3   \", \"   3   \", \"   3   \"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n```{.py .python linenums=\"1\" title=\"Check\"}\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+------+------+---------+\n| a | b |    c |    d |       e |\n+---+---+------+------+---------+\n| 1 | a | 1    |    2 |    3    |\n| 2 | b | 1    |    2 |    3    |\n| 3 | c | 1    |    2 |    3    |\n| 4 | d | 1    |    2 |    3    |\n+---+---+------+------+---------+\n</code></pre> </p> <p>Example 1: One column as list<pre><code>&gt;&gt;&gt; trim_spaces_from_columns(df, [\"c\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---+------+---------+\n| a | b | c |    d |       e |\n+---+---+---+------+---------+\n| 1 | a | 1 |    2 |    3    |\n| 2 | b | 1 |    2 |    3    |\n| 3 | c | 1 |    2 |    3    |\n| 4 | d | 1 |    2 |    3    |\n+---+---+---+------+---------+\n</code></pre> <p>Conclusion: Successfully trimmed the <code>c</code> column.</p> <p>Example 2: Single column as string<pre><code>&gt;&gt;&gt; trim_spaces_from_columns(df, \"d\").show()\n</code></pre> Terminal<pre><code>+---+---+------+---+---------+\n| a | b |    c | d |       e |\n+---+---+------+---+---------+\n| 1 | a | 1    | 2 |    3    |\n| 2 | b | 1    | 2 |    3    |\n| 3 | c | 1    | 2 |    3    |\n| 4 | d | 1    | 2 |    3    |\n+---+---+------+---+---------+\n</code></pre> <p>Conclusion: Successfully trimmed the <code>d</code> column.</p> <p>Example 3: Multiple columns<pre><code>&gt;&gt;&gt; trim_spaces_from_columns(df, [\"c\", \"d\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---------+\n| a | b | c | d |       e |\n+---+---+---+---+---------+\n| 1 | a | 1 | 2 |    3    |\n| 2 | b | 1 | 2 |    3    |\n| 3 | c | 1 | 2 |    3    |\n| 4 | d | 1 | 2 |    3    |\n+---+---+---+---+---------+\n</code></pre> <p>Conclusion: Successfully trimmed the <code>c</code> and <code>d</code> columns.</p> <p>Example 4: All columns<pre><code>&gt;&gt;&gt; trim_spaces_from_columns(df, \"all\").show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 2 | 3 |\n| 2 | b | 1 | 2 | 3 |\n| 3 | c | 1 | 2 | 3 |\n| 4 | d | 1 | 2 | 3 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully trimmed all columns.</p> <p>Example 5: Default config<pre><code>&gt;&gt;&gt; trim_spaces_from_columns(df).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n| 1 | a | 1 | 2 | 3 |\n| 2 | b | 1 | 2 | 3 |\n| 3 | c | 1 | 2 | 3 |\n| 4 | d | 1 | 2 | 3 |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully trimmed all columns.</p> <p>Example 6: Invalid column<pre><code>&gt;&gt;&gt; trim_spaces_from_columns(df, [\"f\"])\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns ['f'] do not exist in the DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\", \"e\"]\n</code></pre> <p>Conclusion: Columns do not exist.</p> Notes Justification <ul> <li>The main reason for this function is because when the data was exported from the Legacy WMS's, there's a whole bunch of trailing spaces in the data fields. My theory is because of the data type in the source system. That is, if it's originally stored as 'char' type, then it will maintain the data length. This issues doesn't seem to be affecting the <code>varchar</code> fields. Nonetheless, this function will strip the white spaces from the data; thus reducing the total size of the data stored therein.</li> <li>The reason why it is necessary to write this out as a custom function, instead of using the <code>F.trim()</code> function from the PySpark library directly is due to the deficiencies of the Java <code>trim()</code> function. More specifically, there are 13 different whitespace characters available in our ascii character set. The Java function only cleans about 6 of these. So therefore, we define this function which iterates through all 13 whitespace characters, and formats them in to a regular expression, to then parse it to the <code>F.regexp_replace()</code> function to be replaced with an empty string (<code>\"\"</code>). Therefore, all 13 characters will be replaced, the strings will be cleaned and trimmed ready for further processing.</li> <li>The reason why this function exists as a standalone, and does not call <code>trim_spaces_from_column()</code> from within a loop is because <code>trim_spaces_from_column()</code> utilises the <code>.withColumn()</code> method to implement the <code>F.regexp_replace()</code> function on columns individually. When implemented iteratively, this process will create huge DAG's for the RDD, and blow out the complexity to a huge extend. Whereas this <code>trim_spaces_from_columns()</code> function will utilise the <code>.withColumns()</code> method to implement the <code>F.regexp_replace()</code> function over all columns at once. This <code>.withColumns()</code> method projects the function down to the underlying dataset in one single execution; not a different execution per column. Therefore, it is more simpler and more efficient.</li> </ul> Regex definition: <code>^[...]+|[...]+$</code> <ul> <li>1st Alternative: <code>^[...]+</code><ul> <li><code>^</code> asserts position at start of a line</li> <li>Match a single character present in the list below <code>[...]</code><ul> <li><code>+</code> matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)</li> <li>matches a single character in the list <code></code> (case sensitive)<ul> <li>matches the character <code></code> with index 160 (A0 or 240) literally (case sensitive)</li> <li>matches the character <code></code> with index 32 (20 or 40) literally (case sensitive)</li> <li>... (repeat for all whitespace characters)</li> </ul> </li> </ul> </li> </ul> </li> <li>2nd Alternative: <code>[...]+$</code><ul> <li>Match a single character present in the list below <code>[...]</code><ul> <li><code>+</code> matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)</li> <li>matches a single character in the list <code></code> (case sensitive)<ul> <li>matches the character <code></code> with index 160 (A0 or 240) literally (case sensitive)</li> <li>matches the character <code></code> with index 32 (20 or 40) literally (case sensitive)</li> <li>... (repeat for all whitespace characters)</li> </ul> </li> </ul> </li> </ul> </li> </ul> See Also <ul> <li><code>trim_spaces_from_column()</code></li> <li><code>ALL_WHITESPACE_CHARACTERS</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef trim_spaces_from_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given list of columns, trim all of the excess white spaces from them.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        columns (Optional[Union[str, str_collection]], optional):\n            The list of columns to be updated.\n            Must be valid columns on `dataframe`.\n            If given as a string, will be executed as a single column (ie. one-element long list).\n            If not given, will apply to all columns in `dataframe` which have the data-type `string`.\n            It is also possible to parse the values `#!py \"all\"` or `#!py \"all_string\"`, which will also apply this function to all columns in `dataframe` which have the data-type `string`.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import trim_spaces_from_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"1   \", \"1   \", \"1   \", \"1   \"],\n        ...             \"d\": [\"   2\", \"   2\", \"   2\", \"   2\"],\n        ...             \"e\": [\"   3   \", \"   3   \", \"   3   \", \"   3   \"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+------+------+---------+\n        | a | b |    c |    d |       e |\n        +---+---+------+------+---------+\n        | 1 | a | 1    |    2 |    3    |\n        | 2 | b | 1    |    2 |    3    |\n        | 3 | c | 1    |    2 |    3    |\n        | 4 | d | 1    |    2 |    3    |\n        +---+---+------+------+---------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: One column as list\"}\n        &gt;&gt;&gt; trim_spaces_from_columns(df, [\"c\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+------+---------+\n        | a | b | c |    d |       e |\n        +---+---+---+------+---------+\n        | 1 | a | 1 |    2 |    3    |\n        | 2 | b | 1 |    2 |    3    |\n        | 3 | c | 1 |    2 |    3    |\n        | 4 | d | 1 |    2 |    3    |\n        +---+---+---+------+---------+\n        ```\n        !!! success \"Conclusion: Successfully trimmed the `c` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Single column as string\"}\n        &gt;&gt;&gt; trim_spaces_from_columns(df, \"d\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+------+---+---------+\n        | a | b |    c | d |       e |\n        +---+---+------+---+---------+\n        | 1 | a | 1    | 2 |    3    |\n        | 2 | b | 1    | 2 |    3    |\n        | 3 | c | 1    | 2 |    3    |\n        | 4 | d | 1    | 2 |    3    |\n        +---+---+------+---+---------+\n        ```\n        !!! success \"Conclusion: Successfully trimmed the `d` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Multiple columns\"}\n        &gt;&gt;&gt; trim_spaces_from_columns(df, [\"c\", \"d\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---------+\n        | a | b | c | d |       e |\n        +---+---+---+---+---------+\n        | 1 | a | 1 | 2 |    3    |\n        | 2 | b | 1 | 2 |    3    |\n        | 3 | c | 1 | 2 |    3    |\n        | 4 | d | 1 | 2 |    3    |\n        +---+---+---+---+---------+\n        ```\n        !!! success \"Conclusion: Successfully trimmed the `c` and `d` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: All columns\"}\n        &gt;&gt;&gt; trim_spaces_from_columns(df, \"all\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 2 | 3 |\n        | 2 | b | 1 | 2 | 3 |\n        | 3 | c | 1 | 2 | 3 |\n        | 4 | d | 1 | 2 | 3 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully trimmed all columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Default config\"}\n        &gt;&gt;&gt; trim_spaces_from_columns(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | e |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 2 | 3 |\n        | 2 | b | 1 | 2 | 3 |\n        | 3 | c | 1 | 2 | 3 |\n        | 4 | d | 1 | 2 | 3 |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully trimmed all columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Invalid column\"}\n        &gt;&gt;&gt; trim_spaces_from_columns(df, [\"f\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns ['f'] do not exist in the DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\", \"e\"]\n        ```\n        !!! failure \"Conclusion: Columns do not exist.\"\n        &lt;/div&gt;\n\n    ???+ info \"Notes\"\n\n        ???+ info \"Justification\"\n            - The main reason for this function is because when the data was exported from the Legacy WMS's, there's a _whole bunch_ of trailing spaces in the data fields. My theory is because of the data type in the source system. That is, if it's originally stored as 'char' type, then it will maintain the data length. This issues doesn't seem to be affecting the `varchar` fields. Nonetheless, this function will strip the white spaces from the data; thus reducing the total size of the data stored therein.\n            - The reason why it is necessary to write this out as a custom function, instead of using the [`F.trim()`][trim] function from the PySpark library directly is due to the deficiencies of the Java [`trim()`](https://docs.oracle.com/javase/8/docs/api/java/lang/String.html#trim) function. More specifically, there are 13 different whitespace characters available in our ascii character set. The Java function only cleans about 6 of these. So therefore, we define this function which iterates through all 13 whitespace characters, and formats them in to a regular expression, to then parse it to the [`F.regexp_replace()`][regexp_replace] function to be replaced with an empty string (`\"\"`). Therefore, all 13 characters will be replaced, the strings will be cleaned and trimmed ready for further processing.\n            - The reason why this function exists as a standalone, and does not call [`trim_spaces_from_column()`][toolbox_pyspark.cleaning.trim_spaces_from_column] from within a loop is because [`trim_spaces_from_column()`][toolbox_pyspark.cleaning.trim_spaces_from_column] utilises the [`.withColumn()`][withColumn] method to implement the [`F.regexp_replace()`][regexp_replace] function on columns individually. When implemented iteratively, this process will create huge DAG's for the RDD, and blow out the complexity to a huge extend. Whereas this [`trim_spaces_from_columns()`][toolbox_pyspark.cleaning.trim_spaces_from_columns] function will utilise the [`.withColumns()`][withColumns] method to implement the [`F.regexp_replace()`][regexp_replace] function over all columns at once. This [`.withColumns()`][withColumns] method projects the function down to the underlying dataset in one single execution; not a different execution per column. Therefore, it is more simpler and more efficient.\n\n            [withColumn]: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html\n            [withColumns]: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumns.html\n            [trim]: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.trim.html\n            [regexp_replace]: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_replace.html\n\n        ???+ info \"Regex definition: `^[...]+|[...]+$`\"\n            - 1st Alternative: `^[...]+`\n                - `^` asserts position at start of a line\n                - Match a single character present in the list below `[...]`\n                    - `+` matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)\n                    - matches a single character in the list `  ` (case sensitive)\n                        - matches the character ` ` with index 160 (A0 or 240) literally (case sensitive)\n                        - matches the character ` ` with index 32 (20 or 40) literally (case sensitive)\n                        - ... (repeat for all whitespace characters)\n            - 2nd Alternative: `[...]+$`\n                - Match a single character present in the list below `[...]`\n                    - `+` matches the previous token between one and unlimited times, as many times as possible, giving back as needed (greedy)\n                    - matches a single character in the list `  ` (case sensitive)\n                        - matches the character ` ` with index 160 (A0 or 240) literally (case sensitive)\n                        - matches the character ` ` with index 32 (20 or 40) literally (case sensitive)\n                        - ... (repeat for all whitespace characters)\n\n    ??? tip \"See Also\"\n        - [`trim_spaces_from_column()`][toolbox_pyspark.cleaning.trim_spaces_from_column]\n        - [`ALL_WHITESPACE_CHARACTERS`][toolbox_pyspark.constants.ALL_WHITESPACE_CHARACTERS]\n    \"\"\"\n    columns = get_columns(dataframe, columns)\n    assert_columns_exists(dataframe=dataframe, columns=columns, match_case=True)\n    space_chars: str_list = WHITESPACES.to_list(\"chr\")  # type:ignore\n    regexp: str = f\"^[{''.join(space_chars)}]+|[{''.join(space_chars)}]+$\"\n    cols_exprs: dict[str, Column] = {\n        col: F.regexp_replace(col, regexp, \"\") for col in columns\n    }\n    return dataframe.withColumns(cols_exprs)\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.apply_function_to_column","title":"apply_function_to_column","text":"<pre><code>apply_function_to_column(\n    dataframe: psDataFrame,\n    column: str,\n    function: str = \"upper\",\n    *function_args,\n    **function_kwargs\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Apply a given PySpark <code>function</code> to a single <code>column</code> on <code>dataframe</code>.</p> Details <p>Under the hood, this function will simply call the <code>.withColumn()</code> method to apply the function named in <code>function</code> from the PySpark <code>functions</code> module. <pre><code>return dataframe.withColumn(column, getattr(F, function)(column, *function_args, **function_kwargs))\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update.</p> required <code>column</code> <code>str</code> <p>The column to update.</p> required <code>function</code> <code>str</code> <p>The function to execute. Must be a valid function from the PySpark <code>functions</code> module. Defaults to <code>\"upper\"</code>.</p> <code>'upper'</code> <code>*function_args</code> <code>Any</code> <p>The arguments to push down to the underlying <code>function</code>.</p> <code>()</code> <code>**function_kwargs</code> <code>Any</code> <p>The keyword arguments to push down to the underlying <code>function</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import apply_function_to_column\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n```{.py .python linenums=\"1\" title=\"Check\"}\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Default params<pre><code>&gt;&gt;&gt; apply_function_to_column(df, \"c\").show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | C | d |\n| 1 | b | C | d |\n| 2 | c | C | d |\n| 3 | d | C | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully applied the <code>upper</code> function to the <code>c</code> column.</p> <p>Example 2: Simple function<pre><code>&gt;&gt;&gt; apply_function_to_column(df, \"c\", \"lower\").show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully applied the <code>lower</code> function to the <code>c</code> column.</p> <p>Example 3: Complex function, using args<pre><code>&gt;&gt;&gt; apply_function_to_column(df, \"d\", \"lpad\", 5, \"?\").show()\n</code></pre> Terminal<pre><code>+---+---+---+-------+\n| a | b | c |     d |\n+---+---+---+-------+\n| 0 | a | c | ????d |\n| 1 | b | c | ????d |\n| 2 | c | c | ????d |\n| 3 | d | c | ????d |\n+---+---+---+-------+\n</code></pre> <p>Conclusion: Successfully applied the <code>lpad</code> function to the <code>d</code> column.</p> <p>Example 4: Complex function, using kwargs<pre><code>&gt;&gt;&gt; new_df = apply_function_to_column(\n...     dataframe=df,\n...     column=\"d\",\n...     function=\"lpad\",\n...     len=5,\n...     pad=\"?\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+-------+\n| a | b | c |     d |\n+---+---+---+-------+\n| 0 | a | c | ????d |\n| 1 | b | c | ????d |\n| 2 | c | c | ????d |\n| 3 | d | c | ????d |\n+---+---+---+-------+\n</code></pre> <p>Conclusion: Successfully applied the <code>lpad</code> function to the <code>d</code> column.</p> <p>Example 5: Different complex function, using kwargs<pre><code>&gt;&gt;&gt; new_df = apply_function_to_column(\n...     dataframe=df,\n...     column=\"b\",\n...     function=\"regexp_replace\",\n...     pattern=\"c\",\n...     replacement=\"17\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+----+---+---+\n| a |  b | c | d |\n+---+----+---+---+\n| 0 |  a | c | d |\n| 1 |  b | c | d |\n| 2 | 17 | c | d |\n| 3 |  d | c | d |\n+---+----+---+---+\n</code></pre> <p>Conclusion: Successfully applied the <code>regexp_replace</code> function to the <code>b</code> column.</p> <p>Example 6: Part of pipe<pre><code>&gt;&gt;&gt; new_df = df.transform(\n...     func=apply_function_to_column,\n...     column=\"d\",\n...     function=\"lpad\",\n...     len=5,\n...     pad=\"?\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+-------+\n| a | b | c |     d |\n+---+---+---+-------+\n| 0 | a | c | ????d |\n| 1 | b | c | ????d |\n| 2 | c | c | ????d |\n| 3 | d | c | ????d |\n+---+---+---+-------+\n</code></pre> <p>Conclusion: Successfully applied the <code>lpad</code> function to the <code>d</code> column.</p> <p>Example 7: Column name in different case<pre><code>&gt;&gt;&gt; new_df = df.transform(\n...     func=apply_function_to_column,\n...     column=\"D\",\n...     function=\"upper\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | D |\n| 1 | b | c | D |\n| 2 | c | c | D |\n| 3 | d | c | D |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully applied the <code>upper</code> function to the <code>D</code> column.</p> <p>Example 8: Invalid column<pre><code>&gt;&gt;&gt; apply_function_to_column(df, \"f\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column 'f' does not exist in the DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Column does not exist.</p> Notes <ul> <li>We have to name the <code>function</code> parameter as the full name because when this function is executed as part of a chain (by using the PySpark <code>.transform()</code> method), that one uses the <code>func</code> parameter.</li> </ul> See Also <ul> <li><code>apply_function_to_columns()</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef apply_function_to_column(\n    dataframe: psDataFrame,\n    column: str,\n    function: str = \"upper\",\n    *function_args,\n    **function_kwargs,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Apply a given PySpark `function` to a single `column` on `dataframe`.\n\n    ???+ abstract \"Details\"\n        Under the hood, this function will simply call the [`.withColumn()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html) method to apply the function named in `function` from the PySpark [`functions`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) module.\n        ```py\n        return dataframe.withColumn(column, getattr(F, function)(column, *function_args, **function_kwargs))\n        ```\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update.\n        column (str):\n            The column to update.\n        function (str, optional):\n            The function to execute. Must be a valid function from the PySpark [`functions`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) module.&lt;br&gt;\n            Defaults to `#!py \"upper\"`.\n        *function_args (Any, optional):\n            The arguments to push down to the underlying `function`.\n        **function_kwargs (Any, optional):\n            The keyword arguments to push down to the underlying `function`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import apply_function_to_column\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n        ...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default params\"}\n        &gt;&gt;&gt; apply_function_to_column(df, \"c\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | C | d |\n        | 1 | b | C | d |\n        | 2 | c | C | d |\n        | 3 | d | C | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully applied the `upper` function to the `c` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Simple function\"}\n        &gt;&gt;&gt; apply_function_to_column(df, \"c\", \"lower\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully applied the `lower` function to the `c` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Complex function, using args\"}\n        &gt;&gt;&gt; apply_function_to_column(df, \"d\", \"lpad\", 5, \"?\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+-------+\n        | a | b | c |     d |\n        +---+---+---+-------+\n        | 0 | a | c | ????d |\n        | 1 | b | c | ????d |\n        | 2 | c | c | ????d |\n        | 3 | d | c | ????d |\n        +---+---+---+-------+\n        ```\n        !!! success \"Conclusion: Successfully applied the `lpad` function to the `d` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Complex function, using kwargs\"}\n        &gt;&gt;&gt; new_df = apply_function_to_column(\n        ...     dataframe=df,\n        ...     column=\"d\",\n        ...     function=\"lpad\",\n        ...     len=5,\n        ...     pad=\"?\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+-------+\n        | a | b | c |     d |\n        +---+---+---+-------+\n        | 0 | a | c | ????d |\n        | 1 | b | c | ????d |\n        | 2 | c | c | ????d |\n        | 3 | d | c | ????d |\n        +---+---+---+-------+\n        ```\n        !!! success \"Conclusion: Successfully applied the `lpad` function to the `d` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Different complex function, using kwargs\"}\n        &gt;&gt;&gt; new_df = apply_function_to_column(\n        ...     dataframe=df,\n        ...     column=\"b\",\n        ...     function=\"regexp_replace\",\n        ...     pattern=\"c\",\n        ...     replacement=\"17\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----+---+---+\n        | a |  b | c | d |\n        +---+----+---+---+\n        | 0 |  a | c | d |\n        | 1 |  b | c | d |\n        | 2 | 17 | c | d |\n        | 3 |  d | c | d |\n        +---+----+---+---+\n        ```\n        !!! success \"Conclusion: Successfully applied the `regexp_replace` function to the `b` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Part of pipe\"}\n        &gt;&gt;&gt; new_df = df.transform(\n        ...     func=apply_function_to_column,\n        ...     column=\"d\",\n        ...     function=\"lpad\",\n        ...     len=5,\n        ...     pad=\"?\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+-------+\n        | a | b | c |     d |\n        +---+---+---+-------+\n        | 0 | a | c | ????d |\n        | 1 | b | c | ????d |\n        | 2 | c | c | ????d |\n        | 3 | d | c | ????d |\n        +---+---+---+-------+\n        ```\n        !!! success \"Conclusion: Successfully applied the `lpad` function to the `d` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: Column name in different case\"}\n        &gt;&gt;&gt; new_df = df.transform(\n        ...     func=apply_function_to_column,\n        ...     column=\"D\",\n        ...     function=\"upper\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | D |\n        | 1 | b | c | D |\n        | 2 | c | c | D |\n        | 3 | d | c | D |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully applied the `upper` function to the `D` column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: Invalid column\"}\n        &gt;&gt;&gt; apply_function_to_column(df, \"f\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column 'f' does not exist in the DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n\n    ??? info \"Notes\"\n        - We have to name the `function` parameter as the full name because when this function is executed as part of a chain (by using the PySpark [`.transform()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transform.html) method), that one uses the `func` parameter.\n\n    ??? tip \"See Also\"\n        - [`apply_function_to_columns()`][toolbox_pyspark.cleaning.apply_function_to_columns]\n    \"\"\"\n    assert_column_exists(dataframe, column, False)\n    return dataframe.withColumn(\n        colName=column,\n        col=getattr(F, function)(column, *function_args, **function_kwargs),\n    )\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.apply_function_to_columns","title":"apply_function_to_columns","text":"<pre><code>apply_function_to_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    function: str = \"upper\",\n    *function_args,\n    **function_kwargs\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Apply a given PySpark <code>function</code> over multiple <code>columns</code> on a given <code>dataframe</code>.</p> Details <p>Under the hood, this function will simply call the <code>.withColumns()</code> method to apply the function named in <code>function</code> from the PySpark <code>functions</code> module. <pre><code>return dataframe.withColumns(\n    {column: getattr(F, function)(column, *args, **kwargs) for column in columns}\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update.</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The columns to update.</p> required <code>function</code> <code>str</code> <p>The function to use. Defaults to <code>\"upper\"</code>.</p> <code>'upper'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import apply_function_to_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n```{.py .python linenums=\"1\" title=\"Check\"}\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Default params<pre><code>&gt;&gt;&gt; apply_function_to_columns(df, [\"b\", \"c\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | A | C | d |\n| 1 | B | C | d |\n| 2 | C | C | d |\n| 3 | D | C | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully applied the <code>upper</code> function to the <code>b</code> and <code>c</code> columns.</p> <p>Example 2: Simple function<pre><code>&gt;&gt;&gt; apply_function_to_columns(df, [\"b\", \"c\"], \"lower\").show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully applied the <code>lower</code> function to the <code>b</code> and <code>c</code> columns.</p> <p>Example 3: Complex function, with args<pre><code>&gt;&gt;&gt; apply_function_to_columns(df, [\"b\", \"c\", \"d\"], \"lpad\", 5, \"?\").show()\n</code></pre> Terminal<pre><code>+---+-------+-------+-------+\n| a |     b |     c |     d |\n+---+-------+-------+-------+\n| 0 | ????a | ????c | ????d |\n| 1 | ????b | ????c | ????d |\n| 2 | ????c | ????c | ????d |\n| 3 | ????d | ????c | ????d |\n+---+-------+-------+-------+\n</code></pre> <p>Conclusion: Successfully applied the <code>lpad</code> function to the <code>b</code>, <code>c</code> and <code>d</code> columns.</p> <p>Example 4: Complex function, with kwargs<pre><code>&gt;&gt;&gt; apply_function_to_columns(\n...     dataframe=df,\n...     columns=[\"b\", \"c\", \"d\"],\n...     function=\"lpad\",\n...     len=5,\n...     pad=\"?\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+-------+-------+-------+\n| a |     b |     c |     d |\n+---+-------+-------+-------+\n| 0 | ????a | ????c | ????d |\n| 1 | ????b | ????c | ????d |\n| 2 | ????c | ????c | ????d |\n| 3 | ????d | ????c | ????d |\n+---+-------+-------+-------+\n</code></pre> <p>Conclusion: Successfully applied the <code>lpad</code> function to the <code>b</code>, <code>c</code> and <code>d</code> columns.</p> <p>Example 5: Different complex function, with kwargs<pre><code>&gt;&gt;&gt; apply_function_to_columns(\n...     dataframe=df,\n...     columns=[\"b\", \"c\", \"d\"],\n...     function=\"regexp_replace\",\n...     pattern=\"c\",\n...     replacement=\"17\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+----+----+---+\n| a |  b |  c | d |\n+---+----+----+---+\n| 0 |  a | 17 | d |\n| 1 |  b | 17 | d |\n| 2 | 17 | 17 | d |\n| 3 |  d | 17 | d |\n+---+----+----+---+\n</code></pre> <p>Conclusion: Successfully applied the <code>regexp_replace</code> function to the <code>b</code>, <code>c</code> and <code>d</code> columns.</p> <p>Example 6: Part of pipe<pre><code>&gt;&gt;&gt; df.transform(\n...     func=apply_function_to_columns,\n...     columns=[\"b\", \"c\", \"d\"],\n...     function=\"lpad\",\n...     len=5,\n...     pad=\"?\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+-------+-------+-------+\n| a |     b |     c |     d |\n+---+-------+-------+-------+\n| 0 | ????a | ????c | ????d |\n| 1 | ????b | ????c | ????d |\n| 2 | ????c | ????c | ????d |\n| 3 | ????d | ????c | ????d |\n+---+-------+-------+-------+\n</code></pre> <p>Conclusion: Successfully applied the <code>lpad</code> function to the <code>b</code>, <code>c</code> and <code>d</code> columns.</p> <p>Example 7: Column name in different case<pre><code>&gt;&gt;&gt; apply_function_to_columns(\n...     dataframe=df,\n...     columns=[\"B\", \"c\", \"D\"],\n...     function=\"upper\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | A | C | D |\n| 1 | B | C | D |\n| 2 | C | C | D |\n| 3 | D | C | D |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully applied the <code>upper</code> function to the <code>B</code>, <code>c</code> and <code>D</code> columns.</p> <p>Example 8: Invalid columns<pre><code>&gt;&gt;&gt; apply_function_to_columns(df, [\"f\"])\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns ['f'] do not exist in the DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Columns do not exist.</p> Notes <ul> <li>We have to name the <code>function</code> parameter as the full name because when this function is executed as part of a chain (by using the PySpark <code>.transform()</code> method), that one uses the <code>func</code> parameter.</li> </ul> See Also <ul> <li><code>apply_function_to_column()</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef apply_function_to_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    function: str = \"upper\",\n    *function_args,\n    **function_kwargs,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Apply a given PySpark `function` over multiple `columns` on a given `dataframe`.\n\n    ???+ abstract \"Details\"\n        Under the hood, this function will simply call the [`.withColumns()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumns.html) method to apply the function named in `function` from the PySpark [`functions`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) module.\n        ```py\n        return dataframe.withColumns(\n            {column: getattr(F, function)(column, *args, **kwargs) for column in columns}\n        )\n        ```\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update.\n        columns (Union[str, str_collection]):\n            The columns to update.\n        function (str, optional):\n            The function to use.&lt;br&gt;\n            Defaults to `#!py \"upper\"`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import apply_function_to_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n        ...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default params\"}\n        &gt;&gt;&gt; apply_function_to_columns(df, [\"b\", \"c\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | A | C | d |\n        | 1 | B | C | d |\n        | 2 | C | C | d |\n        | 3 | D | C | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully applied the `upper` function to the `b` and `c` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Simple function\"}\n        &gt;&gt;&gt; apply_function_to_columns(df, [\"b\", \"c\"], \"lower\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully applied the `lower` function to the `b` and `c` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Complex function, with args\"}\n        &gt;&gt;&gt; apply_function_to_columns(df, [\"b\", \"c\", \"d\"], \"lpad\", 5, \"?\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+-------+-------+-------+\n        | a |     b |     c |     d |\n        +---+-------+-------+-------+\n        | 0 | ????a | ????c | ????d |\n        | 1 | ????b | ????c | ????d |\n        | 2 | ????c | ????c | ????d |\n        | 3 | ????d | ????c | ????d |\n        +---+-------+-------+-------+\n        ```\n        !!! success \"Conclusion: Successfully applied the `lpad` function to the `b`, `c` and `d` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Complex function, with kwargs\"}\n        &gt;&gt;&gt; apply_function_to_columns(\n        ...     dataframe=df,\n        ...     columns=[\"b\", \"c\", \"d\"],\n        ...     function=\"lpad\",\n        ...     len=5,\n        ...     pad=\"?\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+-------+-------+-------+\n        | a |     b |     c |     d |\n        +---+-------+-------+-------+\n        | 0 | ????a | ????c | ????d |\n        | 1 | ????b | ????c | ????d |\n        | 2 | ????c | ????c | ????d |\n        | 3 | ????d | ????c | ????d |\n        +---+-------+-------+-------+\n        ```\n        !!! success \"Conclusion: Successfully applied the `lpad` function to the `b`, `c` and `d` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Different complex function, with kwargs\"}\n        &gt;&gt;&gt; apply_function_to_columns(\n        ...     dataframe=df,\n        ...     columns=[\"b\", \"c\", \"d\"],\n        ...     function=\"regexp_replace\",\n        ...     pattern=\"c\",\n        ...     replacement=\"17\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----+----+---+\n        | a |  b |  c | d |\n        +---+----+----+---+\n        | 0 |  a | 17 | d |\n        | 1 |  b | 17 | d |\n        | 2 | 17 | 17 | d |\n        | 3 |  d | 17 | d |\n        +---+----+----+---+\n        ```\n        !!! success \"Conclusion: Successfully applied the `regexp_replace` function to the `b`, `c` and `d` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Part of pipe\"}\n        &gt;&gt;&gt; df.transform(\n        ...     func=apply_function_to_columns,\n        ...     columns=[\"b\", \"c\", \"d\"],\n        ...     function=\"lpad\",\n        ...     len=5,\n        ...     pad=\"?\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+-------+-------+-------+\n        | a |     b |     c |     d |\n        +---+-------+-------+-------+\n        | 0 | ????a | ????c | ????d |\n        | 1 | ????b | ????c | ????d |\n        | 2 | ????c | ????c | ????d |\n        | 3 | ????d | ????c | ????d |\n        +---+-------+-------+-------+\n        ```\n        !!! success \"Conclusion: Successfully applied the `lpad` function to the `b`, `c` and `d` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: Column name in different case\"}\n        &gt;&gt;&gt; apply_function_to_columns(\n        ...     dataframe=df,\n        ...     columns=[\"B\", \"c\", \"D\"],\n        ...     function=\"upper\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | A | C | D |\n        | 1 | B | C | D |\n        | 2 | C | C | D |\n        | 3 | D | C | D |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully applied the `upper` function to the `B`, `c` and `D` columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: Invalid columns\"}\n        &gt;&gt;&gt; apply_function_to_columns(df, [\"f\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns ['f'] do not exist in the DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! failure \"Conclusion: Columns do not exist.\"\n        &lt;/div&gt;\n\n    ??? info \"Notes\"\n        - We have to name the `function` parameter as the full name because when this function is executed as part of a chain (by using the PySpark [`.transform()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transform.html) method), that one uses the `func` parameter.\n\n    ??? tip \"See Also\"\n        - [`apply_function_to_column()`][toolbox_pyspark.cleaning.apply_function_to_column]\n    \"\"\"\n    columns = get_columns(dataframe, columns)\n    assert_columns_exists(dataframe, columns, False)\n    return dataframe.withColumns(\n        {\n            column: getattr(F, function)(column, *function_args, **function_kwargs)\n            for column in columns\n        }\n    )\n</code></pre>"},{"location":"code/cleaning/#toolbox_pyspark.cleaning.drop_matching_rows","title":"drop_matching_rows","text":"<pre><code>drop_matching_rows(\n    left_table: psDataFrame,\n    right_table: psDataFrame,\n    on_keys: Union[str, str_collection],\n    join_type: VALID_PYAPARK_JOIN_TYPES = \"left_anti\",\n    where_clause: Optional[str] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>This function is designed to remove any rows on the <code>left_table</code> which are existing on the <code>right_table</code>. That's why the <code>join_type</code> should always be <code>left_anti</code>.</p> Details <p>The intention behind this function is originating from the <code>Accumulation</code> layer in the BigDaS environment. The process on this table layer is to only insert rows from the <code>left_table</code> to the <code>right_table</code> with are not existing on the <code>right_table</code>. We include the <code>where_clause</code> here so that we can control any updated rows. Specifically, we check the <code>editdatetime</code> field between the <code>left_table</code> and the <code>right_table</code>, and any record on the <code>left_table</code> where the <code>editdatetime</code> field is greater than the <code>editdatetime</code> value on the <code>right_table</code>, then this row will remain on the <code>left_table</code>, and will later be updated on the <code>right_table</code>.</p> <p>It's important to specify here that this function was created to handle the same table between the <code>left_table</code> and the <code>right_table</code>, which are existing between different layers in the ADLS environment. Logically, it can be used for other purposes (it's generic enough); however, the intention was specifically for cleaning during the data pipeline processes.</p> <p>Parameters:</p> Name Type Description Default <code>left_table</code> <code>DataFrame</code> <p>The DataFrame from which you will be deleting the records.</p> required <code>right_table</code> <code>DataFrame</code> <p>The DataFrame from which to check for existing records. If any matching <code>on_keys</code> are existing on both the <code>right_table</code> and the <code>left_table</code>, then those records will be deleted from the <code>left_table</code>.</p> required <code>on_keys</code> <code>Union[str, str_collection]</code> <p>The matching keys between the two tables. These keys (aka columns) must be existing on both the <code>left_table</code> and the <code>right_table</code>.</p> required <code>join_type</code> <code>VALID_PYAPARK_JOIN_TYPES</code> <p>The type of join to use for this process. For the best performance, keep it as the default value. Defaults to <code>\"left_anti\"</code>.</p> <code>'left_anti'</code> <code>where_clause</code> <code>Optional[str]</code> <p>Any additional conditions to place on this join. Any records which match this condition will be kept on the <code>left_table</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The <code>left_table</code> after it has had it's rows deleted and cleaned by the <code>right_table</code>.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.cleaning import drop_matching_rows\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; left = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...             \"n\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n... right = left.where(\"a in (\"1\", \"2\")\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n```{.py .python linenums=\"1\" title=\"Check\"}\n&gt;&gt;&gt; left.show()\n&gt;&gt;&gt; right.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | n |\n+---+---+---+---+---+\n| 1 | a | 1 | 2 | a |\n| 2 | b | 1 | 2 | b |\n| 3 | c | 1 | 2 | c |\n| 4 | d | 1 | 2 | d |\n+---+---+---+---+---+\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | n |\n+---+---+---+---+---+\n| 1 | a | 1 | 2 | a |\n| 2 | b | 1 | 2 | b |\n+---+---+---+---+---+\n</code></pre> </p> <p>Example 1: Single column<pre><code>&gt;&gt;&gt; drop_matching_rows(\n...     left_table=left,\n...     right_table=right,\n...     on_keys=[\"a\"],\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | n |\n+---+---+---+---+---+\n| 3 | c | 1 | 2 | c |\n| 4 | d | 1 | 2 | d |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully removed the records from the <code>left_table</code> which are existing on the <code>right_table</code>.</p> <p>Example 2: Single column as string<pre><code>&gt;&gt;&gt; left.transform(\n...     drop_matching_rows,\n...     right_table=right,\n...     on_keys=\"a\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | n |\n+---+---+---+---+---+\n| 3 | c | 1 | 2 | c |\n| 4 | d | 1 | 2 | d |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully removed the records from the <code>left_table</code> which are existing on the <code>right_table</code>.</p> <p>Example 3: Multiple key columns<pre><code>&gt;&gt;&gt; drop_matching_rows(\n...     left_table=left,\n...     right_table=right,\n...     on_keys=[\"a\", \"b\"],\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | n |\n+---+---+---+---+---+\n| 3 | c | 1 | 2 | c |\n| 4 | d | 1 | 2 | d |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully removed the records from the <code>left_table</code> which are existing on the <code>right_table</code>.</p> <p>Example 4: Including `where` clause<pre><code>&gt;&gt;&gt; drop_matching_rows(\n...     left_table=left,\n...     right_table=right,\n...     on_keys=[\"a\"],\n...     where_clause=\"n &lt;&gt; 'd'\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | n |\n+---+---+---+---+---+\n| 3 | c | 1 | 2 | c |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully removed the records from the <code>left_table</code> which are existing on the <code>right_table</code> and matched the <code>where</code> clause.</p> <p>Example 5: Part of pipe<pre><code>&gt;&gt;&gt; left.transform(\n...     func=drop_matching_rows,\n...     right_table=right,\n...     on_keys=[\"a\"],\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---+\n| a | b | c | d | n |\n+---+---+---+---+---+\n| 3 | c | 1 | 2 | c |\n| 4 | d | 1 | 2 | d |\n+---+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully removed the records from the <code>left_table</code> which are existing on the <code>right_table</code>.</p> <p>Example 6: Invalid column<pre><code>&gt;&gt;&gt; drop_matching_rows(\n...     left_table=left,\n...     right_table=right,\n...     on_keys=[\"f\"],\n... )\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns ['f'] do not exist in the DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\", \"n\"]\n</code></pre> <p>Conclusion: Columns do not exist.</p> Notes <ul> <li>The <code>on_keys</code> parameter can be a single string or a list of strings. This is to allow for multiple columns to be used as the matching keys.</li> <li>The <code>where_clause</code> parameter is optional. If specified, then only the records which match the condition will be kept on the <code>left_table</code>. It is applied after the join. If not specified, then all records which are existing on the <code>right_table</code> will be removed from the <code>left_table</code>.</li> </ul> See Also <ul> <li><code>assert_columns_exists()</code></li> </ul> Source code in <code>src/toolbox_pyspark/cleaning.py</code> <pre><code>@typechecked\ndef drop_matching_rows(\n    left_table: psDataFrame,\n    right_table: psDataFrame,\n    on_keys: Union[str, str_collection],\n    join_type: VALID_PYAPARK_JOIN_TYPES = \"left_anti\",\n    where_clause: Optional[str] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        This function is designed to _remove_ any rows on the `left_table` which _are_ existing on the `right_table`. That's why the `join_type` should always be `left_anti`.\n\n    ???+ abstract \"Details\"\n        The intention behind this function is originating from the `Accumulation` layer in the BigDaS environment. The process on this table layer is to only _insert_ rows from the `left_table` to the `right_table` with are **not existing** on the `right_table`. We include the `where_clause` here so that we can control any updated rows. Specifically, we check the `editdatetime` field between the `left_table` and the `right_table`, and any record on the `left_table` where the `editdatetime` field is _greater than_ the `editdatetime` value on the `right_table`, then this row will _remain_ on the `left_table`, and will later be _updated_ on the `right_table`.\n\n        It's important to specify here that this function was created to handle the _same table_ between the `left_table` and the `right_table`, which are existing between different layers in the ADLS environment. Logically, it can be used for other purposes (it's generic enough); however, the intention was specifically for cleaning during the data pipeline processes.\n\n    Params:\n        left_table (psDataFrame):\n            The DataFrame _from which_ you will be deleting the records.\n        right_table (psDataFrame):\n            The DataFrame _from which_ to check for existing records. If any matching `on_keys` are existing on both the `right_table` and the `left_table`, then those records will be deleted from the `left_table`.\n        on_keys (Union[str, str_collection]):\n            The matching keys between the two tables. These keys (aka columns) must be existing on both the `left_table` and the `right_table`.\n        join_type (VALID_PYAPARK_JOIN_TYPES, optional):\n            The type of join to use for this process. For the best performance, keep it as the default value.&lt;br&gt;\n            Defaults to `#!py \"left_anti\"`.\n        where_clause (Optional[str], optional):\n            Any additional conditions to place on this join. Any records which **match** this condition will be **kept** on the `left_table`.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Returns:\n        (psDataFrame):\n            The `left_table` after it has had it's rows deleted and cleaned by the `right_table`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.cleaning import drop_matching_rows\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; left = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...             \"n\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ... right = left.where(\"a in (\"1\", \"2\")\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; left.show()\n        &gt;&gt;&gt; right.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | n |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 2 | a |\n        | 2 | b | 1 | 2 | b |\n        | 3 | c | 1 | 2 | c |\n        | 4 | d | 1 | 2 | d |\n        +---+---+---+---+---+\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | n |\n        +---+---+---+---+---+\n        | 1 | a | 1 | 2 | a |\n        | 2 | b | 1 | 2 | b |\n        +---+---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Single column\"}\n        &gt;&gt;&gt; drop_matching_rows(\n        ...     left_table=left,\n        ...     right_table=right,\n        ...     on_keys=[\"a\"],\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | n |\n        +---+---+---+---+---+\n        | 3 | c | 1 | 2 | c |\n        | 4 | d | 1 | 2 | d |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully removed the records from the `left_table` which are existing on the `right_table`.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Single column as string\"}\n        &gt;&gt;&gt; left.transform(\n        ...     drop_matching_rows,\n        ...     right_table=right,\n        ...     on_keys=\"a\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | n |\n        +---+---+---+---+---+\n        | 3 | c | 1 | 2 | c |\n        | 4 | d | 1 | 2 | d |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully removed the records from the `left_table` which are existing on the `right_table`.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Multiple key columns\"}\n        &gt;&gt;&gt; drop_matching_rows(\n        ...     left_table=left,\n        ...     right_table=right,\n        ...     on_keys=[\"a\", \"b\"],\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | n |\n        +---+---+---+---+---+\n        | 3 | c | 1 | 2 | c |\n        | 4 | d | 1 | 2 | d |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully removed the records from the `left_table` which are existing on the `right_table`.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Including `where` clause\"}\n        &gt;&gt;&gt; drop_matching_rows(\n        ...     left_table=left,\n        ...     right_table=right,\n        ...     on_keys=[\"a\"],\n        ...     where_clause=\"n &lt;&gt; 'd'\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | n |\n        +---+---+---+---+---+\n        | 3 | c | 1 | 2 | c |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully removed the records from the `left_table` which are existing on the `right_table` and matched the `where` clause.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Part of pipe\"}\n        &gt;&gt;&gt; left.transform(\n        ...     func=drop_matching_rows,\n        ...     right_table=right,\n        ...     on_keys=[\"a\"],\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---+\n        | a | b | c | d | n |\n        +---+---+---+---+---+\n        | 3 | c | 1 | 2 | c |\n        | 4 | d | 1 | 2 | d |\n        +---+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully removed the records from the `left_table` which are existing on the `right_table`.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Invalid column\"}\n        &gt;&gt;&gt; drop_matching_rows(\n        ...     left_table=left,\n        ...     right_table=right,\n        ...     on_keys=[\"f\"],\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns ['f'] do not exist in the DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\", \"n\"]\n        ```\n        !!! failure \"Conclusion: Columns do not exist.\"\n        &lt;/div&gt;\n\n    ??? info \"Notes\"\n        - The `on_keys` parameter can be a single string or a list of strings. This is to allow for multiple columns to be used as the matching keys.\n        - The `where_clause` parameter is optional. If specified, then only the records which match the condition will be kept on the `left_table`. It is applied after the join. If not specified, then all records which are existing on the `right_table` will be removed from the `left_table`.\n\n    ??? tip \"See Also\"\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n    \"\"\"\n    on_keys = [on_keys] if is_type(on_keys, str) else on_keys\n    assert_columns_exists(left_table, on_keys, False)\n    assert_columns_exists(right_table, on_keys, False)\n    return (\n        left_table.alias(\"left\")\n        .join(right_table.alias(\"right\"), on=on_keys, how=join_type)\n        .where(\"1=1\" if where_clause is None else where_clause)\n        .select([f\"left.{col}\" for col in left_table.columns])\n    )\n</code></pre>"},{"location":"code/columns/","title":"Columns","text":""},{"location":"code/columns/#toolbox_pyspark.columns","title":"toolbox_pyspark.columns","text":"<p>Summary</p> <p>The <code>columns</code> module is used to fetch columns from a given DataFrame using convenient syntax.</p>"},{"location":"code/columns/#toolbox_pyspark.columns.get_columns","title":"get_columns","text":"<pre><code>get_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n) -&gt; str_list\n</code></pre> <p>Summary</p> <p>Get a list of column names from a DataFrame based on optional filter criteria.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame from which to retrieve column names.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>Optional filter criteria for selecting columns. If a string is provided, it can be one of the following options:</p> Value Description <code>\"all\"</code> Return all columns in the DataFrame. <code>\"all_str\"</code> Return columns of string type. <code>\"all_int\"</code> Return columns of integer type. <code>\"all_numeric\"</code> Return columns of numeric types (integers and floats). <code>\"all_datetime\"</code> or <code>\"all_timestamp\"</code> Return columns of datetime or timestamp type. <code>\"all_date\"</code> Return columns of date type. Any other string Return columns matching the provided exact column name. <p>If a list or tuple of column names is provided, return only those columns. Defaults to <code>None</code> (which returns all columns).</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>str_list</code> <p>The selected column names from the DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F\n&gt;&gt;&gt; from toolbox_pyspark.columns import get_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = (\n...     spark\n...     .createDataFrame(\n...         pd.DataFrame(\n...             {\n...                 \"a\": (0, 1, 2, 3),\n...                 \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             }\n...         )\n...     )\n...     .withColumns(\n...         {\n...             \"c\": F.lit(\"1\").cast(\"int\"),\n...             \"d\": F.lit(\"2\").cast(\"string\"),\n...             \"e\": F.lit(\"1.1\").cast(\"float\"),\n...             \"f\": F.lit(\"1.2\").cast(\"double\"),\n...             \"g\": F.lit(\"2022-01-01\").cast(\"date\"),\n...             \"h\": F.lit(\"2022-02-01 01:00:00\").cast(\"timestamp\"),\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n&gt;&gt;&gt; print(df.dtypes)\n</code></pre> Terminal<pre><code>+---+---+---+---+-----+-----+------------+---------------------+\n| a | b | c | d |   e |   f |          g |                   h |\n+---+---+---+---+-----+-----+------------+---------------------+\n| 0 | a | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n| 1 | b | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n| 2 | c | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n| 3 | d | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n+---+---+---+---+-----+-----+------------+---------------------+\n</code></pre> Terminal<pre><code>[\n    (\"a\", \"bigint\"),\n    (\"b\", \"string\"),\n    (\"c\", \"int\"),\n    (\"d\", \"string\"),\n    (\"e\", \"float\"),\n    (\"f\", \"double\"),\n    (\"g\", \"date\"),\n    (\"h\", \"timestamp\"),\n]\n</code></pre> </p> <p>Example 1: Default params<pre><code>&gt;&gt;&gt; print(get_columns(df).columns)\n</code></pre> Terminal<pre><code>[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Specific columns<pre><code>&gt;&gt;&gt; print(get_columns(df, [\"a\", \"b\", \"c\"]).columns)\n</code></pre> Terminal<pre><code>[\"a\", \"b\", \"c\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Single column as list<pre><code>&gt;&gt;&gt; print(get_columns(df, [\"a\"]).columns)\n</code></pre> Terminal<pre><code>[\"a\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Single column as string<pre><code>&gt;&gt;&gt; print(get_columns(df, \"a\").columns)\n</code></pre> Terminal<pre><code>[\"a\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: All columns<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all\").columns)\n</code></pre> Terminal<pre><code>[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: All str<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_str\").columns)\n</code></pre> Terminal<pre><code>[\"b\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 7: All int<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all int\").columns)\n</code></pre> Terminal<pre><code>[\"c\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 8: All float<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_decimal\").columns)\n</code></pre> Terminal<pre><code>[\"e\", \"f\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 9: All numeric<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_numeric\").columns)\n</code></pre> Terminal<pre><code>[\"c\", \"e\", \"f\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 10: All date<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_date\").columns)\n</code></pre> Terminal<pre><code>[\"g\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 11: All datetime<pre><code>&gt;&gt;&gt; print(get_columns(df, \"all_datetime\").columns)\n</code></pre> Terminal<pre><code>[\"h\"]\n</code></pre> <p>Conclusion: Success.</p> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef get_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n) -&gt; str_list:\n    \"\"\"\n    !!! note \"Summary\"\n        Get a list of column names from a DataFrame based on optional filter criteria.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame from which to retrieve column names.\n        columns (Optional[Union[str, str_collection]], optional):\n            Optional filter criteria for selecting columns.&lt;br&gt;\n            If a string is provided, it can be one of the following options:\n\n            | Value | Description |\n            |-------|-------------|\n            | `#!py \"all\"` | Return all columns in the DataFrame.\n            | `#!py \"all_str\"` | Return columns of string type.\n            | `#!py \"all_int\"` | Return columns of integer type.\n            | `#!py \"all_numeric\"` | Return columns of numeric types (integers and floats).\n            | `#!py \"all_datetime\"` or `#!py \"all_timestamp\"` | Return columns of datetime or timestamp type.\n            | `#!py \"all_date\"` | Return columns of date type.\n            | Any other string | Return columns matching the provided exact column name.\n\n            If a list or tuple of column names is provided, return only those columns.&lt;br&gt;\n            Defaults to `#!py None` (which returns all columns).\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (str_list):\n            The selected column names from the DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F\n        &gt;&gt;&gt; from toolbox_pyspark.columns import get_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = (\n        ...     spark\n        ...     .createDataFrame(\n        ...         pd.DataFrame(\n        ...             {\n        ...                 \"a\": (0, 1, 2, 3),\n        ...                 \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             }\n        ...         )\n        ...     )\n        ...     .withColumns(\n        ...         {\n        ...             \"c\": F.lit(\"1\").cast(\"int\"),\n        ...             \"d\": F.lit(\"2\").cast(\"string\"),\n        ...             \"e\": F.lit(\"1.1\").cast(\"float\"),\n        ...             \"f\": F.lit(\"1.2\").cast(\"double\"),\n        ...             \"g\": F.lit(\"2022-01-01\").cast(\"date\"),\n        ...             \"h\": F.lit(\"2022-02-01 01:00:00\").cast(\"timestamp\"),\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        &gt;&gt;&gt; print(df.dtypes)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+-----+-----+------------+---------------------+\n        | a | b | c | d |   e |   f |          g |                   h |\n        +---+---+---+---+-----+-----+------------+---------------------+\n        | 0 | a | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n        | 1 | b | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n        | 2 | c | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n        | 3 | d | 1 | 2 | 1.1 | 1.2 | 2022-01-01 | 2022-02-01 01:00:00 |\n        +---+---+---+---+-----+-----+------------+---------------------+\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        [\n            (\"a\", \"bigint\"),\n            (\"b\", \"string\"),\n            (\"c\", \"int\"),\n            (\"d\", \"string\"),\n            (\"e\", \"float\"),\n            (\"f\", \"double\"),\n            (\"g\", \"date\"),\n            (\"h\", \"timestamp\"),\n        ]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default params\"}\n        &gt;&gt;&gt; print(get_columns(df).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Specific columns\"}\n        &gt;&gt;&gt; print(get_columns(df, [\"a\", \"b\", \"c\"]).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"b\", \"c\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Single column as list\"}\n        &gt;&gt;&gt; print(get_columns(df, [\"a\"]).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Single column as string\"}\n        &gt;&gt;&gt; print(get_columns(df, \"a\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: All columns\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: All str\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_str\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"b\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: All int\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all int\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"c\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: All float\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_decimal\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"e\", \"f\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 9: All numeric\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_numeric\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"c\", \"e\", \"f\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 10: All date\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_date\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"g\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 11: All datetime\"}\n        &gt;&gt;&gt; print(get_columns(df, \"all_datetime\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"h\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n    \"\"\"\n    if columns is None:\n        return dataframe.columns\n    elif is_type(columns, str):\n        if \"all\" in columns:\n            if \"str\" in columns:\n                return [\n                    col for col, typ in dataframe.dtypes if typ in (\"str\", \"string\")\n                ]\n            elif \"int\" in columns:\n                return [\n                    col for col, typ in dataframe.dtypes if typ in (\"int\", \"integer\")\n                ]\n            elif \"numeric\" in columns:\n                return [\n                    col\n                    for col, typ in dataframe.dtypes\n                    if typ in (\"int\", \"integer\", \"float\", \"double\", \"long\")\n                    or \"decimal\" in typ\n                ]\n            elif \"float\" in columns or \"double\" in columns or \"decimal\" in columns:\n                return [\n                    col\n                    for col, typ in dataframe.dtypes\n                    if typ in (\"float\", \"double\", \"long\") or \"decimal\" in typ\n                ]\n            elif \"datetime\" in columns or \"timestamp\" in columns:\n                return [\n                    col\n                    for col, typ in dataframe.dtypes\n                    if typ in (\"datetime\", \"timestamp\")\n                ]\n            elif \"date\" in columns:\n                return [col for col, typ in dataframe.dtypes if typ in [\"date\"]]\n            else:\n                return dataframe.columns\n        else:\n            return [columns]\n    else:\n        return list(columns)\n</code></pre>"},{"location":"code/columns/#toolbox_pyspark.columns.get_columns_by_likeness","title":"get_columns_by_likeness","text":"<pre><code>get_columns_by_likeness(\n    dataframe: psDataFrame,\n    starts_with: Optional[str] = None,\n    contains: Optional[str] = None,\n    ends_with: Optional[str] = None,\n    match_case: bool = False,\n    operator: Literal[\n        \"and\", \"or\", \"and not\", \"or not\"\n    ] = \"and\",\n) -&gt; str_list\n</code></pre> <p>Summary</p> <p>Extract the column names from a given <code>dataframe</code> based on text that the column name contains.</p> Details <p>You can use any combination of <code>startswith</code>, <code>contains</code>, and <code>endswith</code>. Under the hood, these will be implemented with a number of internal <code>lambda</code> functions to determine matches.</p> <p>The <code>operator</code> parameter determines how the conditions (<code>starts_with</code>, <code>contains</code>, <code>ends_with</code>) are combined:</p> Value Description <code>\"and\"</code> All conditions must be true. <code>\"or\"</code> At least one condition must be true. <code>\"and not\"</code> The first condition must be true and the second condition must be false. <code>\"or not\"</code> At least one condition must be true, but not all. <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The <code>dataframe</code> from which to extract the column names.</p> required <code>starts_with</code> <code>Optional[str]</code> <p>Extract any columns that starts with this <code>str</code>. Determined by using the <code>str.startswith()</code> method. Defaults to <code>None</code>.</p> <code>None</code> <code>contains</code> <code>Optional[str]</code> <p>Extract any columns that contains this <code>str</code> anywhere within it. Determined by using the <code>in</code> keyword. Defaults to <code>None</code>.</p> <code>None</code> <code>ends_with</code> <code>Optional[str]</code> <p>Extract any columns that ends with this <code>str</code>. Determined by using the <code>str.endswith()</code> method. Defaults to <code>None</code>.</p> <code>None</code> <code>match_case</code> <code>bool</code> <p>If you want to ensure an exact match for the columns, set this to <code>True</code>, else if you want to match the exact case for the columns, set this to <code>False</code>. Defaults to <code>False</code>.</p> <code>False</code> <code>operator</code> <code>Literal['and', 'or', 'and not', 'or not']</code> <p>The logical operator to place between the functions. Only used when there are multiple values parsed to the parameters: <code>starts_with</code>, <code>contains</code>: <code>ends_with</code>. Defaults to <code>and</code>.</p> <code>'and'</code> <p>Returns:</p> Type Description <code>str_list</code> <p>The list of columns which match the criteria specified.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.columns import get_columns_by_likeness\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; values = list(range(1, 6))\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"aaa\": values,\n...             \"aab\": values,\n...             \"aac\": values,\n...             \"afa\": values,\n...             \"afb\": values,\n...             \"afc\": values,\n...             \"bac\": values,\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+-----+-----+-----+-----+-----+-----+-----+\n| aaa | aab | aac | afa | afb | afc | bac |\n+-----+-----+-----+-----+-----+-----+-----+\n|   1 |   1 |   1 |   1 |   1 |   1 |   1 |\n|   2 |   2 |   2 |   2 |   2 |   2 |   2 |\n|   3 |   3 |   3 |   3 |   3 |   3 |   3 |\n|   4 |   4 |   4 |   4 |   4 |   4 |   4 |\n|   5 |   5 |   5 |   5 |   5 |   5 |   5 |\n+-----+-----+-----+-----+-----+-----+-----+\n</code></pre> </p> <p>Example 1: Starts With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\"))\n</code></pre> Terminal<pre><code>[\"aaa\", \"aab\", \"aac\", \"afa\", \"afb\", \"afc\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Contains<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, contains=\"f\"))\n</code></pre> Terminal<pre><code>[\"afa\", \"afb\", \"afc\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Ends With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, ends_with=\"c\"))\n</code></pre> Terminal<pre><code>[\"aac\", \"afc\", \"bac\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Starts With and Contains<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", contains=\"c\"))\n</code></pre> Terminal<pre><code>[\"aac\", \"afc\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: Starts With and Ends With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", ends_with=\"b\"))\n</code></pre> Terminal<pre><code>[\"aab\", \"afb\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: Contains and Ends With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, contains=\"f\", ends_with=\"b\"))\n</code></pre> Terminal<pre><code>[\"afb\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 7: Starts With and Contains and Ends With<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", contains=\"f\", ends_with=\"b\"))\n</code></pre> Terminal<pre><code>[\"afb\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 8: Using 'or' Operator<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", operator=\"or\", contains=\"f\"))\n</code></pre> Terminal<pre><code>[\"aaa\", \"aab\", \"aac\", \"afa\", \"afb\", \"afc\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 9: Using 'and not' Operator<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", operator=\"and not\", contains=\"f\"))\n</code></pre> Terminal<pre><code>[\"aaa\", \"aab\", \"aac\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 10: Error Example 1<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=123))\n</code></pre> Terminal<pre><code>TypeError: `starts_with` must be a `string` or `None`.\n</code></pre> <p>Conclusion: Error.</p> <p>Example 11: Error Example 2<pre><code>&gt;&gt;&gt; print(get_columns_by_likeness(df, operator=\"xor\"))\n</code></pre> Terminal<pre><code>ValueError: `operator` must be one of 'and', 'or', 'and not', 'or not'\n</code></pre> <p>Conclusion: Error.</p> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef get_columns_by_likeness(\n    dataframe: psDataFrame,\n    starts_with: Optional[str] = None,\n    contains: Optional[str] = None,\n    ends_with: Optional[str] = None,\n    match_case: bool = False,\n    operator: Literal[\"and\", \"or\", \"and not\", \"or not\"] = \"and\",\n) -&gt; str_list:\n    \"\"\"\n    !!! note \"Summary\"\n        Extract the column names from a given `dataframe` based on text that the column name contains.\n\n    ???+ abstract \"Details\"\n        You can use any combination of `startswith`, `contains`, and `endswith`. Under the hood, these will be implemented with a number of internal `#!py lambda` functions to determine matches.\n\n        The `operator` parameter determines how the conditions (`starts_with`, `contains`, `ends_with`) are combined:\n\n        | Value | Description |\n        |-------|-------------|\n        | `\"and\"` | All conditions must be true.\n        | `\"or\"` | At least one condition must be true.\n        | `\"and not\"` | The first condition must be true and the second condition must be false.\n        | `\"or not\"` | At least one condition must be true, but not all.\n\n    Params:\n        dataframe (psDataFrame):\n            The `dataframe` from which to extract the column names.\n        starts_with (Optional[str], optional):\n            Extract any columns that starts with this `#!py str`.&lt;br&gt;\n            Determined by using the `#!py str.startswith()` method.&lt;br&gt;\n            Defaults to `#!py None`.\n        contains (Optional[str], optional):\n            Extract any columns that contains this `#!py str` anywhere within it.&lt;br&gt;\n            Determined by using the `#!py in` keyword.&lt;br&gt;\n            Defaults to `#!py None`.\n        ends_with (Optional[str], optional):\n            Extract any columns that ends with this `#!py str`.&lt;br&gt;\n            Determined by using the `#!py str.endswith()` method.&lt;br&gt;\n            Defaults to `#!py None`.\n        match_case (bool, optional):\n            If you want to ensure an exact match for the columns, set this to `#!py True`, else if you want to match the exact case for the columns, set this to `#!py False`.&lt;br&gt;\n            Defaults to `#!py False`.\n        operator (Literal[\"and\", \"or\", \"and not\", \"or not\"], optional):\n            The logical operator to place between the functions.&lt;br&gt;\n            Only used when there are multiple values parsed to the parameters: `#!py starts_with`, `#!py contains`: `#!py ends_with`.&lt;br&gt;\n            Defaults to `#!py and`.\n\n    Returns:\n        (str_list):\n            The list of columns which match the criteria specified.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.columns import get_columns_by_likeness\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; values = list(range(1, 6))\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"aaa\": values,\n        ...             \"aab\": values,\n        ...             \"aac\": values,\n        ...             \"afa\": values,\n        ...             \"afb\": values,\n        ...             \"afc\": values,\n        ...             \"bac\": values,\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-----+-----+-----+-----+-----+-----+-----+\n        | aaa | aab | aac | afa | afb | afc | bac |\n        +-----+-----+-----+-----+-----+-----+-----+\n        |   1 |   1 |   1 |   1 |   1 |   1 |   1 |\n        |   2 |   2 |   2 |   2 |   2 |   2 |   2 |\n        |   3 |   3 |   3 |   3 |   3 |   3 |   3 |\n        |   4 |   4 |   4 |   4 |   4 |   4 |   4 |\n        |   5 |   5 |   5 |   5 |   5 |   5 |   5 |\n        +-----+-----+-----+-----+-----+-----+-----+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Starts With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aaa\", \"aab\", \"aac\", \"afa\", \"afb\", \"afc\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Contains\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, contains=\"f\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"afa\", \"afb\", \"afc\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Ends With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, ends_with=\"c\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aac\", \"afc\", \"bac\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Starts With and Contains\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", contains=\"c\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aac\", \"afc\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Starts With and Ends With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", ends_with=\"b\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aab\", \"afb\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Contains and Ends With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, contains=\"f\", ends_with=\"b\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"afb\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: Starts With and Contains and Ends With\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", contains=\"f\", ends_with=\"b\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"afb\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: Using 'or' Operator\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", operator=\"or\", contains=\"f\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aaa\", \"aab\", \"aac\", \"afa\", \"afb\", \"afc\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 9: Using 'and not' Operator\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=\"a\", operator=\"and not\", contains=\"f\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"aaa\", \"aab\", \"aac\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 10: Error Example 1\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, starts_with=123))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        TypeError: `starts_with` must be a `string` or `None`.\n        ```\n        !!! failure \"Conclusion: Error.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 11: Error Example 2\"}\n        &gt;&gt;&gt; print(get_columns_by_likeness(df, operator=\"xor\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        ValueError: `operator` must be one of 'and', 'or', 'and not', 'or not'\n        ```\n        !!! failure \"Conclusion: Error.\"\n        &lt;/div&gt;\n    \"\"\"\n\n    # Columns\n    cols: str_list = dataframe.columns\n    if not match_case:\n        cols = [col.upper() for col in cols]\n        starts_with = starts_with.upper() if starts_with is not None else None\n        contains = contains.upper() if contains is not None else None\n        ends_with = ends_with.upper() if ends_with is not None else None\n\n    # Parameters\n    o_: Literal[\"and\", \"or\", \"and not\", \"or not\"] = operator\n    s_: bool = starts_with is not None\n    c_: bool = contains is not None\n    e_: bool = ends_with is not None\n\n    # Functions\n    _ops = {\n        \"and\": lambda x, y: x and y,\n        \"or\": lambda x, y: x or y,\n        \"and not\": lambda x, y: x and not y,\n        \"or not\": lambda x, y: x or not y,\n    }\n    _s = lambda col, s: col.startswith(s)\n    _c = lambda col, c: c in col\n    _e = lambda col, e: col.endswith(e)\n    _sc = lambda col, s, c: _ops[o_](_s(col, s), _c(col, c))\n    _se = lambda col, s, e: _ops[o_](_s(col, s), _e(col, e))\n    _ce = lambda col, c, e: _ops[o_](_c(col, c), _e(col, e))\n    _sce = lambda col, s, c, e: _ops[o_](_ops[o_](_s(col, s), _c(col, c)), _e(col, e))\n\n    # Logic\n    if s_ and not c_ and not e_:\n        return [col for col in cols if _s(col, starts_with)]\n    elif c_ and not s_ and not e_:\n        return [col for col in cols if _c(col, contains)]\n    elif e_ and not s_ and not c_:\n        return [col for col in cols if _e(col, ends_with)]\n    elif s_ and c_ and not e_:\n        return [col for col in cols if _sc(col, starts_with, contains)]\n    elif s_ and e_ and not c_:\n        return [col for col in cols if _se(col, starts_with, ends_with)]\n    elif c_ and e_ and not s_:\n        return [col for col in cols if _ce(col, contains, ends_with)]\n    elif s_ and c_ and e_:\n        return [col for col in cols if _sce(col, starts_with, contains, ends_with)]\n    else:\n        return cols\n</code></pre>"},{"location":"code/columns/#toolbox_pyspark.columns.rename_columns","title":"rename_columns","text":"<pre><code>rename_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n    string_function: str = \"upper\",\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Use one of the common Python string functions to be applied to one or multiple columns.</p> Details <p>The <code>string_function</code> must be a valid string method. For more info on available functions, see: https://docs.python.org/3/library/stdtypes.html#string-methods</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>The columns to be updated. Must be a valid column on <code>dataframe</code>. If not provided, will be applied to all columns. It is also possible to parse the values <code>\"all\"</code>, which will also apply this function to all columns in <code>dataframe</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>string_function</code> <code>str</code> <p>The string function to be applied. Defaults to <code>\"upper\"</code>.</p> <code>'upper'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Import\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.columns import rename_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Single column, default params<pre><code>&gt;&gt;&gt; print(rename_columns(df, \"a\").columns)\n</code></pre> Terminal<pre><code>[\"A\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Single column, simple function<pre><code>&gt;&gt;&gt; print(rename_columns(df, \"a\", \"upper\").columns)\n</code></pre> Terminal<pre><code>[\"A\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Single column, complex function<pre><code>&gt;&gt;&gt; print(rename_columns(df, \"a\", \"replace('b', 'test')\").columns)\n</code></pre> Terminal<pre><code>[\"a\", \"test\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Multiple columns<pre><code>&gt;&gt;&gt; print(rename_columns(df, [\"a\", \"b\"]).columns)\n</code></pre> Terminal<pre><code>[\"A\", \"B\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: Default function over all columns<pre><code>&gt;&gt;&gt; print(rename_columns(df).columns)\n</code></pre> Terminal<pre><code>[\"A\", \"B\", \"C\", \"D\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: Complex function over multiple columns<pre><code>&gt;&gt;&gt; print(rename_columns(df, [\"a\", \"b\"], \"replace('b', 'test')\").columns)\n</code></pre> Terminal<pre><code>[\"a\", \"test\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> See Also <ul> <li><code>assert_columns_exists()</code></li> <li><code>assert_column_exists()</code></li> </ul> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef rename_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n    string_function: str = \"upper\",\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Use one of the common Python string functions to be applied to one or multiple columns.\n\n    ???+ abstract \"Details\"\n        The `string_function` must be a valid string method. For more info on available functions, see: https://docs.python.org/3/library/stdtypes.html#string-methods\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        columns (Optional[Union[str, str_collection]], optional):\n            The columns to be updated.&lt;br&gt;\n            Must be a valid column on `dataframe`.&lt;br&gt;\n            If not provided, will be applied to all columns.&lt;br&gt;\n            It is also possible to parse the values `\"all\"`, which will also apply this function to all columns in `dataframe`.&lt;br&gt;\n            Defaults to `None`.\n        string_function (str, optional):\n            The string function to be applied. Defaults to `\"upper\"`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Import\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.columns import rename_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n        ...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Single column, default params\"}\n        &gt;&gt;&gt; print(rename_columns(df, \"a\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"A\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Single column, simple function\"}\n        &gt;&gt;&gt; print(rename_columns(df, \"a\", \"upper\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"A\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Single column, complex function\"}\n        &gt;&gt;&gt; print(rename_columns(df, \"a\", \"replace('b', 'test')\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"test\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Multiple columns\"}\n        &gt;&gt;&gt; print(rename_columns(df, [\"a\", \"b\"]).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"A\", \"B\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Default function over all columns\"}\n        &gt;&gt;&gt; print(rename_columns(df).columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"A\", \"B\", \"C\", \"D\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Complex function over multiple columns\"}\n        &gt;&gt;&gt; print(rename_columns(df, [\"a\", \"b\"], \"replace('b', 'test')\").columns)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"a\", \"test\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`assert_column_exists()`][toolbox_pyspark.checks.assert_column_exists]\n    \"\"\"\n    columns = get_columns(dataframe, columns)\n    assert_columns_exists(dataframe=dataframe, columns=columns, match_case=True)\n    cols_exprs: dict[str, str] = {\n        col: eval(\n            f\"'{col}'.{string_function}{'()' if not string_function.endswith(')') else ''}\"\n        )\n        for col in columns\n    }\n    return dataframe.withColumnsRenamed(cols_exprs)\n</code></pre>"},{"location":"code/columns/#toolbox_pyspark.columns.reorder_columns","title":"reorder_columns","text":"<pre><code>reorder_columns(\n    dataframe: psDataFrame,\n    new_order: Optional[str_collection] = None,\n    missing_columns_last: bool = True,\n    key_columns_position: Optional[\n        Literal[\"first\", \"last\"]\n    ] = \"first\",\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Reorder the columns in a given DataFrame in to a custom order, or to put the <code>key_</code> columns at the end (that is, to the far right) of the dataframe.</p> Details <p>The decision flow chart is as follows:</p> <pre><code>graph TD\n    a([begin])\n    z([end])\n    b{{new_order}}\n    c{{missing_cols_last}}\n    d{{key_cols_position}}\n    g[cols = dataframe.columns]\n    h[cols = new_order]\n    i[cols += missing_cols]\n    j[cols = non_key_cols + key_cols]\n    k[cols = key_cols + non_key_cols]\n    l[\"return dataframe.select(cols)\"]\n    a --&gt; b\n    b --is not None--&gt; h --&gt; c\n    b --is None--&gt; g --&gt; d\n    c --False--&gt; l\n    c --True--&gt; i ----&gt; l\n    d --\"first\"--&gt; k ---&gt; l\n    d --\"last\"---&gt; j --&gt; l\n    d --None--&gt; l\n    l --&gt; z</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update</p> required <code>new_order</code> <code>Optional[Union[str, str_list, str_tuple, str_set]]</code> <p>The custom order for the columns on the order. Defaults to <code>None</code>.</p> <code>None</code> <code>missing_columns_last</code> <code>bool</code> <p>For any columns existing on <code>dataframes.columns</code>, but missing from <code>new_order</code>, if <code>missing_columns_last=True</code>, then include those missing columns to the right of the dataframe, in the same order that they originally appear. Defaults to <code>True</code>.</p> <code>True</code> <code>key_columns_position</code> <code>Optional[Literal['first', 'last']]</code> <p>Where should the <code>\"key_*\"</code> columns be located?.</p> <ul> <li>If <code>\"first\"</code>, then they will be relocated to the start of the dataframe, before all other columns.</li> <li>If <code>\"last\"</code>, then they will be relocated to the end of the dataframe, after all other columns.</li> <li>If <code>None</code>, they they will remain their original order.</li> </ul> <p>Regardless of their position, their original order will be maintained. Defaults to <code>\"first\"</code>.</p> <code>'first'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.columns import reorder_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"key_a\": [\"0\", \"1\", \"2\", \"3\"],\n...             \"c\": [\"1\", \"1\", \"1\", \"1\"],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...             \"key_c\": [\"1\", \"1\", \"1\", \"1\"],\n...             \"key_e\": [\"3\", \"3\", \"3\", \"3\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+-------+---+---+-------+-------+\n| a | b | key_a | c | d | key_c | key_e |\n+---+---+-------+---+---+-------+-------+\n| 0 | a |     0 | 1 | 2 |     1 |     3 |\n| 1 | b |     1 | 1 | 2 |     1 |     3 |\n| 2 | c |     2 | 1 | 2 |     1 |     3 |\n| 3 | d |     3 | 1 | 2 |     1 |     3 |\n+---+---+-------+---+---+-------+-------+\n</code></pre> </p> <p>Example 1: Default config<pre><code>&gt;&gt;&gt; new_df = reorder_columns(dataframe=df)\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+-------+---+---+---+---+\n| key_a | key_c | key_e | a | b | c | d |\n+-------+-------+-------+---+---+---+---+\n|     0 |     1 |     3 | 0 | a | 1 | 2 |\n|     1 |     1 |     3 | 1 | b | 1 | 2 |\n|     2 |     1 |     3 | 2 | c | 1 | 2 |\n|     3 |     1 |     3 | 3 | d | 1 | 2 |\n+-------+-------+-------+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Custom order<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     new_order=[\"key_a\", \"key_c\", \"b\", \"key_e\", \"a\", \"c\", \"d\"],\n... )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+---+-------+---+---+---+\n| key_a | key_c | b | key_e | a | c | d |\n+-------+-------+---+-------+---+---+---+\n|     0 |     1 | a |     3 | 0 | 1 | 2 |\n|     1 |     1 | b |     3 | 1 | 1 | 2 |\n|     2 |     1 | c |     3 | 2 | 1 | 2 |\n|     3 |     1 | d |     3 | 3 | 1 | 2 |\n+-------+-------+---+-------+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Custom order, include missing columns<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     new_order=[\"key_a\", \"key_c\", \"a\", \"b\"],\n...     missing_columns_last=True,\n...     )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+---+---+-------+---+---+\n| key_a | key_c | a | b | key_e | c | d |\n+-------+-------+---+---+-------+---+---+\n|     0 |     1 | 0 | a |     3 | 1 | 2 |\n|     1 |     1 | 1 | b |     3 | 1 | 2 |\n|     2 |     1 | 2 | c |     3 | 1 | 2 |\n|     3 |     1 | 3 | d |     3 | 1 | 2 |\n+-------+-------+---+---+-------+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Custom order, exclude missing columns<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     new_order=[\"key_a\", \"key_c\", \"a\", \"b\"],\n...     missing_columns_last=False,\n... )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+---+---+\n| key_a | key_c | a | b |\n+-------+-------+---+---+\n|     0 |     1 | 0 | a |\n|     1 |     1 | 1 | b |\n|     2 |     1 | 2 | c |\n|     3 |     1 | 3 | d |\n+-------+-------+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: Keys last<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     key_columns_position=\"last\",\n... )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+-------+-------+-------+\n| a | b | c | d | key_a | key_c | key_e |\n+---+---+---+---+-------+-------+-------+\n| 0 | a | 1 | 2 |     0 |     1 |     3 |\n| 1 | b | 1 | 2 |     1 |     1 |     3 |\n| 2 | c | 1 | 2 |     2 |     1 |     3 |\n| 3 | d | 1 | 2 |     3 |     1 |     3 |\n+---+---+---+---+-------+-------+-------+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: Keys first<pre><code>&gt;&gt;&gt; new_df = reorder_columns(\n...     dataframe=df,\n...     key_columns_position=\"first\",\n... )\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+-------+-------+-------+---+---+---+---+\n| key_a | key_c | key_e | a | b | c | d |\n+-------+-------+-------+---+---+---+---+\n|     0 |     1 |     3 | 0 | a | 1 | 2 |\n|     1 |     1 |     3 | 1 | b | 1 | 2 |\n|     2 |     1 |     3 | 2 | c | 1 | 2 |\n|     3 |     1 |     3 | 3 | d | 1 | 2 |\n+-------+-------+-------+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef reorder_columns(\n    dataframe: psDataFrame,\n    new_order: Optional[str_collection] = None,\n    missing_columns_last: bool = True,\n    key_columns_position: Optional[Literal[\"first\", \"last\"]] = \"first\",\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Reorder the columns in a given DataFrame in to a custom order, or to put the `key_` columns at the end (that is, to the far right) of the dataframe.\n\n    ???+ abstract \"Details\"\n        The decision flow chart is as follows:\n\n        ```mermaid\n        graph TD\n            a([begin])\n            z([end])\n            b{{new_order}}\n            c{{missing_cols_last}}\n            d{{key_cols_position}}\n            g[cols = dataframe.columns]\n            h[cols = new_order]\n            i[cols += missing_cols]\n            j[cols = non_key_cols + key_cols]\n            k[cols = key_cols + non_key_cols]\n            l[\"return dataframe.select(cols)\"]\n            a --&gt; b\n            b --is not None--&gt; h --&gt; c\n            b --is None--&gt; g --&gt; d\n            c --False--&gt; l\n            c --True--&gt; i ----&gt; l\n            d --\"first\"--&gt; k ---&gt; l\n            d --\"last\"---&gt; j --&gt; l\n            d --None--&gt; l\n            l --&gt; z\n        ```\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update\n        new_order (Optional[Union[str, str_list, str_tuple, str_set]], optional):\n            The custom order for the columns on the order.&lt;br&gt;\n            Defaults to `#!py None`.\n        missing_columns_last (bool, optional):\n            For any columns existing on `#!py dataframes.columns`, but missing from `#!py new_order`, if `#!py missing_columns_last=True`, then include those missing columns to the right of the dataframe, in the same order that they originally appear.&lt;br&gt;\n            Defaults to `#!py True`.\n        key_columns_position (Optional[Literal[\"first\", \"last\"]], optional):\n            Where should the `#!py \"key_*\"` columns be located?.&lt;br&gt;\n\n            - If `#!py \"first\"`, then they will be relocated to the start of the dataframe, before all other columns.\n            - If `#!py \"last\"`, then they will be relocated to the end of the dataframe, after all other columns.\n            - If `#!py None`, they they will remain their original order.\n\n            Regardless of their position, their original order will be maintained.\n            Defaults to `#!py \"first\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.columns import reorder_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"key_a\": [\"0\", \"1\", \"2\", \"3\"],\n        ...             \"c\": [\"1\", \"1\", \"1\", \"1\"],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...             \"key_c\": [\"1\", \"1\", \"1\", \"1\"],\n        ...             \"key_e\": [\"3\", \"3\", \"3\", \"3\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+-------+---+---+-------+-------+\n        | a | b | key_a | c | d | key_c | key_e |\n        +---+---+-------+---+---+-------+-------+\n        | 0 | a |     0 | 1 | 2 |     1 |     3 |\n        | 1 | b |     1 | 1 | 2 |     1 |     3 |\n        | 2 | c |     2 | 1 | 2 |     1 |     3 |\n        | 3 | d |     3 | 1 | 2 |     1 |     3 |\n        +---+---+-------+---+---+-------+-------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default config\"}\n        &gt;&gt;&gt; new_df = reorder_columns(dataframe=df)\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+-------+---+---+---+---+\n        | key_a | key_c | key_e | a | b | c | d |\n        +-------+-------+-------+---+---+---+---+\n        |     0 |     1 |     3 | 0 | a | 1 | 2 |\n        |     1 |     1 |     3 | 1 | b | 1 | 2 |\n        |     2 |     1 |     3 | 2 | c | 1 | 2 |\n        |     3 |     1 |     3 | 3 | d | 1 | 2 |\n        +-------+-------+-------+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Custom order\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     new_order=[\"key_a\", \"key_c\", \"b\", \"key_e\", \"a\", \"c\", \"d\"],\n        ... )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+---+-------+---+---+---+\n        | key_a | key_c | b | key_e | a | c | d |\n        +-------+-------+---+-------+---+---+---+\n        |     0 |     1 | a |     3 | 0 | 1 | 2 |\n        |     1 |     1 | b |     3 | 1 | 1 | 2 |\n        |     2 |     1 | c |     3 | 2 | 1 | 2 |\n        |     3 |     1 | d |     3 | 3 | 1 | 2 |\n        +-------+-------+---+-------+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Custom order, include missing columns\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     new_order=[\"key_a\", \"key_c\", \"a\", \"b\"],\n        ...     missing_columns_last=True,\n        ...     )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+---+---+-------+---+---+\n        | key_a | key_c | a | b | key_e | c | d |\n        +-------+-------+---+---+-------+---+---+\n        |     0 |     1 | 0 | a |     3 | 1 | 2 |\n        |     1 |     1 | 1 | b |     3 | 1 | 2 |\n        |     2 |     1 | 2 | c |     3 | 1 | 2 |\n        |     3 |     1 | 3 | d |     3 | 1 | 2 |\n        +-------+-------+---+---+-------+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Custom order, exclude missing columns\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     new_order=[\"key_a\", \"key_c\", \"a\", \"b\"],\n        ...     missing_columns_last=False,\n        ... )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+---+---+\n        | key_a | key_c | a | b |\n        +-------+-------+---+---+\n        |     0 |     1 | 0 | a |\n        |     1 |     1 | 1 | b |\n        |     2 |     1 | 2 | c |\n        |     3 |     1 | 3 | d |\n        +-------+-------+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Keys last\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     key_columns_position=\"last\",\n        ... )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+-------+-------+-------+\n        | a | b | c | d | key_a | key_c | key_e |\n        +---+---+---+---+-------+-------+-------+\n        | 0 | a | 1 | 2 |     0 |     1 |     3 |\n        | 1 | b | 1 | 2 |     1 |     1 |     3 |\n        | 2 | c | 1 | 2 |     2 |     1 |     3 |\n        | 3 | d | 1 | 2 |     3 |     1 |     3 |\n        +---+---+---+---+-------+-------+-------+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Keys first\"}\n        &gt;&gt;&gt; new_df = reorder_columns(\n        ...     dataframe=df,\n        ...     key_columns_position=\"first\",\n        ... )\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +-------+-------+-------+---+---+---+---+\n        | key_a | key_c | key_e | a | b | c | d |\n        +-------+-------+-------+---+---+---+---+\n        |     0 |     1 |     3 | 0 | a | 1 | 2 |\n        |     1 |     1 |     3 | 1 | b | 1 | 2 |\n        |     2 |     1 |     3 | 2 | c | 1 | 2 |\n        |     3 |     1 |     3 | 3 | d | 1 | 2 |\n        +-------+-------+-------+---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n    \"\"\"\n    df_cols: str_list = dataframe.columns\n    if new_order is not None:\n        cols: str_list = get_columns(dataframe, new_order)\n        if missing_columns_last:\n            cols += [col for col in df_cols if col not in new_order]\n    else:\n        non_key_cols: str_list = [\n            col for col in df_cols if not col.lower().startswith(\"key_\")\n        ]\n        key_cols: str_list = [col for col in df_cols if col.lower().startswith(\"key_\")]\n        if key_columns_position == \"first\":\n            cols = key_cols + non_key_cols\n        elif key_columns_position == \"last\":\n            cols = non_key_cols + key_cols\n        else:\n            cols = df_cols\n    return dataframe.select(cols)\n</code></pre>"},{"location":"code/columns/#toolbox_pyspark.columns.delete_columns","title":"delete_columns","text":"<pre><code>delete_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    missing_column_handler: Literal[\n        \"raise\", \"warn\", \"pass\"\n    ] = \"pass\",\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given <code>dataframe</code>, delete the columns listed in <code>columns</code>.</p> Details <p>You can use <code>missing_columns_handler</code> to specify how to handle missing columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The dataframe from which to delete the columns</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The list of columns to delete.</p> required <code>missing_column_handler</code> <code>Literal['raise', 'warn', 'pass']</code> <p>How to handle any columns which are missing from <code>dataframe.columns</code>.</p> <p>If any columns in <code>columns</code> are missing from <code>dataframe.columns</code>, then the following will happen for each option:</p> Option Result <code>\"raise\"</code> An <code>ColumnDoesNotExistError</code> exception will be raised <code>\"warn\"</code> An <code>ColumnDoesNotExistWarning</code> warning will be raised <code>\"pass\"</code> Nothing will be raised <p>Defaults to <code>\"pass\"</code>.</p> <code>'pass'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated <code>dataframe</code>, with the columns listed in <code>columns</code> having been removed.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.columns import delete_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [0, 1, 2, 3],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Single column<pre><code>&gt;&gt;&gt; df.transform(delete_columns, \"a\").show()\n</code></pre> Terminal<pre><code>+---+---+---+\n| b | c | d |\n+---+---+---+\n| a | c | d |\n| b | c | d |\n| c | c | d |\n| d | c | d |\n+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 2: Multiple columns<pre><code>&gt;&gt;&gt; df.transform(delete_columns, [\"a\", \"b\"]).show()\n</code></pre> Terminal<pre><code>+---+---+\n| c | d |\n+---+---+\n| c | d |\n| c | d |\n| c | d |\n| c | d |\n+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 3: Single column missing, raises error<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=\"z\",\n...         missing_column_handler=\"raise\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns [\"z\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 4: Multiple columns, one missing, raises error<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"a\", \"b\", \"z\"],\n...         missing_column_handler=\"raise\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns [\"z\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 5: Multiple columns, all missing, raises error<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"x\", \"y\", \"z\"],\n...         missing_column_handler=\"raise\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns [\"x\", \"y\", \"z\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c\", \"d\"]\n</code></pre> <p>Conclusion: Success.</p> <p>Example 6: Single column missing, raises warning<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=\"z\",\n...         missing_column_handler=\"warn\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>ColumnDoesNotExistWarning: Columns missing from \"dataframe\": [\"z\"].\nWill still proceed to delete columns that do exist.\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 7: Multiple columns, one missing, raises warning<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"a\", \"b\", \"z\"],\n...         missing_column_handler=\"warn\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>ColumnDoesNotExistWarning: Columns missing from \"dataframe\": [\"z\"].\nWill still proceed to delete columns that do exist.\n</code></pre> Terminal<pre><code>+---+---+\n| c | d |\n+---+---+\n| c | d |\n| c | d |\n| c | d |\n| c | d |\n+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 8: Multiple columns, all missing, raises warning<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"x\", \"y\", \"z\"],\n...         missing_column_handler=\"warn\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>ColumnDoesNotExistWarning: Columns missing from \"dataframe\": [\"x\", \"y\", \"z\"].\nWill still proceed to delete columns that do exist.\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 9: Single column missing, nothing raised<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=\"z\",\n...         missing_column_handler=\"pass\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 10: Multiple columns, one missing, nothing raised<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"a\", \"b\", \"z\"],\n...         missing_column_handler=\"pass\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>+---+---+\n| c | d |\n+---+---+\n| c | d |\n| c | d |\n| c | d |\n| c | d |\n+---+---+\n</code></pre> <p>Conclusion: Success.</p> <p>Example 11: Multiple columns, all missing, nothing raised<pre><code>&gt;&gt;&gt; (\n...     df.transform(\n...         delete_columns,\n...         columns=[\"x\", \"y\", \"z\"],\n...         missing_column_handler=\"pass\",\n...     )\n...     .show()\n... )\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 0 | a | c | d |\n| 1 | b | c | d |\n| 2 | c | c | d |\n| 3 | d | c | d |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Success.</p> Source code in <code>src/toolbox_pyspark/columns.py</code> <pre><code>@typechecked\ndef delete_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    missing_column_handler: Literal[\"raise\", \"warn\", \"pass\"] = \"pass\",\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `#!py dataframe`, delete the columns listed in `columns`.\n\n    ???+ abstract \"Details\"\n        You can use `#!py missing_columns_handler` to specify how to handle missing columns.\n\n    Params:\n        dataframe (psDataFrame):\n            The dataframe from which to delete the columns\n        columns (Union[str, str_collection]):\n            The list of columns to delete.\n        missing_column_handler (Literal[\"raise\", \"warn\", \"pass\"], optional):\n            How to handle any columns which are missing from `#!py dataframe.columns`.\n\n            If _any_ columns in `columns` are missing from `#!py dataframe.columns`, then the following will happen for each option:\n\n            | Option | Result |\n            |--------|--------|\n            | `#!py \"raise\"` | An `#!py ColumnDoesNotExistError` exception will be raised\n            | `#!py \"warn\"` | An `#!py ColumnDoesNotExistWarning` warning will be raised\n            | `#!py \"pass\"` | Nothing will be raised\n\n            Defaults to `#!py \"pass\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated `#!py dataframe`, with the columns listed in `#!py columns` having been removed.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.columns import delete_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [0, 1, 2, 3],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [\"c\", \"c\", \"c\", \"c\"],\n        ...             \"d\": [\"d\", \"d\", \"d\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Single column\"}\n        &gt;&gt;&gt; df.transform(delete_columns, \"a\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+\n        | b | c | d |\n        +---+---+---+\n        | a | c | d |\n        | b | c | d |\n        | c | c | d |\n        | d | c | d |\n        +---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Multiple columns\"}\n        &gt;&gt;&gt; df.transform(delete_columns, [\"a\", \"b\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+\n        | c | d |\n        +---+---+\n        | c | d |\n        | c | d |\n        | c | d |\n        | c | d |\n        +---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Single column missing, raises error\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=\"z\",\n        ...         missing_column_handler=\"raise\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns [\"z\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Multiple columns, one missing, raises error\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"a\", \"b\", \"z\"],\n        ...         missing_column_handler=\"raise\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns [\"z\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Multiple columns, all missing, raises error\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"x\", \"y\", \"z\"],\n        ...         missing_column_handler=\"raise\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns [\"x\", \"y\", \"z\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c\", \"d\"]\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Single column missing, raises warning\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=\"z\",\n        ...         missing_column_handler=\"warn\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistWarning: Columns missing from \"dataframe\": [\"z\"].\n        Will still proceed to delete columns that do exist.\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: Multiple columns, one missing, raises warning\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"a\", \"b\", \"z\"],\n        ...         missing_column_handler=\"warn\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistWarning: Columns missing from \"dataframe\": [\"z\"].\n        Will still proceed to delete columns that do exist.\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+\n        | c | d |\n        +---+---+\n        | c | d |\n        | c | d |\n        | c | d |\n        | c | d |\n        +---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 8: Multiple columns, all missing, raises warning\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"x\", \"y\", \"z\"],\n        ...         missing_column_handler=\"warn\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistWarning: Columns missing from \"dataframe\": [\"x\", \"y\", \"z\"].\n        Will still proceed to delete columns that do exist.\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 9: Single column missing, nothing raised\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=\"z\",\n        ...         missing_column_handler=\"pass\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 10: Multiple columns, one missing, nothing raised\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"a\", \"b\", \"z\"],\n        ...         missing_column_handler=\"pass\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+\n        | c | d |\n        +---+---+\n        | c | d |\n        | c | d |\n        | c | d |\n        | c | d |\n        +---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 11: Multiple columns, all missing, nothing raised\"}\n        &gt;&gt;&gt; (\n        ...     df.transform(\n        ...         delete_columns,\n        ...         columns=[\"x\", \"y\", \"z\"],\n        ...         missing_column_handler=\"pass\",\n        ...     )\n        ...     .show()\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 0 | a | c | d |\n        | 1 | b | c | d |\n        | 2 | c | c | d |\n        | 3 | d | c | d |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Success.\"\n        &lt;/div&gt;\n    \"\"\"\n    columns = get_columns(dataframe, columns)\n    if missing_column_handler == \"raise\":\n        assert_columns_exists(dataframe=dataframe, columns=columns)\n    elif missing_column_handler == \"warn\":\n        warn_columns_missing(dataframe=dataframe, columns=columns)\n    elif missing_column_handler == \"pass\":\n        pass\n    return dataframe.select([col for col in dataframe.columns if col not in columns])\n</code></pre>"},{"location":"code/constants/","title":"Constants","text":""},{"location":"code/constants/#toolbox_pyspark.constants","title":"toolbox_pyspark.constants","text":"<p>Summary</p> <p>The <code>constants</code> module is used to hold the definitions of all constant values used across the package.</p>"},{"location":"code/constants/#toolbox_pyspark.constants.ALL_WHITESPACE_CHARACTERS","title":"ALL_WHITESPACE_CHARACTERS  <code>module-attribute</code>","text":"<pre><code>ALL_WHITESPACE_CHARACTERS: list[tuple[str, str, int]] = [\n    (\"character tabulation\", \"U+0009\", 9),\n    (\"line feed\", \"U+000A\", 10),\n    (\"line tabulation\", \"U+000B\", 11),\n    (\"form feed\", \"U+000C\", 12),\n    (\"carriage return\", \"U+000D\", 13),\n    (\"space\", \"U+0020\", 32),\n    (\"next line\", \"U+0085\", 133),\n    (\"no-break space\", \"U+00A0\", 160),\n    (\"ogham space mark\", \"U+1680\", 5760),\n    (\"en quad\", \"U+2000\", 8192),\n    (\"em quad\", \"U+2001\", 8193),\n    (\"en space\", \"U+2002\", 8194),\n    (\"em space\", \"U+2003\", 8195),\n    (\"three-per-em space\", \"U+2004\", 8196),\n    (\"four-per-em space\", \"U+2005\", 8197),\n    (\"six-per-em space\", \"U+2006\", 8198),\n    (\"figure space\", \"U+2007\", 8199),\n    (\"punctuation space\", \"U+2008\", 8200),\n    (\"thin space\", \"U+2009\", 8201),\n    (\"hair space\", \"U+200A\", 8202),\n    (\"line separator\", \"U+2028\", 8232),\n    (\"paragraph separator\", \"U+2029\", 8233),\n    (\"narrow no-break space\", \"U+202F\", 8239),\n    (\"medium mathematical space\", \"U+205F\", 8287),\n    (\"ideographic space\", \"U+3000\", 12288),\n    (\"mongolian vowel separator\", \"U+180E\", 6158),\n    (\"zero width space\", \"U+200B\", 8203),\n    (\"zero width non-joiner\", \"U+200C\", 8204),\n    (\"zero width joiner\", \"U+200D\", 8205),\n    (\"word joiner\", \"U+2060\", 8288),\n    (\"zero width non-breaking space\", \"U+FEFF\", 65279),\n]\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.WHITESPACE_CHARACTERS","title":"WHITESPACE_CHARACTERS  <code>module-attribute</code>","text":"<pre><code>WHITESPACE_CHARACTERS = WhitespaceCharacters(\n    [\n        WhitespaceChatacter(name, unicode, ascii)\n        for (\n            name,\n            unicode,\n            ascii,\n        ) in ALL_WHITESPACE_CHARACTERS\n    ]\n)\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.VALID_PYSPARK_TYPES","title":"VALID_PYSPARK_TYPES  <code>module-attribute</code>","text":"<pre><code>VALID_PYSPARK_TYPES = list(values())\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.VALID_PYSPARK_TYPE_NAMES","title":"VALID_PYSPARK_TYPE_NAMES  <code>module-attribute</code>","text":"<pre><code>VALID_PYSPARK_TYPE_NAMES: str_list = sorted(\n    list(keys()) + [\"str\", \"int\", \"bool\", \"datetime\"]\n)\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.ALL_PYSPARK_TYPES","title":"ALL_PYSPARK_TYPES  <code>module-attribute</code>","text":"<pre><code>ALL_PYSPARK_TYPES = Union[\n    DataType,\n    NullType,\n    CharType,\n    StringType,\n    VarcharType,\n    BinaryType,\n    BooleanType,\n    DateType,\n    TimestampType,\n    TimestampNTZType,\n    DecimalType,\n    DoubleType,\n    FloatType,\n    ByteType,\n    IntegerType,\n    LongType,\n    DayTimeIntervalType,\n    YearMonthIntervalType,\n    ShortType,\n    ArrayType,\n    MapType,\n    StructType,\n]\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.VALID_PYAPARK_JOIN_TYPES","title":"VALID_PYAPARK_JOIN_TYPES  <code>module-attribute</code>","text":"<pre><code>VALID_PYAPARK_JOIN_TYPES = Literal[\n    \"inner\",\n    \"cross\",\n    \"outer\",\n    \"full\",\n    \"fullouter\",\n    \"full_outer\",\n    \"left\",\n    \"leftouter\",\n    \"left_outer\",\n    \"right\",\n    \"rightouter\",\n    \"right_outer\",\n    \"semi\",\n    \"leftsemi\",\n    \"left_semi\",\n    \"anti\",\n    \"leftanti\",\n    \"left_anti\",\n]\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.ALL_PYSPARK_JOIN_TYPES","title":"ALL_PYSPARK_JOIN_TYPES  <code>module-attribute</code>","text":"<pre><code>ALL_PYSPARK_JOIN_TYPES = set(\n    get_args(VALID_PYAPARK_JOIN_TYPES)\n)\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.LITERAL_PANDAS_DATAFRAME_NAMES","title":"LITERAL_PANDAS_DATAFRAME_NAMES  <code>module-attribute</code>","text":"<pre><code>LITERAL_PANDAS_DATAFRAME_NAMES = Literal[\n    \"pandas.DataFrame\",\n    \"pandas\",\n    \"pd.DataFrame\",\n    \"pd.df\",\n    \"pddf\",\n    \"pdDataFrame\",\n    \"pdDF\",\n    \"pd\",\n]\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.LITERAL_PYSPARK_DATAFRAME_NAMES","title":"LITERAL_PYSPARK_DATAFRAME_NAMES  <code>module-attribute</code>","text":"<pre><code>LITERAL_PYSPARK_DATAFRAME_NAMES = Literal[\n    \"spark.DataFrame\",\n    \"pyspark.DataFrame\",\n    \"pyspark\",\n    \"spark\",\n    \"ps.DataFrame\",\n    \"ps.df\",\n    \"psdf\",\n    \"psDataFrame\",\n    \"psDF\",\n    \"ps\",\n]\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.LITERAL_NUMPY_ARRAY_NAMES","title":"LITERAL_NUMPY_ARRAY_NAMES  <code>module-attribute</code>","text":"<pre><code>LITERAL_NUMPY_ARRAY_NAMES = Literal[\n    \"numpy.array\",\n    \"np.array\",\n    \"np\",\n    \"numpy\",\n    \"nparr\",\n    \"npa\",\n    \"np.arr\",\n    \"np.a\",\n]\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.LITERAL_LIST_OBJECT_NAMES","title":"LITERAL_LIST_OBJECT_NAMES  <code>module-attribute</code>","text":"<pre><code>LITERAL_LIST_OBJECT_NAMES = Literal[\n    \"list\", \"lst\", \"l\", \"flat_list\", \"flatten_list\"\n]\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.VALID_PANDAS_DATAFRAME_NAMES","title":"VALID_PANDAS_DATAFRAME_NAMES  <code>module-attribute</code>","text":"<pre><code>VALID_PANDAS_DATAFRAME_NAMES = set(\n    get_args(LITERAL_PANDAS_DATAFRAME_NAMES)\n)\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.VALID_PYSPARK_DATAFRAME_NAMES","title":"VALID_PYSPARK_DATAFRAME_NAMES  <code>module-attribute</code>","text":"<pre><code>VALID_PYSPARK_DATAFRAME_NAMES = set(\n    get_args(LITERAL_PYSPARK_DATAFRAME_NAMES)\n)\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.VALID_NUMPY_ARRAY_NAMES","title":"VALID_NUMPY_ARRAY_NAMES  <code>module-attribute</code>","text":"<pre><code>VALID_NUMPY_ARRAY_NAMES = set(\n    get_args(LITERAL_NUMPY_ARRAY_NAMES)\n)\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.VALID_LIST_OBJECT_NAMES","title":"VALID_LIST_OBJECT_NAMES  <code>module-attribute</code>","text":"<pre><code>VALID_LIST_OBJECT_NAMES = set(\n    get_args(LITERAL_LIST_OBJECT_NAMES)\n)\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants.VALID_DATAFRAME_NAMES","title":"VALID_DATAFRAME_NAMES  <code>module-attribute</code>","text":"<pre><code>VALID_DATAFRAME_NAMES: str_set = union(\n    VALID_PYSPARK_DATAFRAME_NAMES\n)\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants._DEFAULT_DEPRECATION_WARNING_CLASS","title":"_DEFAULT_DEPRECATION_WARNING_CLASS  <code>module-attribute</code>","text":"<pre><code>_DEFAULT_DEPRECATION_WARNING_CLASS = DeprecationWarning\n</code></pre>"},{"location":"code/constants/#toolbox_pyspark.constants._DEFAULT_DEPRECATION_WARNING","title":"_DEFAULT_DEPRECATION_WARNING  <code>module-attribute</code>","text":"<pre><code>_DEFAULT_DEPRECATION_WARNING = partial(\n    warn, category=_DEFAULT_DEPRECATION_WARNING_CLASS\n)\n</code></pre>"},{"location":"code/datetime/","title":"DateTime","text":""},{"location":"code/datetime/#toolbox_pyspark.datetime","title":"toolbox_pyspark.datetime","text":"<p>Summary</p> <p>The <code>datetime</code> module is used for fixing column names that contain datetime data, adding conversions to local datetimes, and for splitting a column in to their date and time components.</p>"},{"location":"code/datetime/#toolbox_pyspark.datetime.rename_datetime_column","title":"rename_datetime_column","text":"<pre><code>rename_datetime_column(\n    dataframe: psDataFrame, column: str\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given column in a Data Frame, if there is not another column existing that has <code>TIME</code> appended to the end, then re-name the column to append <code>TIME</code> to it.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If the <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated Data Frame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.datetime import rename_datetime_column\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c_date\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"h\"),\n...             \"d_date\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"h\"),\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+\n| a | b |              c_date |              d_date |\n+---+---+---------------------+---------------------+\n| 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n| 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n| 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n| 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n+---+---+---------------------+---------------------+\n</code></pre> </p> <p>Example 1: Update column<pre><code>&gt;&gt;&gt; rename_datetime_column(df, \"c_date\").show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+\n| a | b |          c_dateTIME |              d_date |\n+---+---+---------------------+---------------------+\n| 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n| 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n| 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n| 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n+---+---+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully renamed column.</p> <p>Example 2: Missing column<pre><code>&gt;&gt;&gt; rename_datetime_column(df, \"fff\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column \"fff\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c_date\", \"d_date\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> See Also <ul> <li><code>assert_column_exists()</code></li> <li><code>rename_datetime_columns()</code></li> </ul> Source code in <code>src/toolbox_pyspark/datetime.py</code> <pre><code>@typechecked\ndef rename_datetime_column(\n    dataframe: psDataFrame,\n    column: str,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given column in a Data Frame, if there is not another column existing that has `TIME` appended to the end, then re-name the column to append `TIME` to it.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update.\n        column (str):\n            The column to check.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If the `#!py column` does not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated Data Frame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.datetime import rename_datetime_column\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c_date\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"h\"),\n        ...             \"d_date\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"h\"),\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+\n        | a | b |              c_date |              d_date |\n        +---+---+---------------------+---------------------+\n        | 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n        | 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n        | 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n        | 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n        +---+---+---------------------+---------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Update column\"}\n        &gt;&gt;&gt; rename_datetime_column(df, \"c_date\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+\n        | a | b |          c_dateTIME |              d_date |\n        +---+---+---------------------+---------------------+\n        | 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n        | 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n        | 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n        | 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n        +---+---+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully renamed column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Missing column\"}\n        &gt;&gt;&gt; rename_datetime_column(df, \"fff\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column \"fff\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c_date\", \"d_date\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_column_exists()`][toolbox_pyspark.checks.assert_column_exists]\n        - [`rename_datetime_columns()`][toolbox_pyspark.datetime.rename_datetime_columns]\n    \"\"\"\n    assert_column_exists(dataframe=dataframe, column=column, match_case=True)\n    if f\"{column}TIME\" not in dataframe.columns:\n        return dataframe.withColumnRenamed(column, f\"{column}TIME\")\n    else:\n        return dataframe\n</code></pre>"},{"location":"code/datetime/#toolbox_pyspark.datetime.rename_datetime_columns","title":"rename_datetime_columns","text":"<pre><code>rename_datetime_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str_collection, str]] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Fix the column names for the date-time columns.</p> Details <p>This is necessary because in NGW, there are some columns which have <code>datetime</code> data types, but which have the name only containing <code>date</code>. So, this function will fix that.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update.</p> required <code>columns</code> <code>(Optional[Union[str_collection, str]], None)</code> <p>An optional list of columns to update. If this is not provided, or is the value <code>None</code> or <code>\"all\"</code>, then the function will automatically determine which columns to update based on the following logic:</p> <ol> <li>Loop through each column on <code>dataframe</code> to fetch the name and dtype using the method: <code>dataframe.dtypes</code>.<ol> <li>If the column name ends with <code>\"date\"</code></li> <li>AND the column type is <code>\"timestamp\"</code></li> <li>AND there is NOT already a column existing in the <code>dataframe.columns</code> with the name: <code>f\"{column}TIME\"</code></li> <li>THEN rename the column to have the name: <code>f\"{column}TIME\"</code></li> </ol> </li> <li>Next column.</li> </ol> <p>Default: <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.datetime import rename_datetime_column\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c_date\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"h\"),\n...             \"d_date\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"h\"),\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+\n| a | b |              c_date |              d_date |\n+---+---+---------------------+---------------------+\n| 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n| 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n| 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n| 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n+---+---+---------------------+---------------------+\n</code></pre> </p> <p>Example 1: One column<pre><code>&gt;&gt;&gt; rename_datetime_column(df, [\"c_date\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+\n| a | b |          c_dateTIME |              d_date |\n+---+---+---------------------+---------------------+\n| 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n| 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n| 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n| 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n+---+---+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully renamed column.</p> <p>Example 2: One column `str`<pre><code>&gt;&gt;&gt; rename_datetime_column(df, \"c_date\").show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+\n| a | b |          c_dateTIME |              d_date |\n+---+---+---------------------+---------------------+\n| 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n| 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n| 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n| 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n+---+---+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully renamed column.</p> <p>Example 3: All columns<pre><code>&gt;&gt;&gt; rename_datetime_column(df).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+\n| a | b |          c_dateTIME |          d_dateTIME |\n+---+---+---------------------+---------------------+\n| 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n| 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n| 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n| 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n+---+---+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully renamed columns.</p> <p>Example 4: All columns using 'all'<pre><code>&gt;&gt;&gt; rename_datetime_column(df, \"all\").show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+\n| a | b |          c_dateTIME |          d_dateTIME |\n+---+---+---------------------+---------------------+\n| 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n| 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n| 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n| 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n+---+---+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully renamed columns.</p> <p>Example 5: Missing column<pre><code>&gt;&gt;&gt; rename_datetime_columns(df, [\"fff\", \"ggg\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"fff\", \"ggg\"] do not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c_dateTIME\", \"d_dateTIME\"].\n</code></pre> <p>Conclusion: Columns do not exist.</p> See Also <ul> <li><code>assert_columns_exists()</code></li> <li><code>rename_datetime_column()</code></li> </ul> Source code in <code>src/toolbox_pyspark/datetime.py</code> <pre><code>@typechecked\ndef rename_datetime_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str_collection, str]] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Fix the column names for the date-time columns.\n\n    ???+ abstract \"Details\"\n        This is necessary because in NGW, there are some columns which have `datetime` data types, but which have the name only containing `date`.\n        So, this function will fix that.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update.\n        columns (Optional[Union[str_collection, str]], None):\n            An optional list of columns to update.\n            If this is not provided, or is the value `#!py None` or `#!py \"all\"`, then the function will automatically determine which columns to update based on the following logic:\n\n            1. Loop through each column on `dataframe` to fetch the name and dtype using the method: `dataframe.dtypes`.\n                1. If the column name ends with `#!py \"date\"`\n                2. **AND** the column type is `#!py \"timestamp\"`\n                3. **AND** there is **NOT** already a column existing in the `dataframe.columns` with the name: `f\"{column}TIME\"`\n                4. **THEN** rename the column to have the name: `f\"{column}TIME\"`\n            2. Next column.\n\n            Default: `None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.datetime import rename_datetime_column\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c_date\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"h\"),\n        ...             \"d_date\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"h\"),\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+\n        | a | b |              c_date |              d_date |\n        +---+---+---------------------+---------------------+\n        | 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n        | 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n        | 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n        | 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n        +---+---+---------------------+---------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: One column\"}\n        &gt;&gt;&gt; rename_datetime_column(df, [\"c_date\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+\n        | a | b |          c_dateTIME |              d_date |\n        +---+---+---------------------+---------------------+\n        | 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n        | 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n        | 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n        | 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n        +---+---+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully renamed column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column `str`\"}\n        &gt;&gt;&gt; rename_datetime_column(df, \"c_date\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+\n        | a | b |          c_dateTIME |              d_date |\n        +---+---+---------------------+---------------------+\n        | 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n        | 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n        | 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n        | 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n        +---+---+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully renamed column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: All columns\"}\n        &gt;&gt;&gt; rename_datetime_column(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+\n        | a | b |          c_dateTIME |          d_dateTIME |\n        +---+---+---------------------+---------------------+\n        | 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n        | 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n        | 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n        | 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n        +---+---+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully renamed columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: All columns using 'all'\"}\n        &gt;&gt;&gt; rename_datetime_column(df, \"all\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+\n        | a | b |          c_dateTIME |          d_dateTIME |\n        +---+---+---------------------+---------------------+\n        | 0 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 |\n        | 1 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 |\n        | 2 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 |\n        | 3 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 |\n        +---+---+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully renamed columns.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Missing column\"}\n        &gt;&gt;&gt; rename_datetime_columns(df, [\"fff\", \"ggg\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"fff\", \"ggg\"] do not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c_dateTIME\", \"d_dateTIME\"].\n        ```\n        !!! failure \"Conclusion: Columns do not exist.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`rename_datetime_column()`][toolbox_pyspark.datetime.rename_datetime_column]\n    \"\"\"\n    if columns is None or columns == \"all\":\n        datetime_cols: str_list = [\n            col\n            for col in get_columns(dataframe, \"all_datetime\")\n            if col.lower().endswith(\"date\")\n        ]\n        columns = [\n            col\n            for col in datetime_cols\n            if col.lower().endswith(\"date\") and f\"{col}TIME\" not in dataframe.columns\n        ]\n    elif is_type(columns, str):\n        columns = [columns]\n    assert_columns_exists(dataframe, columns, True)\n    for column in columns:\n        dataframe = rename_datetime_column(dataframe, column)\n    return dataframe\n</code></pre>"},{"location":"code/datetime/#toolbox_pyspark.datetime.add_local_datetime_column","title":"add_local_datetime_column","text":"<pre><code>add_local_datetime_column(\n    dataframe: psDataFrame,\n    column: str,\n    from_timezone: Optional[str] = None,\n    column_with_target_timezone: str = \"timezone_location\".upper(),\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For the given <code>column</code>, add a new column with the suffix <code>_LOCAL</code> which is a conversion of the datetime values from <code>column</code> to the desired timezone.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be fixed</p> required <code>column</code> <code>str</code> <p>The name of the column to do the conversion for. Must exist in <code>dataframe.columns</code>, and must be type <code>typestamp</code>.</p> required <code>from_timezone</code> <code>str</code> <p>The timezone which will be converted from. Must be a valid TimeZoneID, for more info, see: TimeZoneID. If not given, will default the <code>from_timezone</code> to be UTC. Default: <code>None</code>.</p> <code>None</code> <code>column_with_target_timezone</code> <code>str</code> <p>The column containing the target timezone value. By default will be the column <code>\"timezone_location\"</code>. Defaults to <code>\"timezone_location\".upper()</code>.</p> <code>upper()</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If <code>\"column\"</code> or <code>column_with_target_timezone</code> does not exist within <code>dataframe.columns</code>.</p> <code>ValueError</code> <p>If the <code>from_timezone</code> or <code>column_with_target_timezone</code> is not a valid timezone.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.datetime import add_local_datetime_column\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"D\"),\n...             \"d\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"D\"),\n...             \"e\": pd.date_range(start=\"2022-03-01\", periods=4, freq=\"D\"),\n...             \"target\": [\"Asia/Singapore\"] * 4,\n...             \"TIMEZONE_LOCATION\": [\"Australia/Perth\"] * 4,\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n| a | b |                   c |                   d |                   e |         target | TIMEZONE_LOCATION |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n</code></pre> </p> <p>Example 1: Converting from UTC time<pre><code>&gt;&gt;&gt; add_local_datetime_column(df, \"c\").show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+\n| a | b |                   c |                   d |                   e |         target | TIMEZONE_LOCATION |             c_LOCAL |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 08:00:00 |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 08:00:00 |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 08:00:00 |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-04 08:00:00 |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully converted from UTC.</p> <p>Example 2: Converting from specific timezone, with custom column containing target timezone<pre><code>&gt;&gt;&gt; add_local_datetime_column(\n...     dataframe=df,\n...     column=\"c\",\n...     from_timezone=\"Australia/Sydney\",\n...     column_with_target_timezone=\"target\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n| a | b |                   c |                   d |                   e |         target | TIMEZONE_LOCATION |               c_UTC |             c_LOCAL |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2021-12-31 13:00:00 | 2021-12-31 21:00:00 |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 13:00:00 | 2022-01-01 21:00:00 |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 13:00:00 | 2022-01-02 21:00:00 |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 13:00:00 | 2022-01-03 21:00:00 |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully converted timezone.</p> <p>Example 3: Invalid column name<pre><code>&gt;&gt;&gt; add_local_datetime_column(df, \"invalid_column\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column \"invalid_column\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c\", \"d\", \"e\", \"target\", \"TIMEZONE_LOCATION\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> <p>Example 4: Invalid timezone<pre><code>&gt;&gt;&gt; add_local_datetime_column(df, \"c\", from_timezone=\"Invalid/Timezone\")\n</code></pre> Terminal<pre><code>ValueError: The timezone \"Invalid/Timezone\" is not a valid timezone.\n</code></pre> <p>Conclusion: Invalid timezone.</p> Notes <ul> <li>If <code>from_timezone is None</code>, then it is assumed that the datetime data in <code>column</code> is already in UTC timezone.</li> <li>If <code>from_timezone is not None</code>, then a new column will be added with the syntax <code>{column}_UTC</code>, then another column added with <code>{column}_LOCAL</code>. This is necessary because PySpark cannot convert immediately from one timezone to another; it must first require a conversion from the <code>from_timezone</code> value to UTC, then a second conversion from UTC to whichever timezone is defined in the column <code>column_with_target_timezone</code>.</li> <li>The reason why this function uses multiple <code>.withColumn()</code> methods, instead of a single <code>.withColumns()</code> expression is because to add the <code>{column}_LOCAL</code> column, it is first necessary for the <code>{column}_UTC</code> column to exist on the <code>dataframe</code>. Therefore, we need to call <code>.withColumn()</code> first to add <code>{column}_UTC</code>, then we need to call <code>.withColumn()</code> a second time to add <code>{column}_LOCAL</code>.</li> </ul> See Also <ul> <li><code>assert_columns_exists()</code></li> <li><code>add_local_datetime_columns()</code></li> </ul> Source code in <code>src/toolbox_pyspark/datetime.py</code> <pre><code>@typechecked\ndef add_local_datetime_column(\n    dataframe: psDataFrame,\n    column: str,\n    from_timezone: Optional[str] = None,\n    column_with_target_timezone: str = \"timezone_location\".upper(),\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For the given `column`, add a new column with the suffix `_LOCAL` which is a conversion of the datetime values from `column` to the desired timezone.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be fixed\n        column (str):\n            The name of the column to do the conversion for. Must exist in `#!py dataframe.columns`, and must be type `typestamp`.\n        from_timezone (str, optional):\n            The timezone which will be converted from. Must be a valid TimeZoneID, for more info, see: [TimeZoneID](https://docs.oracle.com/middleware/12211/wcs/tag-ref/MISC/TimeZones.html).&lt;br&gt;\n            If not given, will default the `from_timezone` to be UTC.&lt;br&gt;\n            Default: `#!py None`.\n        column_with_target_timezone (str, optional):\n            The column containing the target timezone value. By default will be the column `#!py \"timezone_location\"`.&lt;br&gt;\n            Defaults to `#!py \"timezone_location\".upper()`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If `#!py \"column\"` or `column_with_target_timezone` does not exist within `#!py dataframe.columns`.\n        ValueError:\n            If the `from_timezone` or `column_with_target_timezone` is not a valid timezone.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.datetime import add_local_datetime_column\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"D\"),\n        ...             \"d\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"D\"),\n        ...             \"e\": pd.date_range(start=\"2022-03-01\", periods=4, freq=\"D\"),\n        ...             \"target\": [\"Asia/Singapore\"] * 4,\n        ...             \"TIMEZONE_LOCATION\": [\"Australia/Perth\"] * 4,\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n        | a | b |                   c |                   d |                   e |         target | TIMEZONE_LOCATION |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Converting from UTC time\"}\n        &gt;&gt;&gt; add_local_datetime_column(df, \"c\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+\n        | a | b |                   c |                   d |                   e |         target | TIMEZONE_LOCATION |             c_LOCAL |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 08:00:00 |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 08:00:00 |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 08:00:00 |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-04 08:00:00 |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully converted from UTC.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Converting from specific timezone, with custom column containing target timezone\"}\n        &gt;&gt;&gt; add_local_datetime_column(\n        ...     dataframe=df,\n        ...     column=\"c\",\n        ...     from_timezone=\"Australia/Sydney\",\n        ...     column_with_target_timezone=\"target\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        | a | b |                   c |                   d |                   e |         target | TIMEZONE_LOCATION |               c_UTC |             c_LOCAL |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2021-12-31 13:00:00 | 2021-12-31 21:00:00 |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 13:00:00 | 2022-01-01 21:00:00 |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 13:00:00 | 2022-01-02 21:00:00 |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 13:00:00 | 2022-01-03 21:00:00 |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully converted timezone.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid column name\"}\n        &gt;&gt;&gt; add_local_datetime_column(df, \"invalid_column\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column \"invalid_column\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c\", \"d\", \"e\", \"target\", \"TIMEZONE_LOCATION\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Invalid timezone\"}\n        &gt;&gt;&gt; add_local_datetime_column(df, \"c\", from_timezone=\"Invalid/Timezone\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ValueError: The timezone \"Invalid/Timezone\" is not a valid timezone.\n        ```\n        !!! failure \"Conclusion: Invalid timezone.\"\n        &lt;/div&gt;\n\n    ??? info \"Notes\"\n        - If `#!py from_timezone is None`, then it is assumed that the datetime data in `column` is _already_ in UTC timezone.&lt;br&gt;\n        - If `#!py from_timezone is not None`, then a new column will be added with the syntax `#!py {column}_UTC`, then another column added with `#!py {column}_LOCAL`. This is necessary because PySpark cannot convert immediately from one timezone to another; it must first require a conversion from the `from_timezone` value _to_ UTC, then a second conversion _from_ UTC to whichever timezone is defined in the column `column_with_target_timezone`.&lt;br&gt;\n        - The reason why this function uses multiple [`.withColumn()`][withColumn] methods, instead of a single [`.withColumns()`][withColumns] expression is because to add the `#!py {column}_LOCAL` column, it is first necessary for the `#!py {column}_UTC` column to exist on the `dataframe`. Therefore, we need to call [`.withColumn()`][withColumn] first to add `#!py {column}_UTC`, then we need to call [`.withColumn()`][withColumn] a second time to add `#!py {column}_LOCAL`.\n        [withColumn]: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html\n        [withColumns]: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumns.html\n\n    ??? tip \"See Also\"\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`add_local_datetime_columns()`][toolbox_pyspark.datetime.add_local_datetime_columns]\n    \"\"\"\n    assert_columns_exists(\n        dataframe=dataframe, columns=[column, column_with_target_timezone]\n    )\n    require_utc: bool = f\"{column}_UTC\" not in dataframe.columns\n    require_local: bool = f\"{column}_LOCAL\" not in dataframe.columns\n    if from_timezone is not None:\n        if require_utc:\n            dataframe = dataframe.withColumn(\n                f\"{column}_UTC\",\n                F.to_utc_timestamp(\n                    F.col(column).cast(\"timestamp\"),\n                    from_timezone.title(),\n                ),\n            )\n        if require_local:\n            dataframe = dataframe.withColumn(\n                f\"{column}_LOCAL\",\n                F.from_utc_timestamp(\n                    F.col(f\"{column}_UTC\").cast(\"timestamp\"),\n                    F.col(column_with_target_timezone),\n                ),\n            )\n    else:\n        if require_local:\n            dataframe = dataframe.withColumn(\n                f\"{column}_LOCAL\",\n                F.from_utc_timestamp(\n                    F.col(column).cast(\"timestamp\"),\n                    F.col(column_with_target_timezone),\n                ),\n            )\n    return dataframe\n</code></pre>"},{"location":"code/datetime/#toolbox_pyspark.datetime.add_local_datetime_columns","title":"add_local_datetime_columns","text":"<pre><code>add_local_datetime_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n    from_timezone: Optional[str] = None,\n    column_with_target_timezone: str = \"timezone_location\".upper(),\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For each of the <code>data</code> or <code>datetime</code> columns in <code>dataframe</code>, add a new column which is converting it to the timezone of the local datetime.</p> Details <p>Under the hood, this function will call <code>add_local_datetime_column()</code> for each <code>column</code> in <code>columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>The columns to check. If not provided, it will use all of the columns which contains the text <code>date</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>from_timezone</code> <code>Optional[str]</code> <p>The timezone which will be converted from. If not given, will default the from timezone to be UTC. Defaults to <code>None</code>.</p> <code>None</code> <code>column_with_target_timezone</code> <code>str</code> <p>The column containing the target timezone value. By default will be the column <code>\"timezone_location\"</code>. Defaults to <code>\"timezone_location\".upper()</code>.</p> <code>upper()</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <code>ValueError</code> <p>If the <code>from_timezone</code> or <code>column_with_target_timezone</code> is not a valid timezone.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.datetime import add_local_datetime_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"D\"),\n...             \"d_datetime\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"D\"),\n...             \"e_datetime\": pd.date_range(start=\"2022-03-01\", periods=4, freq=\"D\"),\n...             \"target\": [\"Asia/Singapore\"] * 4,\n...             \"TIMEZONE_LOCATION\": [\"Australia/Perth\"] * 4,\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n| a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n</code></pre> </p> <p>Example 1: Default config<pre><code>&gt;&gt;&gt; add_local_datetime_columns(df).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n| a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |    d_datetime_LOCAL |    e_datetime_LOCAL |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-01 08:00:00 | 2022-03-01 08:00:00 |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-02 08:00:00 | 2022-03-02 08:00:00 |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-03 08:00:00 | 2022-03-03 08:00:00 |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-04 08:00:00 | 2022-03-04 08:00:00 |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully converted columns to local timezone.</p> <p>Example 2: Semi-custom config<pre><code>&gt;&gt;&gt; add_local_datetime_columns(df, [\"c\", \"d_datetime\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n| a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |             c_LOCAL |    d_datetime_LOCAL |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 08:00:00 | 2022-02-01 08:00:00 |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 08:00:00 | 2022-02-02 08:00:00 |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 08:00:00 | 2022-02-03 08:00:00 |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-04 08:00:00 | 2022-02-04 08:00:00 |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully converted columns to local timezone.</p> <p>Example 3: Full-custom config<pre><code>&gt;&gt;&gt; add_local_datetime_columns(\n...     dataframe=df,\n...     columns=[\"c\", \"d_datetime\"],\n...     from_timezone=\"Australia/Sydney\",\n...     column_with_target_timezone=\"target\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n| a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |               c_UTC |             c_LOCAL |      d_datetime_UTC |    d_datetime_LOCAL |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2021-12-31 13:00:00 | 2021-12-31 21:00:00 | 2022-01-31 13:00:00 | 2022-02-01 08:00:00 |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 13:00:00 | 2022-01-01 21:00:00 | 2022-02-01 13:00:00 | 2022-02-02 08:00:00 |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 13:00:00 | 2022-01-02 21:00:00 | 2022-02-02 13:00:00 | 2022-02-03 08:00:00 |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 13:00:00 | 2022-01-03 21:00:00 | 2022-02-03 13:00:00 | 2022-02-04 08:00:00 |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully converted columns to local time zone, from other custom time zone.</p> <p>Example 4: Single column<pre><code>&gt;&gt;&gt; add_local_datetime_columns(\n...     dataframe=df,\n...     columns=\"c\",\n...     from_timezone=\"Australia/Sydney\",\n...     column_with_target_timezone=\"target\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n| a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |               c_UTC |             c_LOCAL |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2021-12-31 13:00:00 | 2021-12-31 21:00:00 |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 13:00:00 | 2022-01-01 21:00:00 |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 13:00:00 | 2022-01-02 21:00:00 |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 13:00:00 | 2022-01-03 21:00:00 |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully converted single column from other time zone to local time zone.</p> <p>Example 5: All columns<pre><code>&gt;&gt;&gt; add_local_datetime_columns(\n...     dataframe=df,\n...     columns=\"all\",\n...     from_timezone=\"Australia/Sydney\",\n...     column_with_target_timezone=\"target\",\n... ).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n| a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |      d_datetime_UTC |    d_datetime_LOCAL |      e_datetime_UTC |    e_datetime_LOCAL |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-31 13:00:00 | 2022-02-01 08:00:00 | 2022-02-28 13:00:00 | 2022-03-01 08:00:00 |\n| 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-01 13:00:00 | 2022-02-02 08:00:00 | 2022-03-01 13:00:00 | 2022-03-02 08:00:00 |\n| 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-02 13:00:00 | 2022-02-03 08:00:00 | 2022-03-02 13:00:00 | 2022-03-03 08:00:00 |\n| 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-03 13:00:00 | 2022-02-04 08:00:00 | 2022-03-03 13:00:00 | 2022-03-04 08:00:00 |\n+---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n</code></pre> <p>Conclusion: Successfully converted all date time columns from other time zone to local time zone.</p> <p>Example 6: Invalid column name<pre><code>&gt;&gt;&gt; add_local_datetime_columns(df, \"invalid_column\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column \"invalid_column\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c\", \"d_datetime\", \"e_datetime\", \"target\", \"TIMEZONE_LOCATION\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> <p>Example 7: Invalid timezone<pre><code>&gt;&gt;&gt; add_local_datetime_columns(df, \"c\", from_timezone=\"Invalid/Timezone\")\n</code></pre> Terminal<pre><code>ValueError: The timezone \"Invalid/Timezone\" is not a valid timezone.\n</code></pre> <p>Conclusion: Invalid timezone.</p> See Also <ul> <li><code>add_local_datetime_column()</code></li> </ul> Source code in <code>src/toolbox_pyspark/datetime.py</code> <pre><code>@typechecked\ndef add_local_datetime_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n    from_timezone: Optional[str] = None,\n    column_with_target_timezone: str = \"timezone_location\".upper(),\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For each of the `data` or `datetime` columns in `dataframe`, add a new column which is converting it to the timezone of the local datetime.\n\n    ???+ abstract \"Details\"\n        Under the hood, this function will call [`add_local_datetime_column()`][toolbox_pyspark.datetime.add_local_datetime_column] for each `column` in `columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update.\n        columns (Optional[Union[str, str_collection]], optional):\n            The columns to check. If not provided, it will use all of the columns which contains the text `date`.&lt;br&gt;\n            Defaults to `#!py None`.\n        from_timezone (Optional[str], optional):\n            The timezone which will be converted from. If not given, will default the from timezone to be UTC.&lt;br&gt;\n            Defaults to `#!py None`.\n        column_with_target_timezone (str, optional):\n            The column containing the target timezone value. By default will be the column `#!py \"timezone_location\"`.&lt;br&gt;\n            Defaults to `#!py \"timezone_location\".upper()`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n        ValueError:\n            If the `from_timezone` or `column_with_target_timezone` is not a valid timezone.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.datetime import add_local_datetime_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"D\"),\n        ...             \"d_datetime\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"D\"),\n        ...             \"e_datetime\": pd.date_range(start=\"2022-03-01\", periods=4, freq=\"D\"),\n        ...             \"target\": [\"Asia/Singapore\"] * 4,\n        ...             \"TIMEZONE_LOCATION\": [\"Australia/Perth\"] * 4,\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n        | a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default config\"}\n        &gt;&gt;&gt; add_local_datetime_columns(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        | a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |    d_datetime_LOCAL |    e_datetime_LOCAL |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-01 08:00:00 | 2022-03-01 08:00:00 |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-02 08:00:00 | 2022-03-02 08:00:00 |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-03 08:00:00 | 2022-03-03 08:00:00 |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-04 08:00:00 | 2022-03-04 08:00:00 |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully converted columns to local timezone.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Semi-custom config\"}\n        &gt;&gt;&gt; add_local_datetime_columns(df, [\"c\", \"d_datetime\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        | a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |             c_LOCAL |    d_datetime_LOCAL |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 08:00:00 | 2022-02-01 08:00:00 |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 08:00:00 | 2022-02-02 08:00:00 |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 08:00:00 | 2022-02-03 08:00:00 |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-04 08:00:00 | 2022-02-04 08:00:00 |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully converted columns to local timezone.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Full-custom config\"}\n        &gt;&gt;&gt; add_local_datetime_columns(\n        ...     dataframe=df,\n        ...     columns=[\"c\", \"d_datetime\"],\n        ...     from_timezone=\"Australia/Sydney\",\n        ...     column_with_target_timezone=\"target\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n        | a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |               c_UTC |             c_LOCAL |      d_datetime_UTC |    d_datetime_LOCAL |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2021-12-31 13:00:00 | 2021-12-31 21:00:00 | 2022-01-31 13:00:00 | 2022-02-01 08:00:00 |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 13:00:00 | 2022-01-01 21:00:00 | 2022-02-01 13:00:00 | 2022-02-02 08:00:00 |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 13:00:00 | 2022-01-02 21:00:00 | 2022-02-02 13:00:00 | 2022-02-03 08:00:00 |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 13:00:00 | 2022-01-03 21:00:00 | 2022-02-03 13:00:00 | 2022-02-04 08:00:00 |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully converted columns to local time zone, from other custom time zone.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Single column\"}\n        &gt;&gt;&gt; add_local_datetime_columns(\n        ...     dataframe=df,\n        ...     columns=\"c\",\n        ...     from_timezone=\"Australia/Sydney\",\n        ...     column_with_target_timezone=\"target\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        | a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |               c_UTC |             c_LOCAL |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2021-12-31 13:00:00 | 2021-12-31 21:00:00 |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-01 13:00:00 | 2022-01-01 21:00:00 |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-02 13:00:00 | 2022-01-02 21:00:00 |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-03 13:00:00 | 2022-01-03 21:00:00 |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully converted single column from other time zone to local time zone.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: All columns\"}\n        &gt;&gt;&gt; add_local_datetime_columns(\n        ...     dataframe=df,\n        ...     columns=\"all\",\n        ...     from_timezone=\"Australia/Sydney\",\n        ...     column_with_target_timezone=\"target\",\n        ... ).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n        | a | b |                   c |          d_datetime |          e_datetime |         target | TIMEZONE_LOCATION |      d_datetime_UTC |    d_datetime_LOCAL |      e_datetime_UTC |    e_datetime_LOCAL |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-01-31 13:00:00 | 2022-02-01 08:00:00 | 2022-02-28 13:00:00 | 2022-03-01 08:00:00 |\n        | 2 | b | 2022-01-02 00:00:00 | 2022-02-02 00:00:00 | 2022-03-02 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-01 13:00:00 | 2022-02-02 08:00:00 | 2022-03-01 13:00:00 | 2022-03-02 08:00:00 |\n        | 3 | c | 2022-01-03 00:00:00 | 2022-02-03 00:00:00 | 2022-03-03 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-02 13:00:00 | 2022-02-03 08:00:00 | 2022-03-02 13:00:00 | 2022-03-03 08:00:00 |\n        | 4 | d | 2022-01-04 00:00:00 | 2022-02-04 00:00:00 | 2022-03-04 00:00:00 | Asia/Singapore |   Australia/Perth | 2022-02-03 13:00:00 | 2022-02-04 08:00:00 | 2022-03-03 13:00:00 | 2022-03-04 08:00:00 |\n        +---+---+---------------------+---------------------+---------------------+----------------+-------------------+---------------------+---------------------+---------------------+---------------------+\n        ```\n        !!! success \"Conclusion: Successfully converted all date time columns from other time zone to local time zone.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Invalid column name\"}\n        &gt;&gt;&gt; add_local_datetime_columns(df, \"invalid_column\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column \"invalid_column\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c\", \"d_datetime\", \"e_datetime\", \"target\", \"TIMEZONE_LOCATION\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 7: Invalid timezone\"}\n        &gt;&gt;&gt; add_local_datetime_columns(df, \"c\", from_timezone=\"Invalid/Timezone\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ValueError: The timezone \"Invalid/Timezone\" is not a valid timezone.\n        ```\n        !!! failure \"Conclusion: Invalid timezone.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`add_local_datetime_column()`][toolbox_pyspark.datetime.add_local_datetime_column]\n    \"\"\"\n    if columns is None or columns in [\"all\"]:\n        columns = [col for col in dataframe.columns if col.lower().endswith(\"datetime\")]\n    elif is_type(columns, str):\n        columns = [columns]\n    assert_columns_exists(dataframe, list(columns) + [column_with_target_timezone])\n    for column in columns:\n        dataframe = add_local_datetime_column(\n            dataframe=dataframe,\n            column=column,\n            from_timezone=from_timezone,\n            column_with_target_timezone=column_with_target_timezone,\n        )\n    return dataframe\n</code></pre>"},{"location":"code/datetime/#toolbox_pyspark.datetime.split_datetime_column","title":"split_datetime_column","text":"<pre><code>split_datetime_column(\n    dataframe: psDataFrame, column: str\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Take the column <code>column</code>, which should be a <code>timestamp</code> type, and split it in to it's respective <code>date</code> and <code>time</code> components.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update.</p> required <code>column</code> <code>str</code> <p>The column to split.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If the <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <code>TypeError</code> <p>If the <code>column</code> is not type <code>timestamp</code> or <code>datetime</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.datetime import split_datetime_column\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c_datetime\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"h\"),\n...             \"d_datetime\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"h\"),\n...             \"e_datetime\": pd.date_range(start=\"2022-03-01\", periods=4, freq=\"h\"),\n...             \"TIMEZONE_LOCATION\": [\"Australia/Perth\"] * 4,\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+-------------------+\n| a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |\n+---+---+---------------------+---------------------+---------------------+-------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth |\n| 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth |\n| 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth |\n| 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth |\n+---+---+---------------------+---------------------+---------------------+-------------------+\n</code></pre> </p> <p>Example 1: Default config<pre><code>&gt;&gt;&gt; split_datetime_column(df, \"c_datetime\").show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n| a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 |\n| 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 |\n| 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 |\n| 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n</code></pre> <p>Conclusion: Successfully split the column in to it's Date and Time constituents.</p> <p>Example 2: Invalid column name<pre><code>&gt;&gt;&gt; split_datetime_column(df, \"invalid_column\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column \"invalid_column\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c_datetime\", \"d_datetime\", \"e_datetime\", \"TIMEZONE_LOCATION\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> <p>Example 2: Invalid column name<pre><code>&gt;&gt;&gt; split_datetime_column(df, \"b\")\n</code></pre> Terminal<pre><code>TypeError: Column must be type 'timestamp' or 'datetime'.\nCurrent type: [('b', 'string')]\n</code></pre> <p>Conclusion: Column is not the correct type for splitting.</p> See Also <ul> <li><code>split_datetime_columns()</code></li> </ul> Source code in <code>src/toolbox_pyspark/datetime.py</code> <pre><code>@typechecked\ndef split_datetime_column(\n    dataframe: psDataFrame,\n    column: str,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Take the column `column`, which should be a `timestamp` type, and split it in to it's respective `date` and `time` components.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update.\n        column (str):\n            The column to split.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If the `#!py column` does not exist within `#!py dataframe.columns`.\n        TypeError:\n            If the `column` is not type `timestamp` or `datetime`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.datetime import split_datetime_column\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c_datetime\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"h\"),\n        ...             \"d_datetime\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"h\"),\n        ...             \"e_datetime\": pd.date_range(start=\"2022-03-01\", periods=4, freq=\"h\"),\n        ...             \"TIMEZONE_LOCATION\": [\"Australia/Perth\"] * 4,\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+-------------------+\n        | a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |\n        +---+---+---------------------+---------------------+---------------------+-------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth |\n        | 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth |\n        | 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth |\n        | 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth |\n        +---+---+---------------------+---------------------+---------------------+-------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default config\"}\n        &gt;&gt;&gt; split_datetime_column(df, \"c_datetime\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n        | a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 |\n        | 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 |\n        | 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 |\n        | 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n        ```\n        !!! success \"Conclusion: Successfully split the column in to it's Date and Time constituents.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Invalid column name\"}\n        &gt;&gt;&gt; split_datetime_column(df, \"invalid_column\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column \"invalid_column\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c_datetime\", \"d_datetime\", \"e_datetime\", \"TIMEZONE_LOCATION\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Invalid column name\"}\n        &gt;&gt;&gt; split_datetime_column(df, \"b\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        TypeError: Column must be type 'timestamp' or 'datetime'.\n        Current type: [('b', 'string')]\n        ```\n        !!! failure \"Conclusion: Column is not the correct type for splitting.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`split_datetime_columns()`][toolbox_pyspark.datetime.split_datetime_columns]\n    \"\"\"\n    assert_column_exists(dataframe, column)\n    datetime_cols: str_list = get_columns(dataframe, \"all_datetime\")\n    if not is_in(column, datetime_cols):\n        raise TypeError(\n            \"Column must be type 'timestamp' or 'datetime'.\\n\"\n            f\"Current type: {[(col,typ) for col,typ in dataframe.dtypes if col == column]}\"\n        )\n    col_date_name: str = column.upper().replace(\"DATETIME\", \"DATE\")\n    col_time_name: str = column.upper().replace(\"DATETIME\", \"TIME\")\n    col_date_value: Column = (\n        F.date_format(column, \"yyyy-MM-dd\").cast(\"string\").cast(\"date\")\n    )\n    col_time_value: Column = F.date_format(column, \"HH:mm:ss\").cast(\"string\")\n    return dataframe.withColumns(\n        {\n            col_date_name: col_date_value,\n            col_time_name: col_time_value,\n        }\n    )\n</code></pre>"},{"location":"code/datetime/#toolbox_pyspark.datetime.split_datetime_columns","title":"split_datetime_columns","text":"<pre><code>split_datetime_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For all the columns listed in <code>columns</code>, split them each in to their respective <code>date</code> and <code>time</code> components.</p> Details <p>The reason why this function is structured this way, and not re-calling <code>split_datetime_column()</code> in each iteration of <code>columns</code> is due to <code>pyspark</code> RDD complexity. More specifically, if it were to call <code>split_datetime_column()</code> each time, the RDD would get incredibly and unnecessarily complicated. However, by doing it this way, using the <code>.withColumns()</code> method, it will project the SQL expression once down to the underlying dataframe; not multiple times. Therefore, in this way, the underlying SQL execution plan is now much less complicated; albeit that the coding DRY principle is not strictly being followed here.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to update.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>The list of columns to update. If not given, it will generate the list of columns from the <code>dataframe.columns</code> which contain the text <code>datetime</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <code>TypeError</code> <p>If any of the columns in <code>columns</code> are not type <code>timestamp</code> or <code>datetime</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.datetime import split_datetime_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c_datetime\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"h\"),\n...             \"d_datetime\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"h\"),\n...             \"e_datetime\": pd.date_range(start=\"2022-03-01\", periods=4, freq=\"h\"),\n...             \"TIMEZONE_LOCATION\": [\"Australia/Perth\"] * 4,\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+-------------------+\n| a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |\n+---+---+---------------------+---------------------+---------------------+-------------------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth |\n| 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth |\n| 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth |\n| 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth |\n+---+---+---------------------+---------------------+---------------------+-------------------+\n</code></pre> </p> <p>Example 1: Default config<pre><code>&gt;&gt;&gt; split_datetime_columns(df).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n| a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |     D_DATE |   D_TIME |     E_DATE |   E_TIME |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 | 2022-02-01 | 00:00:00 | 2022-03-01 | 00:00:00 |\n| 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 | 2022-02-01 | 01:00:00 | 2022-03-01 | 01:00:00 |\n| 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 | 2022-02-01 | 02:00:00 | 2022-03-01 | 02:00:00 |\n| 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 | 2022-02-01 | 03:00:00 | 2022-03-01 | 03:00:00 |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n</code></pre> <p>Conclusion: Successfully split all DateTime columns in to their Date and Time constituents.</p> <p>Example 2: Custom config<pre><code>&gt;&gt;&gt; split_datetime_columns(df, [\"c_datetime\", \"d_datetime\"]).show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+\n| a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |     D_DATE |   D_TIME |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 | 2022-02-01 | 00:00:00 |\n| 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 | 2022-02-01 | 01:00:00 |\n| 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 | 2022-02-01 | 02:00:00 |\n| 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 | 2022-02-01 | 03:00:00 |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+\n</code></pre> <p>Conclusion: Successfully split two columns into their Date and Time constituents.</p> <p>Example 3: All columns<pre><code>&gt;&gt;&gt; split_datetime_columns(df, \"all\").show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n| a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |     D_DATE |   D_TIME |     E_DATE |   E_TIME |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 | 2022-02-01 | 00:00:00 | 2022-03-01 | 00:00:00 |\n| 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 | 2022-02-01 | 01:00:00 | 2022-03-01 | 01:00:00 |\n| 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 | 2022-02-01 | 02:00:00 | 2022-03-01 | 02:00:00 |\n| 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 | 2022-02-01 | 03:00:00 | 2022-03-01 | 03:00:00 |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n</code></pre> <p>Conclusion: Successfully split all DateTime columns in to their Date and Time constituents.</p> <p>Example 4: Single column<pre><code>&gt;&gt;&gt; split_datetime_columns(df, \"c_datetime\").show()\n</code></pre> Terminal<pre><code>+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n| a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n| 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 |\n| 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 |\n| 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 |\n| 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 |\n+---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n</code></pre> <p>Conclusion: Successfully split a single column in to it's Date and Time constituents.</p> <p>Example 5: Invalid column name<pre><code>&gt;&gt;&gt; split_datetime_columns(df, \"invalid_column\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column \"invalid_column\" does not exist in \"dataframe\".\nTry one of: [\"a\", \"b\", \"c_datetime\", \"d_datetime\", \"e_datetime\", \"TIMEZONE_LOCATION\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> <p>Example 6: Invalid column type<pre><code>&gt;&gt;&gt; split_datetime_columns(df, \"b\")\n</code></pre> Terminal<pre><code>TypeError: Column must be type 'timestamp' or 'datetime'.\nCurrent type: [('b', 'string')]\n</code></pre> <p>Conclusion: Column is not the correct type for splitting.</p> See Also <ul> <li><code>split_datetime_column()</code></li> </ul> Source code in <code>src/toolbox_pyspark/datetime.py</code> <pre><code>@typechecked\ndef split_datetime_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For all the columns listed in `columns`, split them each in to their respective `date` and `time` components.\n\n    ???+ abstract \"Details\"\n        The reason why this function is structured this way, and not re-calling [`split_datetime_column()`][toolbox_pyspark.datetime.split_datetime_column] in each iteration of `columns` is due to `#!py pyspark` RDD complexity. More specifically, if it _were_ to call [`split_datetime_column()`][toolbox_pyspark.datetime.split_datetime_column] each time, the RDD would get incredibly and unnecessarily complicated. However, by doing it this way, using the [`.withColumns()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumns.html) method, it will project the SQL expression **once** down to the underlying dataframe; not multiple times. Therefore, in this way, the underlying SQL execution plan is now much less complicated; albeit that the coding DRY principle is not strictly being followed here.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to update.\n        columns (Optional[Union[str, str_collection]], optional):\n            The list of columns to update. If not given, it will generate the list of columns from the `#!py dataframe.columns` which contain the text `datetime`.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n        TypeError:\n            If any of the columns in `columns` are not type `timestamp` or `datetime`.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.datetime import split_datetime_columns\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c_datetime\": pd.date_range(start=\"2022-01-01\", periods=4, freq=\"h\"),\n        ...             \"d_datetime\": pd.date_range(start=\"2022-02-01\", periods=4, freq=\"h\"),\n        ...             \"e_datetime\": pd.date_range(start=\"2022-03-01\", periods=4, freq=\"h\"),\n        ...             \"TIMEZONE_LOCATION\": [\"Australia/Perth\"] * 4,\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+-------------------+\n        | a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |\n        +---+---+---------------------+---------------------+---------------------+-------------------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth |\n        | 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth |\n        | 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth |\n        | 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth |\n        +---+---+---------------------+---------------------+---------------------+-------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Default config\"}\n        &gt;&gt;&gt; split_datetime_columns(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n        | a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |     D_DATE |   D_TIME |     E_DATE |   E_TIME |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 | 2022-02-01 | 00:00:00 | 2022-03-01 | 00:00:00 |\n        | 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 | 2022-02-01 | 01:00:00 | 2022-03-01 | 01:00:00 |\n        | 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 | 2022-02-01 | 02:00:00 | 2022-03-01 | 02:00:00 |\n        | 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 | 2022-02-01 | 03:00:00 | 2022-03-01 | 03:00:00 |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n        ```\n        !!! success \"Conclusion: Successfully split all DateTime columns in to their Date and Time constituents.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Custom config\"}\n        &gt;&gt;&gt; split_datetime_columns(df, [\"c_datetime\", \"d_datetime\"]).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+\n        | a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |     D_DATE |   D_TIME |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 | 2022-02-01 | 00:00:00 |\n        | 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 | 2022-02-01 | 01:00:00 |\n        | 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 | 2022-02-01 | 02:00:00 |\n        | 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 | 2022-02-01 | 03:00:00 |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+\n        ```\n        !!! success \"Conclusion: Successfully split two columns into their Date and Time constituents.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: All columns\"}\n        &gt;&gt;&gt; split_datetime_columns(df, \"all\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n        | a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |     D_DATE |   D_TIME |     E_DATE |   E_TIME |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 | 2022-02-01 | 00:00:00 | 2022-03-01 | 00:00:00 |\n        | 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 | 2022-02-01 | 01:00:00 | 2022-03-01 | 01:00:00 |\n        | 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 | 2022-02-01 | 02:00:00 | 2022-03-01 | 02:00:00 |\n        | 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 | 2022-02-01 | 03:00:00 | 2022-03-01 | 03:00:00 |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+------------+----------+------------+----------+\n        ```\n        !!! success \"Conclusion: Successfully split all DateTime columns in to their Date and Time constituents.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Single column\"}\n        &gt;&gt;&gt; split_datetime_columns(df, \"c_datetime\").show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n        | a | b |          c_datetime |          d_datetime |          e_datetime | TIMEZONE_LOCATION |     C_DATE |   C_TIME |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n        | 1 | a | 2022-01-01 00:00:00 | 2022-02-01 00:00:00 | 2022-03-01 00:00:00 |   Australia/Perth | 2022-01-01 | 00:00:00 |\n        | 2 | b | 2022-01-01 01:00:00 | 2022-02-01 01:00:00 | 2022-03-01 01:00:00 |   Australia/Perth | 2022-01-01 | 01:00:00 |\n        | 3 | c | 2022-01-01 02:00:00 | 2022-02-01 02:00:00 | 2022-03-01 02:00:00 |   Australia/Perth | 2022-01-01 | 02:00:00 |\n        | 4 | d | 2022-01-01 03:00:00 | 2022-02-01 03:00:00 | 2022-03-01 03:00:00 |   Australia/Perth | 2022-01-01 | 03:00:00 |\n        +---+---+---------------------+---------------------+---------------------+-------------------+------------+----------+\n        ```\n        !!! success \"Conclusion: Successfully split a single column in to it's Date and Time constituents.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 5: Invalid column name\"}\n        &gt;&gt;&gt; split_datetime_columns(df, \"invalid_column\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column \"invalid_column\" does not exist in \"dataframe\".\n        Try one of: [\"a\", \"b\", \"c_datetime\", \"d_datetime\", \"e_datetime\", \"TIMEZONE_LOCATION\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 6: Invalid column type\"}\n        &gt;&gt;&gt; split_datetime_columns(df, \"b\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        TypeError: Column must be type 'timestamp' or 'datetime'.\n        Current type: [('b', 'string')]\n        ```\n        !!! failure \"Conclusion: Column is not the correct type for splitting.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`split_datetime_column()`][toolbox_pyspark.datetime.split_datetime_column]\n    \"\"\"\n    if columns is None or columns in [\"all\"]:\n        columns = [col for col in dataframe.columns if \"datetime\" in col.lower()]\n    elif is_type(columns, str):\n        columns = [columns]\n    assert_columns_exists(dataframe, columns)\n    datetime_cols: str_list = get_columns(dataframe, \"all_datetime\")\n    if not is_all_in(columns, datetime_cols):\n        raise TypeError(\n            \"Columns to split must be type 'timestamp' or 'datetime'.\\n\"\n            f\"Current types: {[(col,typ) for col,typ in dataframe.dtypes if col in columns]}\"\n        )\n    cols_exprs: dict[str, Column] = {}\n    for column in columns:\n        col_date_name: str = column.upper().replace(\"DATETIME\", \"DATE\")\n        col_time_name: str = column.upper().replace(\"DATETIME\", \"TIME\")\n        col_date_value: Column = (\n            F.date_format(column, \"yyyy-MM-dd\").cast(\"string\").cast(\"date\")\n        )\n        col_time_value: Column = F.date_format(column, \"HH:mm:ss\").cast(\"string\")\n        cols_exprs[col_date_name] = col_date_value\n        cols_exprs[col_time_name] = col_time_value\n    return dataframe.withColumns(cols_exprs)\n</code></pre>"},{"location":"code/dimensions/","title":"Dimensions","text":""},{"location":"code/dimensions/#toolbox_pyspark.dimensions","title":"toolbox_pyspark.dimensions","text":"<p>Summary</p> <p>The <code>dimensions</code> module is used for checking the dimensions of <code>pyspark</code> <code>dataframe</code>'s.</p>"},{"location":"code/dimensions/#toolbox_pyspark.dimensions.get_dims","title":"get_dims","text":"<pre><code>get_dims(\n    dataframe: psDataFrame,\n    use_names: bool = True,\n    use_comma: bool = True,\n) -&gt; Union[\n    dict[str, str],\n    dict[str, int],\n    tuple[str, str],\n    tuple[int, int],\n]\n</code></pre> <p>Summary</p> <p>Extract the dimensions of a given <code>dataframe</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The table to check.</p> required <code>use_names</code> <code>bool</code> <p>Whether or not to add <code>names</code> to the returned object. If <code>True</code>, then will return a <code>dict</code> with two keys only, for the number of <code>rows</code> and <code>cols</code>. Defaults to <code>True</code>.</p> <code>True</code> <code>use_comma</code> <code>bool</code> <p>Whether or not to add a comma <code>,</code> to the returned object. Defaults to <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, Union[str, int]], tuple[str, ...], tTuple[int, ...]]</code> <p>The dimensions of the given <code>dataframe</code>.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.dimensions import get_dims\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame({\n...         'a': range(5000),\n...         'b': range(5000),\n...     })\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; print(df.count())\n&gt;&gt;&gt; print(len(df.columns))\n</code></pre> <pre><code>5000\n</code></pre> <p><pre><code>2\n</code></pre> </p> <p>Names and commas<pre><code>&gt;&gt;&gt; print(get_dims(dataframe=df, use_names=True, use_commas=True))\n</code></pre> <pre><code>{\"rows\": \"5,000\", \"cols\": \"2\"}\n</code></pre> </p> <p>Names but no commas<pre><code>&gt;&gt;&gt; print(get_dims(dataframe=df, use_names=True, use_commas=False))\n</code></pre> <pre><code>{\"rows\": 5000, \"cols\": 2}\n</code></pre> </p> <p>Commas but no names<pre><code>&gt;&gt;&gt; print(get_dims(dataframe=df, use_names=False, use_commas=True))\n</code></pre> <pre><code>(\"5,000\", \"2\")\n</code></pre> </p> <p>Neither names nor commas<pre><code>&gt;&gt;&gt; print(get_dims(dataframe=df, use_names=False, use_commas=False))\n</code></pre> <pre><code>(5000, 2)\n</code></pre> </p> Source code in <code>src/toolbox_pyspark/dimensions.py</code> <pre><code>@typechecked\ndef get_dims(\n    dataframe: psDataFrame,\n    use_names: bool = True,\n    use_comma: bool = True,\n) -&gt; Union[dict[str, str], dict[str, int], tuple[str, str], tuple[int, int]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Extract the dimensions of a given `dataframe`.\n\n    Params:\n        dataframe (psDataFrame):\n            The table to check.\n        use_names (bool, optional):\n            Whether or not to add `names` to the returned object.&lt;br&gt;\n            If `#!py True`, then will return a `#!py dict` with two keys only, for the number of `rows` and `cols`.&lt;br&gt;\n            Defaults to `#!py True`.\n        use_comma (bool, optional):\n            Whether or not to add a comma `,` to the returned object.&lt;br&gt;\n            Defaults to `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (Union[Dict[str, Union[str, int]], tuple[str, ...], tTuple[int, ...]]):\n            The dimensions of the given `dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.dimensions import get_dims\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame({\n        ...         'a': range(5000),\n        ...         'b': range(5000),\n        ...     })\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; print(df.count())\n        &gt;&gt;&gt; print(len(df.columns))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n        5000\n        ```\n\n        ```{.txt .text}\n        2\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Names and commas\"}\n        &gt;&gt;&gt; print(get_dims(dataframe=df, use_names=True, use_commas=True))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        {\"rows\": \"5,000\", \"cols\": \"2\"}\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Names but no commas\"}\n        &gt;&gt;&gt; print(get_dims(dataframe=df, use_names=True, use_commas=False))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        {\"rows\": 5000, \"cols\": 2}\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Commas but no names\"}\n        &gt;&gt;&gt; print(get_dims(dataframe=df, use_names=False, use_commas=True))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        (\"5,000\", \"2\")\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Neither names nor commas\"}\n        &gt;&gt;&gt; print(get_dims(dataframe=df, use_names=False, use_commas=False))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        (5000, 2)\n        ```\n        &lt;/div&gt;\n    \"\"\"\n    dims: tuple[int, int] = (dataframe.count(), len(dataframe.columns))\n    if use_names and use_comma:\n        return {\"rows\": f\"{dims[0]:,}\", \"cols\": f\"{dims[1]:,}\"}\n    elif use_names and not use_comma:\n        return {\"rows\": dims[0], \"cols\": dims[1]}\n    elif not use_names and use_comma:\n        return (f\"{dims[0]:,}\", f\"{dims[1]:,}\")\n    else:\n        return dims\n</code></pre>"},{"location":"code/dimensions/#toolbox_pyspark.dimensions.get_dims_of_tables","title":"get_dims_of_tables","text":"<pre><code>get_dims_of_tables(\n    tables: str_list,\n    scope: Optional[dict] = None,\n    use_comma: bool = True,\n) -&gt; pdDataFrame\n</code></pre> <p>Summary</p> <p>Take in a list of the names of some tables, and for each of them, check their dimensions.</p> Details <p>This function will check against the <code>global()</code> scope. So you need to be careful if you're dealing with massive amounts of data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>tables</code> <code>List[str]</code> <p>The list of the tables that will be checked.</p> required <code>scope</code> <code>dict</code> <p>This is the scope against which the tables will be checked. If <code>None</code>, then it will use the <code>global()</code> scope by default.. Defaults to <code>None</code>.</p> <code>None</code> <code>use_comma</code> <code>bool</code> <p>Whether or not the dimensions from the tables should be formatted as a string with a comma as the thousandths delimiter. Defaults to <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A <code>pandas</code> <code>dataframe</code> with four columns: <code>[\"table\", \"type\", \"rows\", \"cols\"]</code>.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.dimensions import get_dims_of_tables, get_dims\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df1 = spark.createDataFrame(\n...     pd.DataFrame({\n...         'a': range(5000),\n...         'b': range(5000),\n...     })\n... )\n&gt;&gt;&gt; df2 = spark.createDataFrame(\n...     pd.DataFrame({\n...         'a': range(10000),\n...         'b': range(10000),\n...         'c': range(10000),\n...     })\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; print(get_dims(df1))\n&gt;&gt;&gt; print(get_dims(df1))\n</code></pre> <pre><code>{\"rows\": \"5000\", \"cols\": \"2\"}\n</code></pre> <p><pre><code>{\"rows\": \"10000\", \"cols\": \"3\"}\n</code></pre> </p> <p>Basic usage<pre><code>&gt;&gt;&gt; print(get_dims_of_tables(['df1', 'df2']))\n</code></pre> <pre><code>  table type  rows cols\n0   df1      5,000    2\n1   df2      1,000    3\n</code></pre> </p> <p>No commas<pre><code>&gt;&gt;&gt; print(get_dims_of_tables(['df1', 'df2'], use_commas=False))\n</code></pre> <pre><code>  table type rows cols\n0   df1      5000    2\n1   df2      1000    3\n</code></pre> </p> <p>Missing DF<pre><code>&gt;&gt;&gt; display(get_dims_of_tables(['df1', 'df2', 'df3'], use_comma=False))\n</code></pre> <pre><code>  table type rows cols\n0   df1      5000    2\n1   df2      1000    3\n1   df3       NaN  NaN\n</code></pre> </p> Notes <ul> <li>The first column of the returned table is the name of the table from the <code>scope</code> provided.</li> <li>The second column of the returned table is the <code>type</code> of the table. That is, whether the table is one of <code>[\"prd\", \"arc\", \"acm\"]</code>, which are for 'production', 'archive', accumulation' categories. This is designated by the table containing an underscore (<code>_</code>), and having a suffic of either one of: <code>\"prd\"</code>, <code>\"arc\"</code>, or <code>\"acm\"</code>. If the table does not contain this info, then the value in this second column will just be blank.</li> <li>If one of the tables given in the <code>tables</code> list does not exist in the <code>scope</code>, then the values given in the <code>rows</code> and <code>cols</code> columns will either be the values: <code>np.nan</code> or <code>\"Did not load\"</code>.</li> </ul> Source code in <code>src/toolbox_pyspark/dimensions.py</code> <pre><code>@typechecked\ndef get_dims_of_tables(\n    tables: str_list,\n    scope: Optional[dict] = None,\n    use_comma: bool = True,\n) -&gt; pdDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Take in a list of the names of some tables, and for each of them, check their dimensions.\n\n    ???+ abstract \"Details\"\n        This function will check against the `#!py global()` scope. So you need to be careful if you're dealing with massive amounts of data in memory.\n\n    Params:\n        tables (List[str]):\n            The list of the tables that will be checked.\n        scope (dict, optional):\n            This is the scope against which the tables will be checked.&lt;br&gt;\n            If `#!py None`, then it will use the `#!py global()` scope by default..&lt;br&gt;\n            Defaults to `#!py None`.\n        use_comma (bool, optional):\n            Whether or not the dimensions from the tables should be formatted as a string with a comma as the thousandths delimiter.&lt;br&gt;\n            Defaults to `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (pdDataFrame):\n            A `pandas` `dataframe` with four columns: `#!py [\"table\", \"type\", \"rows\", \"cols\"]`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.dimensions import get_dims_of_tables, get_dims\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df1 = spark.createDataFrame(\n        ...     pd.DataFrame({\n        ...         'a': range(5000),\n        ...         'b': range(5000),\n        ...     })\n        ... )\n        &gt;&gt;&gt; df2 = spark.createDataFrame(\n        ...     pd.DataFrame({\n        ...         'a': range(10000),\n        ...         'b': range(10000),\n        ...         'c': range(10000),\n        ...     })\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; print(get_dims(df1))\n        &gt;&gt;&gt; print(get_dims(df1))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n        {\"rows\": \"5000\", \"cols\": \"2\"}\n        ```\n\n        ```{.txt .text}\n        {\"rows\": \"10000\", \"cols\": \"3\"}\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; print(get_dims_of_tables(['df1', 'df2']))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n          table type  rows cols\n        0   df1      5,000    2\n        1   df2      1,000    3\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"No commas\"}\n        &gt;&gt;&gt; print(get_dims_of_tables(['df1', 'df2'], use_commas=False))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n          table type rows cols\n        0   df1      5000    2\n        1   df2      1000    3\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Missing DF\"}\n        &gt;&gt;&gt; display(get_dims_of_tables(['df1', 'df2', 'df3'], use_comma=False))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n          table type rows cols\n        0   df1      5000    2\n        1   df2      1000    3\n        1   df3       NaN  NaN\n        ```\n        &lt;/div&gt;\n\n    ??? info \"Notes\"\n        - The first column of the returned table is the name of the table from the `scope` provided.\n        - The second column of the returned table is the `type` of the table. That is, whether the table is one of `#!py [\"prd\", \"arc\", \"acm\"]`, which are for 'production', 'archive', accumulation' categories. This is designated by the table containing an underscore (`_`), and having a suffic of either one of: `#!py \"prd\"`, `#!py \"arc\"`, or `#!py \"acm\"`. If the table does not contain this info, then the value in this second column will just be blank.\n        - If one of the tables given in the `tables` list does not exist in the `scope`, then the values given in the `rows` and `cols` columns will either be the values: `#!py np.nan` or `#!py \"Did not load\"`.\n    \"\"\"\n    sizes: Dict[str, list] = {\n        \"table\": list(),\n        \"type\": list(),\n        \"rows\": list(),\n        \"cols\": list(),\n    }\n    rows: Union[str, int, float]\n    cols: Union[str, int, float]\n    for tbl, typ in [\n        (\n            table.rsplit(\"_\", 1)\n            if \"_\" in table and table.endswith((\"acm\", \"arc\", \"prd\"))\n            else (table, \"\")\n        )\n        for table in tables\n    ]:\n        try:\n            tmp: psDataFrame = eval(\n                f\"{tbl}{f'_{typ}' if typ!='' else ''}\",\n                globals() if scope is None else scope,\n            )\n            rows, cols = get_dims(tmp, use_names=False, use_comma=use_comma)\n        except Exception:\n            if use_comma:\n                rows = cols = \"Did not load\"\n            else:\n                rows = cols = np.nan\n        sizes[\"table\"].append(tbl)\n        sizes[\"type\"].append(typ)\n        sizes[\"rows\"].append(rows)\n        sizes[\"cols\"].append(cols)\n    return pdDataFrame(sizes)\n</code></pre>"},{"location":"code/io/","title":"IO","text":""},{"location":"code/io/#toolbox_pyspark.io","title":"toolbox_pyspark.io","text":"<p>Summary</p> <p>The <code>io</code> module is used for reading and writing tables to/from directories.</p>"},{"location":"code/io/#toolbox_pyspark.io.read_from_path","title":"read_from_path","text":"<pre><code>read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Read an object from a given <code>path</code> in to memory as a <code>pyspark</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to read in.</p> required <code>path</code> <code>str</code> <p>The path from which it will be read.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The Spark session to use for the reading.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format of the object at location <code>path</code>. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>read_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. Like, for example:</p> <ul> <li>If the object is a CSV, you may want to define that it has a header row: <code>{\"header\": \"true\"}</code>.</li> <li>If the object is a Delta table, you may want to query a specific version: <code>{versionOf\": \"0\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The loaded dataframe.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         \"c\": [1, 1, 1, 1],\n...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...     }\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write data\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> Terminal<pre><code>[\"table.csv\", \"table.parquet\"]\n</code></pre> </p> <p>Example 1: Read CSV<pre><code>&gt;&gt;&gt; df_csv = read_from_path(\n...     name=\"table.csv\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"csv\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_csv.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully read CSV.</p> <p>Example 2: Read Parquet<pre><code>&gt;&gt;&gt; df_parquet = read_from_path(\n...     name=\"table.parquet\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"parquet\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_parquet.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully read Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Read an object from a given `path` in to memory as a `pyspark` dataframe.\n\n    Params:\n        name (str):\n            The name of the table to read in.\n        path (str):\n            The path from which it will be read.\n        spark_session (SparkSession):\n            The Spark session to use for the reading.\n        data_format (Optional[str], optional):\n            The format of the object at location `path`.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        read_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            Like, for example:&lt;br&gt;\n\n            - If the object is a CSV, you may want to define that it has a header row: `#!py {\"header\": \"true\"}`.\n            - If the object is a Delta table, you may want to query a specific version: `#!py {versionOf\": \"0\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The loaded dataframe.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4],\n        ...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         \"c\": [1, 1, 1, 1],\n        ...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Write data\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"table.csv\", \"table.parquet\"]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Read CSV\"}\n        &gt;&gt;&gt; df_csv = read_from_path(\n        ...     name=\"table.csv\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"csv\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df_csv.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully read CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Read Parquet\"}\n        &gt;&gt;&gt; df_parquet = read_from_path(\n        ...     name=\"table.parquet\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"parquet\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df_parquet.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully read Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    data_format: str = data_format or \"parquet\"\n    reader: DataFrameReader = spark_session.read.format(data_format)\n    if read_options:\n        reader.options(**read_options)\n    return reader.load(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.write_to_path","title":"write_to_path","text":"<pre><code>write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>For a given <code>table</code>, write it out to a specified <code>path</code> with name <code>name</code> and format <code>format</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame</code> <p>The table to be written. Must be a valid <code>pyspark</code> DataFrame (<code>pyspark.sql.DataFrame</code>).</p> required <code>name</code> <code>str</code> <p>The name of the table where it will be written.</p> required <code>path</code> <code>str</code> <p>The path location for where to save the table.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format that the <code>table</code> will be written to. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>write_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. Like, for example:</p> <ul> <li>If you are writing to a Delta object, and wanted to overwrite the schema: <code>{\"overwriteSchema\": \"true\"}</code>.</li> <li>If you\"re writing to a CSV file, and wanted to specify the header row: <code>{\"header\": \"true\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Note <p>You know that this function is successful if the table exists at the specified location, and there are no errors thrown.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Write to CSV<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     mode=\"overwrite\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully written to CSV.</p> <p>Example 2: Write to Parquet<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     mode=\"overwrite\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully written to Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `table`, write it out to a specified `path` with name `name` and format `format`.\n\n    Params:\n        table (psDataFrame):\n            The table to be written. Must be a valid `pyspark` DataFrame ([`pyspark.sql.DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)).\n        name (str):\n            The name of the table where it will be written.\n        path (str):\n            The path location for where to save the table.\n        data_format (Optional[str], optional):\n            The format that the `table` will be written to.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        write_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            Like, for example:\n\n            - If you are writing to a Delta object, and wanted to overwrite the schema: `#!py {\"overwriteSchema\": \"true\"}`.\n            - If you\"re writing to a CSV file, and wanted to specify the header row: `#!py {\"header\": \"true\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n        partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ tip \"Note\"\n        You know that this function is successful if the table exists at the specified location, and there are no errors thrown.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Write to CSV\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     mode=\"overwrite\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully written to CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Write to Parquet\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     mode=\"overwrite\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully written to Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    write_options: str_dict = write_options or dict()\n    data_format: str = data_format or \"parquet\"\n    writer: DataFrameWriter = table.write.mode(mode).format(data_format)\n    if write_options:\n        writer.options(**write_options)\n    if partition_cols is not None:\n        partition_cols = (\n            [partition_cols] if is_type(partition_cols, str) else partition_cols\n        )\n        writer = writer.partitionBy(list(partition_cols))\n    writer.save(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.transfer_table","title":"transfer_table","text":"<pre><code>transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[\n        str_collection\n    ] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Copy a table from one location to another.</p> Details <p>This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.</p> <p>Parameters:</p> Name Type Description Default <code>spark_session</code> <code>SparkSession</code> <p>The spark session to use for the transfer. Necessary in order to instantiate the reading process.</p> required <code>from_table_path</code> <code>str</code> <p>The path from which the table will be read.</p> required <code>from_table_name</code> <code>str</code> <p>The name of the table to be read.</p> required <code>from_table_format</code> <code>str</code> <p>The format of the data at the reading location.</p> required <code>to_table_path</code> <code>str</code> <p>The location where to save the table to.</p> required <code>to_table_name</code> <code>str</code> <p>The name of the table where it will be saved.</p> required <code>to_table_format</code> <code>str</code> <p>The format of the saved table.</p> required <code>from_table_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>to_table_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Note <p>You know that this function is successful if the table exists at the specified location, and there are no errors thrown.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         \"c\": [1, 1, 1 1],\n...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...     }\n... )\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> Terminal<pre><code>[\"table.csv\", \"table.parquet\"]\n</code></pre> </p> <p>Example 1: Transfer CSV<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.csv\",\n...     to_table_format=\"csv\",\n...     from_table_options={\"header\": \"true\"},\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./other\",\n...     data_format=\"csv\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred CSV to CSV.</p> <p>Example 2: Transfer Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.parquet\",\n...     from_table_format=\"parquet\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred Parquet to Parquet.</p> <p>Example 3: Transfer CSV to Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred CSV to Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Copy a table from one location to another.\n\n    ???+ abstract \"Details\"\n        This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.\n\n    Params:\n        spark_session (SparkSession):\n            The spark session to use for the transfer. Necessary in order to instantiate the reading process.\n        from_table_path (str):\n            The path from which the table will be read.\n        from_table_name (str):\n            The name of the table to be read.\n        from_table_format (str):\n            The format of the data at the reading location.\n        to_table_path (str):\n            The location where to save the table to.\n        to_table_name (str):\n            The name of the table where it will be saved.\n        to_table_format (str):\n            The format of the saved table.\n        from_table_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        to_table_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ tip \"Note\"\n        You know that this function is successful if the table exists at the specified location, and there are no errors thrown.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4],\n        ...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         \"c\": [1, 1, 1 1],\n        ...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"table.csv\", \"table.parquet\"]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Transfer CSV\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.csv\",\n        ...     to_table_format=\"csv\",\n        ...     from_table_options={\"header\": \"true\"},\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./other\",\n        ...     data_format=\"csv\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred CSV to CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Transfer Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.parquet\",\n        ...     from_table_format=\"parquet\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred Parquet to Parquet.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Transfer CSV to Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred CSV to Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    from_table_options: str_dict = from_table_options or dict()\n    to_table_options: str_dict = to_table_options or dict()\n    from_table: psDataFrame = read_from_path(\n        name=from_table_name,\n        path=from_table_path,\n        spark_session=spark_session,\n        data_format=from_table_format,\n        read_options=from_table_options,\n    )\n    write_to_path(\n        table=from_table,\n        name=to_table_name,\n        path=to_table_path,\n        data_format=to_table_format,\n        mode=to_table_mode,\n        write_options=to_table_options,\n        partition_cols=to_table_partition_cols,\n    )\n</code></pre>"},{"location":"code/keys/","title":"Keys","text":""},{"location":"code/keys/#toolbox_pyspark.keys","title":"toolbox_pyspark.keys","text":"<p>Summary</p> <p>The <code>keys</code> module is used for creating new columns to act as keys (primary and foreign), to be used for joins with other tables, or to create relationships within downstream applications, like PowerBI.</p>"},{"location":"code/keys/#toolbox_pyspark.keys.add_key_from_columns","title":"add_key_from_columns","text":"<pre><code>add_key_from_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    join_character: Optional[str] = \"_\",\n    key_name: Optional[str] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Using a list of column names, add a new column which is a combination of all of them.</p> Details <p>This is a combine key, and is especially important because PowerBI cannot handle joins on multiple columns.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The table to be updated.</p> required <code>columns</code> <code>Union[str, str_collection]</code> <p>The columns to be combined. If <code>columns</code> is a <code>str</code>, then it will be coerced to a single-element list: <code>[columns]</code>.</p> required <code>join_character</code> <code>Optional[str]</code> <p>The character to use to combine the columns together. Defaults to <code>\"_\"</code>.</p> <code>'_'</code> <code>key_name</code> <code>Optional[str]</code> <p>The name of the column to be given to the key. If not provided, it will form as the capitalised string of all the other column names, prefixed with <code>key_</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated <code>dataframe</code>.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"b\"])\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---------+\n| a | b | c | d | key_A_B |\n+---+---+---+---+---------+\n| 1 | a | 1 | 2 | 1_a     |\n| 2 | b | 1 | 2 | 2_b     |\n| 3 | c | 1 | 2 | 3_c     |\n| 4 | d | 1 | 2 | 4_d     |\n+---+---+---+---+---------+\n</code></pre> <p>Conclusion: Successfully added new key column to DataFrame.</p> <p>Example 2: Single column<pre><code>&gt;&gt;&gt; new_df = add_key_from_columns(df, \"a\")\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+-------+\n| a | b | c | d | key_A |\n+---+---+---+---+-------+\n| 1 | a | 1 | 2 | 1     |\n| 2 | b | 1 | 2 | 2     |\n| 3 | c | 1 | 2 | 3     |\n| 4 | d | 1 | 2 | 4     |\n+---+---+---+---+-------+\n</code></pre> <p>Conclusion: Successfully added new key column to DataFrame.</p> <p>Example 3: New name<pre><code>&gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"b\"], \"new_key\")\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---------+\n| a | b | c | d | new_key |\n+---+---+---+---+---------+\n| 1 | a | 1 | 2 | 1_a     |\n| 2 | b | 1 | 2 | 2_b     |\n| 3 | c | 1 | 2 | 3_c     |\n| 4 | d | 1 | 2 | 4_d     |\n+---+---+---+---+---------+\n</code></pre> <p>Conclusion: Successfully added new key column to DataFrame.</p> <p>Example 4: Raise error<pre><code>&gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"x\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"x\"] do not exist in \"dataframe\". Try one of: [\"a\", \"b\", \"c\", \"d\"].\n</code></pre> <p>Conclusion: Invalid column selection.</p> Source code in <code>src/toolbox_pyspark/keys.py</code> <pre><code>@typechecked\ndef add_key_from_columns(\n    dataframe: psDataFrame,\n    columns: Union[str, str_collection],\n    join_character: Optional[str] = \"_\",\n    key_name: Optional[str] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Using a list of column names, add a new column which is a combination of all of them.\n\n    ???+ abstract \"Details\"\n        This is a combine key, and is especially important because PowerBI cannot handle joins on multiple columns.\n\n    Params:\n        dataframe (psDataFrame):\n            The table to be updated.\n        columns (Union[str, str_collection]):\n            The columns to be combined.&lt;br&gt;\n            If `columns` is a `#!py str`, then it will be coerced to a single-element list: `#!py [columns]`.\n        join_character (Optional[str], optional):\n            The character to use to combine the columns together.&lt;br&gt;\n            Defaults to `#!py \"_\"`.\n        key_name (Optional[str], optional):\n            The name of the column to be given to the key.\n            If not provided, it will form as the capitalised string of all the other column names, prefixed with `key_`.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated `dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"b\"])\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---------+\n        | a | b | c | d | key_A_B |\n        +---+---+---+---+---------+\n        | 1 | a | 1 | 2 | 1_a     |\n        | 2 | b | 1 | 2 | 2_b     |\n        | 3 | c | 1 | 2 | 3_c     |\n        | 4 | d | 1 | 2 | 4_d     |\n        +---+---+---+---+---------+\n        ```\n        !!! success \"Conclusion: Successfully added new key column to DataFrame.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Single column\"}\n        &gt;&gt;&gt; new_df = add_key_from_columns(df, \"a\")\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+-------+\n        | a | b | c | d | key_A |\n        +---+---+---+---+-------+\n        | 1 | a | 1 | 2 | 1     |\n        | 2 | b | 1 | 2 | 2     |\n        | 3 | c | 1 | 2 | 3     |\n        | 4 | d | 1 | 2 | 4     |\n        +---+---+---+---+-------+\n        ```\n        !!! success \"Conclusion: Successfully added new key column to DataFrame.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: New name\"}\n        &gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"b\"], \"new_key\")\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---------+\n        | a | b | c | d | new_key |\n        +---+---+---+---+---------+\n        | 1 | a | 1 | 2 | 1_a     |\n        | 2 | b | 1 | 2 | 2_b     |\n        | 3 | c | 1 | 2 | 3_c     |\n        | 4 | d | 1 | 2 | 4_d     |\n        +---+---+---+---+---------+\n        ```\n        !!! success \"Conclusion: Successfully added new key column to DataFrame.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Raise error\"}\n        &gt;&gt;&gt; new_df = add_key_from_columns(df, [\"a\", \"x\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"x\"] do not exist in \"dataframe\". Try one of: [\"a\", \"b\", \"c\", \"d\"].\n        ```\n        !!! failure \"Conclusion: Invalid column selection.\"\n        &lt;/div&gt;\n    \"\"\"\n    columns = [columns] if is_type(columns, str) else columns\n    assert_columns_exists(dataframe, columns)\n    join_character = join_character or \"\"\n    key_name = key_name or f\"key_{'_'.join([col.upper() for col in columns])}\"\n    return dataframe.withColumn(\n        key_name,\n        F.concat_ws(join_character, *columns),\n    )\n</code></pre>"},{"location":"code/keys/#toolbox_pyspark.keys.add_keys_from_columns","title":"add_keys_from_columns","text":"<pre><code>add_keys_from_columns(\n    dataframe: psDataFrame,\n    collection_of_columns: Union[\n        tuple[Union[str, str_collection], ...],\n        list[Union[str, str_collection]],\n        dict[str, Union[str, str_collection]],\n    ],\n    join_character: Optional[str] = \"_\",\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Add multiple new keys, each of which are collections of other columns.</p> Details <p>There are a few reasons why this functionality would be needed:</p> <ol> <li>When you wanted to create a new single column to act as a combine key, derived from multiple other columns.</li> <li>When you're interacting with PowerBI, it will only allow you to create relationships on one single column, not a combination of multiple columns.</li> <li>When you're joining multiple tables together, each of them join on a different combination of different columns, and you want to make your <code>pyspark</code> joins cleaner, instead of using <code>list</code>'s of multiple <code>F.col(...)</code> equality checks.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The table to be updated.</p> required <code>collection_of_columns</code> <code>Union[tuple[Union[str, str_collection], ...], [Union[str, str_collection]], dict[str, Union[str, str_collection]]]</code> <p>The collection of columns to be combined together. If it is a <code>list</code> of <code>list</code>'s of <code>str</code>'s (or similar), then the key name will be derived from a concatenation of the original columns names. If it's a <code>dict</code> where the values are a <code>list</code> of <code>str</code>'s (or similar), then the column name for the new key is taken from the key of the dictionary.</p> required <code>join_character</code> <code>Optional[str]</code> <p>The character to use to combine the columns together. Defaults to <code>\"_\"</code>.</p> <code>'_'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If any of the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated <code>dataframe</code>.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; new_df = add_keys_from_columns(df, [[\"a\", \"b\"], [\"b\", \"c\"]])\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+---------+---------+\n| a | b | c | d | key_A_B | key_B_C |\n+---+---+---+---+---------+---------+\n| 1 | a | 1 | 2 | 1_a     | a_1     |\n| 2 | b | 1 | 2 | 2_b     | b_1     |\n| 3 | c | 1 | 2 | 3_c     | c_1     |\n| 4 | d | 1 | 2 | 4_d     | d_1     |\n+---+---+---+---+---------+---------+\n</code></pre> <p>Conclusion: Successfully added two new key columns to DataFrame.</p> <p>Example 2: Created from dict<pre><code>&gt;&gt;&gt; new_df = add_keys_from_columns(df, {\"first\": [\"a\", \"b\"], \"second\": [\"b\", \"c\"]])\n&gt;&gt;&gt; new_df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+-------+--------+\n| a | b | c | d | first | second |\n+---+---+---+---+-------+--------+\n| 1 | a | 1 | 2 | 1_a   | a_1    |\n| 2 | b | 1 | 2 | 2_b   | b_1    |\n| 3 | c | 1 | 2 | 3_c   | c_1    |\n| 4 | d | 1 | 2 | 4_d   | d_1    |\n+---+---+---+---+-------+--------+\n</code></pre> <p>Conclusion: Successfully added two new key columns to DataFrame.</p> Source code in <code>src/toolbox_pyspark/keys.py</code> <pre><code>@typechecked\ndef add_keys_from_columns(\n    dataframe: psDataFrame,\n    collection_of_columns: Union[\n        tuple[Union[str, str_collection], ...],\n        list[Union[str, str_collection]],\n        dict[str, Union[str, str_collection]],\n    ],\n    join_character: Optional[str] = \"_\",\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Add multiple new keys, each of which are collections of other columns.\n\n    ???+ abstract \"Details\"\n        There are a few reasons why this functionality would be needed:\n\n        1. When you wanted to create a new single column to act as a combine key, derived from multiple other columns.\n        1. When you're interacting with PowerBI, it will only allow you to create relationships on one single column, not a combination of multiple columns.\n        1. When you're joining multiple tables together, each of them join on a different combination of different columns, and you want to make your `pyspark` joins cleaner, instead of using `#!py list`'s of multiple `#!py F.col(...)` equality checks.\n\n    Params:\n        dataframe (psDataFrame):\n            The table to be updated.\n        collection_of_columns (Union[tuple[Union[str, str_collection], ...], [Union[str, str_collection]], dict[str, Union[str, str_collection]]]):\n            The collection of columns to be combined together.&lt;br&gt;\n            If it is a `#!py list` of `#!py list`'s of `#!py str`'s (or similar), then the key name will be derived from a concatenation of the original columns names.&lt;br&gt;\n            If it's a `#!py dict` where the values are a `#!py list` of `#!py str`'s (or similar), then the column name for the new key is taken from the key of the dictionary.\n        join_character (Optional[str], optional):\n            The character to use to combine the columns together.&lt;br&gt;\n            Defaults to `#!py \"_\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If any of the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (psDataFrame):\n            The updated `dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; new_df = add_keys_from_columns(df, [[\"a\", \"b\"], [\"b\", \"c\"]])\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+---------+---------+\n        | a | b | c | d | key_A_B | key_B_C |\n        +---+---+---+---+---------+---------+\n        | 1 | a | 1 | 2 | 1_a     | a_1     |\n        | 2 | b | 1 | 2 | 2_b     | b_1     |\n        | 3 | c | 1 | 2 | 3_c     | c_1     |\n        | 4 | d | 1 | 2 | 4_d     | d_1     |\n        +---+---+---+---+---------+---------+\n        ```\n        !!! success \"Conclusion: Successfully added two new key columns to DataFrame.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Created from dict\"}\n        &gt;&gt;&gt; new_df = add_keys_from_columns(df, {\"first\": [\"a\", \"b\"], \"second\": [\"b\", \"c\"]])\n        &gt;&gt;&gt; new_df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+-------+--------+\n        | a | b | c | d | first | second |\n        +---+---+---+---+-------+--------+\n        | 1 | a | 1 | 2 | 1_a   | a_1    |\n        | 2 | b | 1 | 2 | 2_b   | b_1    |\n        | 3 | c | 1 | 2 | 3_c   | c_1    |\n        | 4 | d | 1 | 2 | 4_d   | d_1    |\n        +---+---+---+---+-------+--------+\n        ```\n        !!! success \"Conclusion: Successfully added two new key columns to DataFrame.\"\n        &lt;/div&gt;\n    \"\"\"\n    join_character = join_character or \"\"\n    if is_type(collection_of_columns, dict):\n        for key_name, columns in collection_of_columns.items():\n            dataframe = add_key_from_columns(\n                dataframe, columns, join_character, key_name\n            )\n    elif is_type(collection_of_columns, (tuple, list)):\n        for columns in collection_of_columns:\n            dataframe = add_key_from_columns(dataframe, columns, join_character)\n    return dataframe\n</code></pre>"},{"location":"code/scale/","title":"Scale","text":""},{"location":"code/scale/#toolbox_pyspark.scale","title":"toolbox_pyspark.scale","text":"<p>Summary</p> <p>The <code>scale</code> module is used for rounding a column (or columns) to a given rounding accuracy.</p>"},{"location":"code/scale/#toolbox_pyspark.scale.round_column","title":"round_column","text":"<pre><code>round_column(\n    dataframe: psDataFrame,\n    column: str,\n    scale: int = DEFAULT_DECIMAL_ACCURACY,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given <code>dataframe</code>, on a given <code>column</code> if the column data type is decimal (that is, one of: <code>[\"float\", \"double\", \"decimal\"]</code>), then round that column to a <code>scale</code> accuracy at a given number of decimal places.</p> Details <p>Realistically, under the hood, this function is super simple. It merely runs: Python<pre><code>dataframe = dataframe.withColumn(colName=column, col=F.round(col=column, scale=scale))\n</code></pre> This function merely adds some additional validation, and is enabled to run in a pyspark <code>.transform()</code> method. For more info, see: <code>pyspark.sql.DataFrame.transform</code></p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The <code>dataframe</code> to be transformed.</p> required <code>column</code> <code>str</code> <p>The desired column to be rounded.</p> required <code>scale</code> <code>int</code> <p>The required level of rounding for the column. If not provided explicitly, it will default to the global value <code>DEFAULT_DECIMAL_ACCURACY</code>; which is <code>10</code>. Defaults to <code>DEFAULT_DECIMAL_ACCURACY</code>.</p> <code>DEFAULT_DECIMAL_ACCURACY</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>TypeError</code> <p>If the given <code>column</code> is not one of the correct data types for rounding. It must be one of: <code>[\"float\", \"double\", \"decimal\"]</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed <code>dataframe</code> containing the column which has now been rounded.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F, types as T\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = (\n...     spark\n...     .createDataFrame(\n...         pd.DataFrame(\n...             {\n...                 \"a\": range(20),\n...                 \"b\": [f\"1.{'0'*val}1\" for val in range(20)],\n...                 \"c\": [f\"1.{'0'*val}6\" for val in range(20)],\n...             }\n...         )\n...     )\n...     .withColumns(\n...         {\n...             \"b\": F.col(\"b\").cast(T.DecimalType(21, 20)),\n...             \"c\": F.col(\"c\").cast(T.DecimalType(21, 20)),\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show(truncate=False)\n</code></pre> Terminal<pre><code>+---+----------------------+----------------------+\n|a  |b                     |c                     |\n+---+----------------------+----------------------+\n|0  |1.10000000000000000000|1.60000000000000000000|\n|1  |1.01000000000000000000|1.06000000000000000000|\n|2  |1.00100000000000000000|1.00600000000000000000|\n|3  |1.00010000000000000000|1.00060000000000000000|\n|4  |1.00001000000000000000|1.00006000000000000000|\n|5  |1.00000100000000000000|1.00000600000000000000|\n|6  |1.00000010000000000000|1.00000060000000000000|\n|7  |1.00000001000000000000|1.00000006000000000000|\n|8  |1.00000000100000000000|1.00000000600000000000|\n|9  |1.00000000010000000000|1.00000000060000000000|\n|10 |1.00000000001000000000|1.00000000006000000000|\n|11 |1.00000000000100000000|1.00000000000600000000|\n|12 |1.00000000000010000000|1.00000000000060000000|\n|13 |1.00000000000001000000|1.00000000000006000000|\n|14 |1.00000000000000100000|1.00000000000000600000|\n|15 |1.00000000000000010000|1.00000000000000060000|\n|16 |1.00000000000000001000|1.00000000000000006000|\n|17 |1.00000000000000000100|1.00000000000000000600|\n|18 |1.00000000000000000010|1.00000000000000000060|\n|19 |1.00000000000000000001|1.00000000000000000006|\n+---+----------------------+----------------------+\n</code></pre> </p> <p>Example 1: Round with defaults<pre><code>&gt;&gt;&gt; round_column(df, \"b\").show(truncate=False)\n</code></pre> Terminal<pre><code>+---+------------+----------------------+\n|a  |b           |c                     |\n+---+------------+----------------------+\n|0  |1.1000000000|1.60000000000000000000|\n|1  |1.0100000000|1.06000000000000000000|\n|2  |1.0010000000|1.00600000000000000000|\n|3  |1.0001000000|1.00060000000000000000|\n|4  |1.0000100000|1.00006000000000000000|\n|5  |1.0000010000|1.00000600000000000000|\n|6  |1.0000001000|1.00000060000000000000|\n|7  |1.0000000100|1.00000006000000000000|\n|8  |1.0000000010|1.00000000600000000000|\n|9  |1.0000000001|1.00000000060000000000|\n|10 |1.0000000000|1.00000000006000000000|\n|11 |1.0000000000|1.00000000000600000000|\n|12 |1.0000000000|1.00000000000060000000|\n|13 |1.0000000000|1.00000000000006000000|\n|14 |1.0000000000|1.00000000000000600000|\n|15 |1.0000000000|1.00000000000000060000|\n|16 |1.0000000000|1.00000000000000006000|\n|17 |1.0000000000|1.00000000000000000600|\n|18 |1.0000000000|1.00000000000000000060|\n|19 |1.0000000000|1.00000000000000000006|\n+---+------------+----------------------+\n</code></pre> <p>Conclusion: Successfully rounded column <code>b</code>.</p> <p>Example 2: Round to custom number<pre><code>&gt;&gt;&gt; round_column(df, \"c\", 5).show(truncate=False)\n</code></pre> Terminal<pre><code>+---+----------------------+-------+\n|a  |b                     |c      |\n+---+----------------------+-------+\n|0  |1.10000000000000000000|1.60000|\n|1  |1.01000000000000000000|1.06000|\n|2  |1.00100000000000000000|1.00600|\n|3  |1.00010000000000000000|1.00060|\n|4  |1.00001000000000000000|1.00006|\n|5  |1.00000100000000000000|1.00001|\n|6  |1.00000010000000000000|1.00000|\n|7  |1.00000001000000000000|1.00000|\n|8  |1.00000000100000000000|1.00000|\n|9  |1.00000000010000000000|1.00000|\n|10 |1.00000000001000000000|1.00000|\n|11 |1.00000000000100000000|1.00000|\n|12 |1.00000000000010000000|1.00000|\n|13 |1.00000000000001000000|1.00000|\n|14 |1.00000000000000100000|1.00000|\n|15 |1.00000000000000010000|1.00000|\n|16 |1.00000000000000001000|1.00000|\n|17 |1.00000000000000000100|1.00000|\n|18 |1.00000000000000000010|1.00000|\n|19 |1.00000000000000000001|1.00000|\n+---+----------------------+-------+\n</code></pre> <p>Conclusion: Successfully rounded column <code>b</code> to 5 decimal points.</p> <p>Example 3: Raise error<pre><code>&gt;&gt;&gt; round_column(df, \"a\").show(truncate=False)\n</code></pre> Terminal<pre><code>TypeError: Column is not the correct type. Please check.\nFor column 'a', the type is 'bigint'.\nIn order to round it, it needs to be one of: '[\"float\", \"double\", \"decimal\"]'.\n</code></pre> <p>Conclusion: Cannot round a column <code>a</code>.</p> Source code in <code>src/toolbox_pyspark/scale.py</code> <pre><code>@typechecked\ndef round_column(\n    dataframe: psDataFrame,\n    column: str,\n    scale: int = DEFAULT_DECIMAL_ACCURACY,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `dataframe`, on a given `column` if the column data type is decimal (that is, one of: `#!py [\"float\", \"double\", \"decimal\"]`), then round that column to a `scale` accuracy at a given number of decimal places.\n\n    ???+ abstract \"Details\"\n        Realistically, under the hood, this function is super simple. It merely runs:\n        ```{.py .python linenums=\"1\" title=\"Python\"}\n        dataframe = dataframe.withColumn(colName=column, col=F.round(col=column, scale=scale))\n        ```\n        This function merely adds some additional validation, and is enabled to run in a pyspark `.transform()` method.\n        For more info, see: [`pyspark.sql.DataFrame.transform`](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.transform.html)\n\n    Params:\n        dataframe (psDataFrame):\n            The `dataframe` to be transformed.\n        column (str):\n            The desired column to be rounded.\n        scale (int, optional):\n            The required level of rounding for the column.&lt;br&gt;\n            If not provided explicitly, it will default to the global value `#!py DEFAULT_DECIMAL_ACCURACY`; which is `#!py 10`.&lt;br&gt;\n            Defaults to `#!py DEFAULT_DECIMAL_ACCURACY`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        TypeError:\n            If the given `column` is not one of the correct data types for rounding. It must be one of: `#!py [\"float\", \"double\", \"decimal\"]`.\n\n    Returns:\n        (psDataFrame):\n            The transformed `dataframe` containing the column which has now been rounded.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F, types as T\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = (\n        ...     spark\n        ...     .createDataFrame(\n        ...         pd.DataFrame(\n        ...             {\n        ...                 \"a\": range(20),\n        ...                 \"b\": [f\"1.{'0'*val}1\" for val in range(20)],\n        ...                 \"c\": [f\"1.{'0'*val}6\" for val in range(20)],\n        ...             }\n        ...         )\n        ...     )\n        ...     .withColumns(\n        ...         {\n        ...             \"b\": F.col(\"b\").cast(T.DecimalType(21, 20)),\n        ...             \"c\": F.col(\"c\").cast(T.DecimalType(21, 20)),\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----------------------+----------------------+\n        |a  |b                     |c                     |\n        +---+----------------------+----------------------+\n        |0  |1.10000000000000000000|1.60000000000000000000|\n        |1  |1.01000000000000000000|1.06000000000000000000|\n        |2  |1.00100000000000000000|1.00600000000000000000|\n        |3  |1.00010000000000000000|1.00060000000000000000|\n        |4  |1.00001000000000000000|1.00006000000000000000|\n        |5  |1.00000100000000000000|1.00000600000000000000|\n        |6  |1.00000010000000000000|1.00000060000000000000|\n        |7  |1.00000001000000000000|1.00000006000000000000|\n        |8  |1.00000000100000000000|1.00000000600000000000|\n        |9  |1.00000000010000000000|1.00000000060000000000|\n        |10 |1.00000000001000000000|1.00000000006000000000|\n        |11 |1.00000000000100000000|1.00000000000600000000|\n        |12 |1.00000000000010000000|1.00000000000060000000|\n        |13 |1.00000000000001000000|1.00000000000006000000|\n        |14 |1.00000000000000100000|1.00000000000000600000|\n        |15 |1.00000000000000010000|1.00000000000000060000|\n        |16 |1.00000000000000001000|1.00000000000000006000|\n        |17 |1.00000000000000000100|1.00000000000000000600|\n        |18 |1.00000000000000000010|1.00000000000000000060|\n        |19 |1.00000000000000000001|1.00000000000000000006|\n        +---+----------------------+----------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Round with defaults\"}\n        &gt;&gt;&gt; round_column(df, \"b\").show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+------------+----------------------+\n        |a  |b           |c                     |\n        +---+------------+----------------------+\n        |0  |1.1000000000|1.60000000000000000000|\n        |1  |1.0100000000|1.06000000000000000000|\n        |2  |1.0010000000|1.00600000000000000000|\n        |3  |1.0001000000|1.00060000000000000000|\n        |4  |1.0000100000|1.00006000000000000000|\n        |5  |1.0000010000|1.00000600000000000000|\n        |6  |1.0000001000|1.00000060000000000000|\n        |7  |1.0000000100|1.00000006000000000000|\n        |8  |1.0000000010|1.00000000600000000000|\n        |9  |1.0000000001|1.00000000060000000000|\n        |10 |1.0000000000|1.00000000006000000000|\n        |11 |1.0000000000|1.00000000000600000000|\n        |12 |1.0000000000|1.00000000000060000000|\n        |13 |1.0000000000|1.00000000000006000000|\n        |14 |1.0000000000|1.00000000000000600000|\n        |15 |1.0000000000|1.00000000000000060000|\n        |16 |1.0000000000|1.00000000000000006000|\n        |17 |1.0000000000|1.00000000000000000600|\n        |18 |1.0000000000|1.00000000000000000060|\n        |19 |1.0000000000|1.00000000000000000006|\n        +---+------------+----------------------+\n        ```\n        !!! success \"Conclusion: Successfully rounded column `b`.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Round to custom number\"}\n        &gt;&gt;&gt; round_column(df, \"c\", 5).show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----------------------+-------+\n        |a  |b                     |c      |\n        +---+----------------------+-------+\n        |0  |1.10000000000000000000|1.60000|\n        |1  |1.01000000000000000000|1.06000|\n        |2  |1.00100000000000000000|1.00600|\n        |3  |1.00010000000000000000|1.00060|\n        |4  |1.00001000000000000000|1.00006|\n        |5  |1.00000100000000000000|1.00001|\n        |6  |1.00000010000000000000|1.00000|\n        |7  |1.00000001000000000000|1.00000|\n        |8  |1.00000000100000000000|1.00000|\n        |9  |1.00000000010000000000|1.00000|\n        |10 |1.00000000001000000000|1.00000|\n        |11 |1.00000000000100000000|1.00000|\n        |12 |1.00000000000010000000|1.00000|\n        |13 |1.00000000000001000000|1.00000|\n        |14 |1.00000000000000100000|1.00000|\n        |15 |1.00000000000000010000|1.00000|\n        |16 |1.00000000000000001000|1.00000|\n        |17 |1.00000000000000000100|1.00000|\n        |18 |1.00000000000000000010|1.00000|\n        |19 |1.00000000000000000001|1.00000|\n        +---+----------------------+-------+\n        ```\n        !!! success \"Conclusion: Successfully rounded column `b` to 5 decimal points.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Raise error\"}\n        &gt;&gt;&gt; round_column(df, \"a\").show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        TypeError: Column is not the correct type. Please check.\n        For column 'a', the type is 'bigint'.\n        In order to round it, it needs to be one of: '[\"float\", \"double\", \"decimal\"]'.\n        ```\n        !!! failure \"Conclusion: Cannot round a column `a`.\"\n        &lt;/div&gt;\n    \"\"\"\n    assert_column_exists(dataframe, column)\n    col_type: str = [\n        typ.split(\"(\")[0] for col, typ in dataframe.dtypes if col == column\n    ][0]\n    if col_type not in VALID_TYPES:\n        raise TypeError(\n            f\"Column is not the correct type. Please check.\\n\"\n            f\"For column '{column}', the type is '{col_type}'.\\n\"\n            f\"In order to round it, it needs to be one of: '{VALID_TYPES}'.\"\n        )\n    return dataframe.withColumn(colName=column, col=F.round(col=column, scale=scale))\n</code></pre>"},{"location":"code/scale/#toolbox_pyspark.scale.round_columns","title":"round_columns","text":"<pre><code>round_columns(\n    dataframe: psDataFrame,\n    columns: Optional[\n        Union[str, str_collection]\n    ] = \"all_float\",\n    scale: int = DEFAULT_DECIMAL_ACCURACY,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>For a given <code>dataframe</code>, on a set of <code>columns</code> if the column data type is decimal (that is, one of: <code>[\"float\", \"double\", \"decimal\"]</code>), then round that column to a <code>scale</code> accuracy at a given number of decimal places.</p> Details <p>Realistically, under the hood, this function is super simple. It merely runs: Python<pre><code>dataframe = dataframe.withColumns({col: F.round(col, scale) for col in columns})\n</code></pre> This function merely adds some additional validation, and is enabled to run in a pyspark <code>.transform()</code> method. For more info, see: <code>pyspark.sql.DataFrame.transform</code></p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The <code>dataframe</code> to be transformed.</p> required <code>columns</code> <code>Optional[Union[str, str_collection]]</code> <p>The desired column to be rounded. If no value is parsed, or is the value <code>None</code>, or one of <code>[\"all\", \"all_float\"]</code>, then it will default to all numeric decimal columns on the <code>dataframe</code>. If the value is a <code>str</code>, then it will be coerced to a single-element list, like: <code>[columns]</code>. Defaults to <code>\"all_float\"</code>.</p> <code>'all_float'</code> <code>scale</code> <code>int</code> <p>The required level of rounding for the column. If not provided explicitly, it will default to the global value <code>DEFAULT_DECIMAL_ACCURACY</code>; which is <code>10</code>. Defaults to <code>DEFAULT_DECIMAL_ACCURACY</code>.</p> <code>DEFAULT_DECIMAL_ACCURACY</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>TypeError</code> <p>If any of the given <code>columns</code> are not one of the correct data types for rounding. They must be one of: <code>[\"float\", \"double\", \"decimal\"]</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed <code>dataframe</code> containing the column which has now been rounded.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F, types as T\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = (\n...     spark\n...     .createDataFrame(\n...         pd.DataFrame(\n...             {\n...                 \"a\": range(20),\n...                 \"b\": [f\"1.{'0'*val}1\" for val in range(20)],\n...                 \"c\": [f\"1.{'0'*val}6\" for val in range(20)],\n...             }\n...         )\n...     )\n...     .withColumns(\n...         {\n...             \"b\": F.col(\"b\").cast(T.DecimalType(21, 20)),\n...             \"c\": F.col(\"c\").cast(T.DecimalType(21, 20)),\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; df.show(truncate=False)\n</code></pre> Terminal<pre><code>+---+----------------------+----------------------+\n|a  |b                     |c                     |\n+---+----------------------+----------------------+\n|0  |1.10000000000000000000|1.60000000000000000000|\n|1  |1.01000000000000000000|1.06000000000000000000|\n|2  |1.00100000000000000000|1.00600000000000000000|\n|3  |1.00010000000000000000|1.00060000000000000000|\n|4  |1.00001000000000000000|1.00006000000000000000|\n|5  |1.00000100000000000000|1.00000600000000000000|\n|6  |1.00000010000000000000|1.00000060000000000000|\n|7  |1.00000001000000000000|1.00000006000000000000|\n|8  |1.00000000100000000000|1.00000000600000000000|\n|9  |1.00000000010000000000|1.00000000060000000000|\n|10 |1.00000000001000000000|1.00000000006000000000|\n|11 |1.00000000000100000000|1.00000000000600000000|\n|12 |1.00000000000010000000|1.00000000000060000000|\n|13 |1.00000000000001000000|1.00000000000006000000|\n|14 |1.00000000000000100000|1.00000000000000600000|\n|15 |1.00000000000000010000|1.00000000000000060000|\n|16 |1.00000000000000001000|1.00000000000000006000|\n|17 |1.00000000000000000100|1.00000000000000000600|\n|18 |1.00000000000000000010|1.00000000000000000060|\n|19 |1.00000000000000000001|1.00000000000000000006|\n+---+----------------------+----------------------+\n</code></pre> </p> <p>Example 1: Round with defaults<pre><code>&gt;&gt;&gt; round_columns(df).show(truncate=False)\n</code></pre> Terminal<pre><code>+---+------------+------------+\n|  a|           b|           c|\n+---+------------+------------+\n|  0|1.1000000000|1.6000000000|\n|  1|1.0100000000|1.0600000000|\n|  2|1.0010000000|1.0060000000|\n|  3|1.0001000000|1.0006000000|\n|  4|1.0000100000|1.0000600000|\n|  5|1.0000010000|1.0000060000|\n|  6|1.0000001000|1.0000006000|\n|  7|1.0000000100|1.0000000600|\n|  8|1.0000000010|1.0000000060|\n|  9|1.0000000001|1.0000000006|\n| 10|1.0000000000|1.0000000001|\n| 11|1.0000000000|1.0000000000|\n| 12|1.0000000000|1.0000000000|\n| 13|1.0000000000|1.0000000000|\n| 14|1.0000000000|1.0000000000|\n| 15|1.0000000000|1.0000000000|\n| 16|1.0000000000|1.0000000000|\n| 17|1.0000000000|1.0000000000|\n| 18|1.0000000000|1.0000000000|\n| 19|1.0000000000|1.0000000000|\n+---+------------+------------+\n</code></pre> </p> <p>Example 2: Round to custom number<pre><code>&gt;&gt;&gt; round_columns(df, \"c\", 5).show(truncate=False)\n</code></pre> Terminal<pre><code>+---+----------------------+-------+\n|a  |b                     |c      |\n+---+----------------------+-------+\n|0  |1.10000000000000000000|1.60000|\n|1  |1.01000000000000000000|1.06000|\n|2  |1.00100000000000000000|1.00600|\n|3  |1.00010000000000000000|1.00060|\n|4  |1.00001000000000000000|1.00006|\n|5  |1.00000100000000000000|1.00001|\n|6  |1.00000010000000000000|1.00000|\n|7  |1.00000001000000000000|1.00000|\n|8  |1.00000000100000000000|1.00000|\n|9  |1.00000000010000000000|1.00000|\n|10 |1.00000000001000000000|1.00000|\n|11 |1.00000000000100000000|1.00000|\n|12 |1.00000000000010000000|1.00000|\n|13 |1.00000000000001000000|1.00000|\n|14 |1.00000000000000100000|1.00000|\n|15 |1.00000000000000010000|1.00000|\n|16 |1.00000000000000001000|1.00000|\n|17 |1.00000000000000000100|1.00000|\n|18 |1.00000000000000000010|1.00000|\n|19 |1.00000000000000000001|1.00000|\n+---+----------------------+-------+\n</code></pre> </p> <p>Example 3: Raise error<pre><code>&gt;&gt;&gt; round_columns(df, [\"a\", \"b\"]).show(truncate=False)\n</code></pre> Terminal<pre><code>TypeError: Columns are not the correct types. Please check.\nThese columns are invalid: '[(\"a\", \"bigint\")]'.\nIn order to round them, they need to be one of: '[\"float\", \"double\", \"decimal\"]'.\n</code></pre> </p> Source code in <code>src/toolbox_pyspark/scale.py</code> <pre><code>@typechecked\ndef round_columns(\n    dataframe: psDataFrame,\n    columns: Optional[Union[str, str_collection]] = \"all_float\",\n    scale: int = DEFAULT_DECIMAL_ACCURACY,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `dataframe`, on a set of `columns` if the column data type is decimal (that is, one of: `#!py [\"float\", \"double\", \"decimal\"]`), then round that column to a `scale` accuracy at a given number of decimal places.\n\n    ???+ abstract \"Details\"\n        Realistically, under the hood, this function is super simple. It merely runs:\n        ```{.py .python linenums=\"1\" title=\"Python\"}\n        dataframe = dataframe.withColumns({col: F.round(col, scale) for col in columns})\n        ```\n        This function merely adds some additional validation, and is enabled to run in a pyspark `.transform()` method.\n        For more info, see: [`pyspark.sql.DataFrame.transform`](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.transform.html)\n\n    Params:\n        dataframe (psDataFrame):\n            The `dataframe` to be transformed.\n        columns (Optional[Union[str, str_collection]], optional):\n            The desired column to be rounded.&lt;br&gt;\n            If no value is parsed, or is the value `#!py None`, or one of `#!py [\"all\", \"all_float\"]`, then it will default to all numeric decimal columns on the `dataframe`.&lt;br&gt;\n            If the value is a `#!py str`, then it will be coerced to a single-element list, like: `#!py [columns]`.&lt;br&gt;\n            Defaults to `#!py \"all_float\"`.\n        scale (int, optional):\n            The required level of rounding for the column.&lt;br&gt;\n            If not provided explicitly, it will default to the global value `#!py DEFAULT_DECIMAL_ACCURACY`; which is `#!py 10`.&lt;br&gt;\n            Defaults to `#!py DEFAULT_DECIMAL_ACCURACY`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        TypeError:\n            If any of the given `columns` are not one of the correct data types for rounding. They must be one of: `#!py [\"float\", \"double\", \"decimal\"]`.\n\n    Returns:\n        (psDataFrame):\n            The transformed `dataframe` containing the column which has now been rounded.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F, types as T\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = (\n        ...     spark\n        ...     .createDataFrame(\n        ...         pd.DataFrame(\n        ...             {\n        ...                 \"a\": range(20),\n        ...                 \"b\": [f\"1.{'0'*val}1\" for val in range(20)],\n        ...                 \"c\": [f\"1.{'0'*val}6\" for val in range(20)],\n        ...             }\n        ...         )\n        ...     )\n        ...     .withColumns(\n        ...         {\n        ...             \"b\": F.col(\"b\").cast(T.DecimalType(21, 20)),\n        ...             \"c\": F.col(\"c\").cast(T.DecimalType(21, 20)),\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; df.show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----------------------+----------------------+\n        |a  |b                     |c                     |\n        +---+----------------------+----------------------+\n        |0  |1.10000000000000000000|1.60000000000000000000|\n        |1  |1.01000000000000000000|1.06000000000000000000|\n        |2  |1.00100000000000000000|1.00600000000000000000|\n        |3  |1.00010000000000000000|1.00060000000000000000|\n        |4  |1.00001000000000000000|1.00006000000000000000|\n        |5  |1.00000100000000000000|1.00000600000000000000|\n        |6  |1.00000010000000000000|1.00000060000000000000|\n        |7  |1.00000001000000000000|1.00000006000000000000|\n        |8  |1.00000000100000000000|1.00000000600000000000|\n        |9  |1.00000000010000000000|1.00000000060000000000|\n        |10 |1.00000000001000000000|1.00000000006000000000|\n        |11 |1.00000000000100000000|1.00000000000600000000|\n        |12 |1.00000000000010000000|1.00000000000060000000|\n        |13 |1.00000000000001000000|1.00000000000006000000|\n        |14 |1.00000000000000100000|1.00000000000000600000|\n        |15 |1.00000000000000010000|1.00000000000000060000|\n        |16 |1.00000000000000001000|1.00000000000000006000|\n        |17 |1.00000000000000000100|1.00000000000000000600|\n        |18 |1.00000000000000000010|1.00000000000000000060|\n        |19 |1.00000000000000000001|1.00000000000000000006|\n        +---+----------------------+----------------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Round with defaults\"}\n        &gt;&gt;&gt; round_columns(df).show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+------------+------------+\n        |  a|           b|           c|\n        +---+------------+------------+\n        |  0|1.1000000000|1.6000000000|\n        |  1|1.0100000000|1.0600000000|\n        |  2|1.0010000000|1.0060000000|\n        |  3|1.0001000000|1.0006000000|\n        |  4|1.0000100000|1.0000600000|\n        |  5|1.0000010000|1.0000060000|\n        |  6|1.0000001000|1.0000006000|\n        |  7|1.0000000100|1.0000000600|\n        |  8|1.0000000010|1.0000000060|\n        |  9|1.0000000001|1.0000000006|\n        | 10|1.0000000000|1.0000000001|\n        | 11|1.0000000000|1.0000000000|\n        | 12|1.0000000000|1.0000000000|\n        | 13|1.0000000000|1.0000000000|\n        | 14|1.0000000000|1.0000000000|\n        | 15|1.0000000000|1.0000000000|\n        | 16|1.0000000000|1.0000000000|\n        | 17|1.0000000000|1.0000000000|\n        | 18|1.0000000000|1.0000000000|\n        | 19|1.0000000000|1.0000000000|\n        +---+------------+------------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Round to custom number\"}\n        &gt;&gt;&gt; round_columns(df, \"c\", 5).show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+----------------------+-------+\n        |a  |b                     |c      |\n        +---+----------------------+-------+\n        |0  |1.10000000000000000000|1.60000|\n        |1  |1.01000000000000000000|1.06000|\n        |2  |1.00100000000000000000|1.00600|\n        |3  |1.00010000000000000000|1.00060|\n        |4  |1.00001000000000000000|1.00006|\n        |5  |1.00000100000000000000|1.00001|\n        |6  |1.00000010000000000000|1.00000|\n        |7  |1.00000001000000000000|1.00000|\n        |8  |1.00000000100000000000|1.00000|\n        |9  |1.00000000010000000000|1.00000|\n        |10 |1.00000000001000000000|1.00000|\n        |11 |1.00000000000100000000|1.00000|\n        |12 |1.00000000000010000000|1.00000|\n        |13 |1.00000000000001000000|1.00000|\n        |14 |1.00000000000000100000|1.00000|\n        |15 |1.00000000000000010000|1.00000|\n        |16 |1.00000000000000001000|1.00000|\n        |17 |1.00000000000000000100|1.00000|\n        |18 |1.00000000000000000010|1.00000|\n        |19 |1.00000000000000000001|1.00000|\n        +---+----------------------+-------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Raise error\"}\n        &gt;&gt;&gt; round_columns(df, [\"a\", \"b\"]).show(truncate=False)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        TypeError: Columns are not the correct types. Please check.\n        These columns are invalid: '[(\"a\", \"bigint\")]'.\n        In order to round them, they need to be one of: '[\"float\", \"double\", \"decimal\"]'.\n        ```\n        &lt;/div&gt;\n    \"\"\"\n    if columns is None or columns in [\"all\", \"all_float\"]:\n        columns = [\n            col for col, typ in dataframe.dtypes if typ.split(\"(\")[0] in VALID_TYPES\n        ]\n    elif is_type(columns, str):\n        columns = [columns]\n    assert_columns_exists(dataframe, columns)\n    invalid_cols: list[tuple[str, str]] = [\n        (col, typ.split(\"(\")[0])\n        for col, typ in dataframe.dtypes\n        if col in columns and typ.split(\"(\")[0] not in VALID_TYPES\n    ]\n    if len(invalid_cols) &gt; 0:\n        raise TypeError(\n            f\"Columns are not the correct types. Please check.\\n\"\n            f\"These columns are invalid: '{invalid_cols}'.\\n\"\n            f\"In order to round them, they need to be one of: '{VALID_TYPES}'.\"\n        )\n    return dataframe.withColumns({col: F.round(col, scale) for col in columns})\n</code></pre>"},{"location":"code/types/","title":"Types","text":""},{"location":"code/types/#toolbox_pyspark.types","title":"toolbox_pyspark.types","text":"<p>Summary</p> <p>The <code>types</code> module is used to get, check, and change a datafames column data types.</p>"},{"location":"code/types/#toolbox_pyspark.types.get_column_types","title":"get_column_types","text":"<pre><code>get_column_types(\n    dataframe: psDataFrame, output_type: str = \"psDataFrame\"\n) -&gt; Union[psDataFrame, pdDataFrame]\n</code></pre> <p>Summary</p> <p>This is a convenient function to return the data types from a given table as either a <code>pyspark.sql.DataFrame</code> or <code>pandas.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be checked.</p> required <code>output_type</code> <code>str</code> <p>How should the data be returned? As <code>pdDataFrame</code> or <code>psDataFrame</code>.</p> <p>For <code>pandas</code>, use one of:</p> <pre><code>[\n    \"pandas\", \"pandas.DataFrame\",\n    \"pd.df\",  \"pd.DataFrame\",\n    \"pddf\",   \"pdDataFrame\",\n    \"pd\",     \"pdDF\",\n]\n</code></pre> <p>For <code>pyspark</code> use one of:</p> <pre><code>[\n    \"pyspark\", \"spark.DataFrame\",\n    \"spark\",   \"pyspark.DataFrame\",\n    \"ps.df\",   \"ps.DataFrame\",\n    \"psdf\",    \"psDataFrame\",\n    \"ps\",      \"psDF\",\n]\n</code></pre> <p>Any other options are invalid. Defaults to <code>\"psDataFrame\"</code>.</p> <code>'psDataFrame'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>InvalidPySparkDataTypeError</code> <p>If the given value parsed to <code>output_type</code> is not one of the given valid types.</p> <p>Returns:</p> Type Description <code>Union[DataFrame, DataFrame]</code> <p>The DataFrame where each row represents a column on the original <code>dataframe</code> object, and which has two columns:</p> <ol> <li>The column name from <code>dataframe</code>; and</li> <li>The data type for that column in <code>dataframe</code>.</li> </ol> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; print(df.dtypes)\n</code></pre> Terminal<pre><code>[\n    (\"a\", \"bigint\"),\n    (\"b\", \"string\"),\n    (\"c\", \"bigint\"),\n    (\"d\", \"string\"),\n]\n</code></pre> </p> <p>Example 1: Return PySpark<pre><code>&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully print PySpark output.</p> <p>Example 2: Return Pandas<pre><code>&gt;&gt;&gt; print(get_column_types(df, \"pd\"))\n</code></pre> Terminal<pre><code>   col_name  col_type\n0         a    bigint\n1         b    string\n2         c    bigint\n3         d    string\n</code></pre> <p>Conclusion: Successfully print Pandas output.</p> <p>Example 3: Invalid output<pre><code>&gt;&gt;&gt; print(get_column_types(df, \"foo\"))\n</code></pre> Terminal<pre><code>InvalidDataFrameNameError: Invalid value for `output_type`: \"foo\".\nMust be one of: [\"pandas.DataFrame\", \"pandas\", \"pd.DataFrame\", \"pd.df\", \"pddf\", \"pdDataFrame\", \"pdDF\", \"pd\", \"spark.DataFrame\", \"pyspark.DataFrame\", \"pyspark\", \"spark\", \"ps.DataFrame\", \"ps.df\", \"psdf\", \"psDataFrame\", \"psDF\", \"ps\"]\n</code></pre> <p>Conclusion: Invalid input.</p> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef get_column_types(\n    dataframe: psDataFrame,\n    output_type: str = \"psDataFrame\",\n) -&gt; Union[psDataFrame, pdDataFrame]:\n    \"\"\"\n    !!! note \"Summary\"\n        This is a convenient function to return the data types from a given table as either a `#!py pyspark.sql.DataFrame` or `#!py pandas.DataFrame`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be checked.\n\n        output_type (str, optional):\n            How should the data be returned? As `#!py pdDataFrame` or `#!py psDataFrame`.\n\n            For `#!py pandas`, use one of:\n\n            ```{.py .python}\n            [\n                \"pandas\", \"pandas.DataFrame\",\n                \"pd.df\",  \"pd.DataFrame\",\n                \"pddf\",   \"pdDataFrame\",\n                \"pd\",     \"pdDF\",\n            ]\n            ```\n\n            &lt;/div&gt;\n\n            For `#!py pyspark` use one of:\n\n            ```{.py .python}\n            [\n                \"pyspark\", \"spark.DataFrame\",\n                \"spark\",   \"pyspark.DataFrame\",\n                \"ps.df\",   \"ps.DataFrame\",\n                \"psdf\",    \"psDataFrame\",\n                \"ps\",      \"psDF\",\n            ]\n            ```\n\n            Any other options are invalid.&lt;br&gt;\n            Defaults to `#!py \"psDataFrame\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        InvalidPySparkDataTypeError:\n            If the given value parsed to `#!py output_type` is not one of the given valid types.\n\n    Returns:\n        (Union[psDataFrame, pdDataFrame]):\n            The DataFrame where each row represents a column on the original `#!py dataframe` object, and which has two columns:\n\n            1. The column name from `#!py dataframe`; and\n            2. The data type for that column in `#!py dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; print(df.dtypes)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        [\n            (\"a\", \"bigint\"),\n            (\"b\", \"string\"),\n            (\"c\", \"bigint\"),\n            (\"d\", \"string\"),\n        ]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Return PySpark\"}\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully print PySpark output.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Return Pandas\"}\n        &gt;&gt;&gt; print(get_column_types(df, \"pd\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n           col_name  col_type\n        0         a    bigint\n        1         b    string\n        2         c    bigint\n        3         d    string\n        ```\n        !!! success \"Conclusion: Successfully print Pandas output.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid output\"}\n        &gt;&gt;&gt; print(get_column_types(df, \"foo\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        InvalidDataFrameNameError: Invalid value for `output_type`: \"foo\".\n        Must be one of: [\"pandas.DataFrame\", \"pandas\", \"pd.DataFrame\", \"pd.df\", \"pddf\", \"pdDataFrame\", \"pdDF\", \"pd\", \"spark.DataFrame\", \"pyspark.DataFrame\", \"pyspark\", \"spark\", \"ps.DataFrame\", \"ps.df\", \"psdf\", \"psDataFrame\", \"psDF\", \"ps\"]\n        ```\n        !!! failure \"Conclusion: Invalid input.\"\n        &lt;/div&gt;\n    \"\"\"\n    if output_type not in VALID_DATAFRAME_NAMES:\n        raise InvalidDataFrameNameError(\n            f\"Invalid value for `output_type`: '{output_type}'.\\n\"\n            f\"Must be one of: {VALID_DATAFRAME_NAMES}\"\n        )\n    output = pd.DataFrame(dataframe.dtypes, columns=[\"col_name\", \"col_type\"])\n    if output_type in VALID_PYSPARK_DATAFRAME_NAMES:\n        return dataframe.sparkSession.createDataFrame(output)\n    else:\n        return output\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.cast_column_to_type","title":"cast_column_to_type","text":"<pre><code>cast_column_to_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>This is a convenience function for casting a single column on a given table to another data type.</p> Details <p>At it's core, it will call the function like this:</p> <pre><code>dataframe = dataframe.withColumn(column, F.col(column).cast(datatype))\n</code></pre> <p>The reason for wrapping it up in this function is for validation of a columns existence and convenient re-declaration of the same.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>column</code> <code>str</code> <p>The column to be updated.</p> required <code>datatype</code> <code>Union[str, type, DataType]</code> <p>The datatype to be cast to. Must be a valid <code>pyspark</code> DataType.</p> <p>Use one of the following: <pre><code>[\n    \"string\",  \"char\",\n    \"varchar\", \"binary\",\n    \"boolean\", \"decimal\",\n    \"float\",   \"double\",\n    \"byte\",    \"short\",\n    \"integer\", \"long\",\n    \"date\",    \"timestamp\",\n    \"void\",    \"timestamp_ntz\",\n]\n</code></pre></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>ColumnDoesNotExistError</code> <p>If the <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <code>ParseException</code> <p>If the given <code>datatype</code> is not a valid PySpark DataType.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 1: Valid casting<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, \"a\", \"string\")\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast column to type.</p> <p>Example 2: Invalid column<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, \"x\", \"string\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Column \"x\" does not exist in DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\"].\n</code></pre> <p>Conclusion: Column <code>x</code> does not exist as a valid column.</p> <p>Example 3: Invalid datatype<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, \"b\", \"foo\")\n</code></pre> Terminal<pre><code>ParseException: DataType \"foo\" is not supported.\n</code></pre> <p>Conclusion: Datatype <code>foo</code> is not valid.</p> See Also <ul> <li><code>assert_column_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef cast_column_to_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        This is a convenience function for casting a single column on a given table to another data type.\n\n    ???+ abstract \"Details\"\n\n        At it's core, it will call the function like this:\n\n        ```{.py .python linenums=\"1\"}\n        dataframe = dataframe.withColumn(column, F.col(column).cast(datatype))\n        ```\n\n        The reason for wrapping it up in this function is for validation of a columns existence and convenient re-declaration of the same.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        column (str):\n            The column to be updated.\n        datatype (Union[str, type, T.DataType]):\n            The datatype to be cast to.\n            Must be a valid `#!py pyspark` DataType.\n\n            Use one of the following:\n            ```{.py .python}\n            [\n                \"string\",  \"char\",\n                \"varchar\", \"binary\",\n                \"boolean\", \"decimal\",\n                \"float\",   \"double\",\n                \"byte\",    \"short\",\n                \"integer\", \"long\",\n                \"date\",    \"timestamp\",\n                \"void\",    \"timestamp_ntz\",\n            ]\n            ```\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        ColumnDoesNotExistError:\n            If the `#!py column` does not exist within `#!py dataframe.columns`.\n        ParseException:\n            If the given `#!py datatype` is not a valid PySpark DataType.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Valid casting\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, \"a\", \"string\")\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast column to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Invalid column\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, \"x\", \"string\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Column \"x\" does not exist in DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\"].\n        ```\n        !!! failure \"Conclusion: Column `x` does not exist as a valid column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid datatype\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, \"b\", \"foo\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ParseException: DataType \"foo\" is not supported.\n        ```\n        !!! failure \"Conclusion: Datatype `foo` is not valid.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_column_exists()`][toolbox_pyspark.checks.column_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n    assert_column_exists(dataframe, column)\n    datatype = _validate_pyspark_datatype(datatype=datatype)\n    return dataframe.withColumn(column, F.col(column).cast(datatype))  # type:ignore\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.cast_columns_to_type","title":"cast_columns_to_type","text":"<pre><code>cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_list],\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Cast multiple columns to a given type.</p> Details <p>An extension of <code>cast_column_to_type()</code> to allow casting of multiple columns simultaneously.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>columns</code> <code>Union[str, str_list]</code> <p>The list of columns to be updated. They all must be valid columns existing on <code>DataFrame</code>.</p> required <code>datatype</code> <code>Union[str, type, DataType]</code> <p>The datatype to be cast to. Must be a valid PySpark DataType.</p> <p>Use one of the following:     <pre><code>[\n    \"string\",  \"char\",\n    \"varchar\", \"binary\",\n    \"boolean\", \"decimal\",\n    \"float\",   \"double\",\n    \"byte\",    \"short\",\n    \"integer\", \"long\",\n    \"date\",    \"timestamp\",\n    \"void\",    \"timestamp_ntz\",\n]\n</code></pre></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, [\"a\"], \"string\")\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | bigint   |\n| d        | bigint   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast column to type.</p> <p>Example 2: Multiple columns<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, [\"c\", \"d\"], \"string\")\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast columns to type.</p> <p>Example 3: Invalid column<pre><code>&gt;&gt;&gt; df = cast_columns_to_type(df, [\"x\", \"y\"], \"string\")\n</code></pre> Terminal<pre><code>ColumnDoesNotExistError: Columns [\"x\", \"y\"] do not exist in DataFrame.\nTry one of: [\"a\", \"b\", \"c\", \"d\"].\n</code></pre> <p>Conclusion: Columns <code>[x]</code> does not exist as a valid column.</p> <p>Example 4: Invalid datatype<pre><code>&gt;&gt;&gt; df = cast_columns_to_type(df, [\"a\", \"b\"], \"foo\")\n</code></pre> Terminal<pre><code>ParseException: DataType \"foo\" is not supported.\n</code></pre> <p>Conclusion: Datatype <code>foo</code> is not valid.</p> See Also <ul> <li><code>assert_columns_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_list],\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Cast multiple columns to a given type.\n\n    ???+ abstract \"Details\"\n        An extension of [`#!py cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type] to allow casting of multiple columns simultaneously.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        columns (Union[str, str_list]):\n            The list of columns to be updated. They all must be valid columns existing on `#!py DataFrame`.\n        datatype (Union[str, type, T.DataType]):\n            The datatype to be cast to.\n            Must be a valid PySpark DataType.\n\n            Use one of the following:\n                ```{.py .python}\n                [\n                    \"string\",  \"char\",\n                    \"varchar\", \"binary\",\n                    \"boolean\", \"decimal\",\n                    \"float\",   \"double\",\n                    \"byte\",    \"short\",\n                    \"integer\", \"long\",\n                    \"date\",    \"timestamp\",\n                    \"void\",    \"timestamp_ntz\",\n                ]\n                ```\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, [\"a\"], \"string\")\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | bigint   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast column to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Multiple columns\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, [\"c\", \"d\"], \"string\")\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast columns to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid column\"}\n        &gt;&gt;&gt; df = cast_columns_to_type(df, [\"x\", \"y\"], \"string\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ColumnDoesNotExistError: Columns [\"x\", \"y\"] do not exist in DataFrame.\n        Try one of: [\"a\", \"b\", \"c\", \"d\"].\n        ```\n        !!! failure \"Conclusion: Columns `[x]` does not exist as a valid column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Invalid datatype\"}\n        &gt;&gt;&gt; df = cast_columns_to_type(df, [\"a\", \"b\"], \"foo\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ParseException: DataType \"foo\" is not supported.\n        ```\n        !!! failure \"Conclusion: Datatype `foo` is not valid.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n    columns = [columns] if is_type(columns, str) else columns\n    assert_columns_exists(dataframe, columns)\n    datatype = _validate_pyspark_datatype(datatype=datatype)\n    return dataframe.withColumns({col: F.col(col).cast(datatype) for col in columns})\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.map_cast_columns_to_type","title":"map_cast_columns_to_type","text":"<pre><code>map_cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns_type_mapping: dict[\n        Union[str, type, T.DataType],\n        Union[str, str_list, str_tuple],\n    ],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Take a dictionary mapping of where the keys is the type and the values are the column(s), and apply that to the given dataframe.</p> Details <p>Applies <code>cast_columns_to_type()</code> and <code>cast_column_to_type()</code> under the hood.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <code>columns_type_mapping</code> <code>Dict[Union[str, type, DataType], Union[str, str_list, str_tuple]]</code> <p>The mapping of the columns to manipulate. The format must be: <code>{type: columns}</code>. Where the keys are the relevant type to cast to, and the values are the column(s) for casting.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data frame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"str\": [\"a\", \"c\"]})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast columns to type.</p> <p>Example 2: Multiple types<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"int\": [\"a\", \"c\"], \"str\": [\"b\"], \"float\": \"d\"})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | float    |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast columns to types.</p> <p>Example 3: All to single type<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {str: [col for col in df.columns]})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast all columns to type.</p> See Also <ul> <li><code>cast_column_to_type()</code></li> <li><code>cast_columns_to_type()</code></li> <li><code>assert_columns_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef map_cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns_type_mapping: dict[\n        Union[str, type, T.DataType],\n        Union[str, str_list, str_tuple],\n    ],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Take a dictionary mapping of where the keys is the type and the values are the column(s), and apply that to the given dataframe.\n\n    ???+ abstract \"Details\"\n        Applies [`#!py cast_columns_to_type()`][toolbox_pyspark.types.cast_columns_to_type] and [`#!py cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type] under the hood.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to transform.\n        columns_type_mapping (Dict[ Union[str, type, T.DataType], Union[str, str_list, str_tuple], ]):\n            The mapping of the columns to manipulate.&lt;br&gt;\n            The format must be: `#!py {type: columns}`.&lt;br&gt;\n            Where the keys are the relevant type to cast to, and the values are the column(s) for casting.\n\n    Returns:\n        (psDataFrame):\n            The transformed data frame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"str\": [\"a\", \"c\"]})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast columns to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Multiple types\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"int\": [\"a\", \"c\"], \"str\": [\"b\"], \"float\": \"d\"})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | float    |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast columns to types.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: All to single type\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {str: [col for col in df.columns]})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast all columns to type.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type]\n        - [`cast_columns_to_type()`][toolbox_pyspark.types.cast_columns_to_type]\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n\n    # Ensure all keys are `str`\n    keys = (*columns_type_mapping.keys(),)\n    for key in keys:\n        if is_type(key, type):\n            if key.__name__ in keys:\n                columns_type_mapping[key.__name__] = list(\n                    columns_type_mapping[key.__name__]\n                ) + list(columns_type_mapping.pop(key))\n            else:\n                columns_type_mapping[key.__name__] = columns_type_mapping.pop(key)\n\n    # Reverse keys and values\n    reversed_mapping = dict_reverse_keys_and_values(dictionary=columns_type_mapping)\n\n    # Validate\n    assert_columns_exists(dataframe, reversed_mapping.keys())\n\n    # Apply mapping to dataframe\n    try:\n        dataframe = dataframe.withColumns(\n            {\n                col: F.col(col).cast(_validate_pyspark_datatype(typ))\n                for col, typ in reversed_mapping.items()\n            }\n        )\n    except Exception as e:  # pragma: no cover\n        raise RuntimeError(f\"Raised {e.__class__.__name__}: {e}\") from e\n\n    # Return\n    return dataframe\n</code></pre>"},{"location":"usage/overview/","title":"Overview<code>toolbox-pyspark</code>","text":""},{"location":"usage/overview/#introduction","title":"Introduction","text":"<p>The purpose of this package is to provide some helper files/functions/classes for generic PySpark processes.</p>"},{"location":"usage/overview/#key-urls","title":"Key URLs","text":"<p>For reference, these URL's are used:</p> Type Source URL Git Repo GitHub https://github.com/data-science-extensions/toolbox-pyspark Python Package PyPI https://pypi.org/project/toolbox-pyspark Package Docs Pages https://data-science-extensions.com/toolbox-pyspark"},{"location":"usage/overview/#installation","title":"Installation","text":"<p>You can install and use this package multiple ways by using <code>pip</code>, <code>pipenv</code>, or <code>poetry</code>.</p>"},{"location":"usage/overview/#using-pip","title":"Using <code>pip</code>:","text":"<ol> <li> <p>In your terminal, run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install --requirement=requirements.txt\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-pipenv","title":"Using <code>pipenv</code>:","text":"<ol> <li> <p>Install using environment variables:</p> <p>In your <code>Pipfile</code> file, add:</p> <pre><code>[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = false\nname = \"pypi\"\n\n[packages]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install pipenv\npython3 -m pipenv install --verbose --skip-lock --categories=root index=pypi toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m run pipenv install --verbose --skip-lock --requirements=requirements.txt\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>python3 -m pipenv install --verbose --skip-lock toolbox-pyspark\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-poetry","title":"Using <code>poetry</code>:","text":"<ol> <li> <p>In your <code>pyproject.toml</code> file, add:</p> <pre><code>[tool.poetry.dependencies]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>poetry add toolbox-pyspark\npoetry install\npoetry sync\n</code></pre> </li> </ol>"},{"location":"usage/overview/#contribution","title":"Contribution","text":"<p>Contribution is always welcome.</p> <ol> <li> <p>First, either fork or branch the main repo.</p> </li> <li> <p>Clone your forked/branched repo.</p> </li> <li> <p>Build your environment:</p> <ol> <li> <p>With <code>pipenv</code> on Windows:</p> <pre><code>if (-not (Test-Path .venv)) {mkdir .venv}\npython -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>pipenv</code> on Linux:</p> <pre><code>mkdir .venv\npython3 -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Windows:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install poetry\npython -m poetry init\npython -m poetry add $(cat requirements/root.txt)\npython -m poetry add --group=dev $(cat requirements/dev.txt)\npython -m poetry add --group=test $(cat requirements/test.txt)\npython -m poetry add --group=docs $(cat requirements/docs.txt)\npython -m poetry install\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Linux:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install poetry\npython3 -m poetry init\npython3 -m poetry add $(cat requirements/root.txt)\npython3 -m poetry add --group=dev $(cat requirements/dev.txt)\npython3 -m poetry add --group=test $(cat requirements/test.txt)\npython3 -m poetry add --group=docs $(cat requirements/docs.txt)\npython3 -m poetry install\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> </ol> </li> <li> <p>Start contributing.</p> </li> <li> <p>When you're happy with the changes, raise a Pull Request to merge with the main branch again.</p> </li> </ol>"},{"location":"usage/overview/#build-and-test","title":"Build and Test","text":"<p>To ensure that the package is working as expected, please ensure that:</p> <ol> <li>You write your code as per PEP8 requirements.</li> <li>You write a UnitTest for each function/feature you include.</li> <li>The CodeCoverage is 100%.</li> <li>All UnitTests are passing.</li> <li>MyPy is passing 100%.</li> </ol>"},{"location":"usage/overview/#testing","title":"Testing","text":"<ul> <li> <p>Run them all together</p> <pre><code>poetry run make check\n</code></pre> </li> <li> <p>Or run them individually:</p> <ul> <li> <p>Black <pre><code>poetry run make check-black\n</code></pre></p> </li> <li> <p>PyTests:     <pre><code>poetry run make ckeck-pytest\n</code></pre></p> </li> <li> <p>MyPy:     <pre><code>poetry run make check-mypy\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"usage/roadmap/","title":"Roadmap","text":"Module Completed Issue Milestone <code>io</code> \u2705 <code>checks</code> \u2705 <code>types</code> \u2705 <code>keys</code> \u2705 <code>scale</code> \u2705 <code>dimensions</code> \u2705 <code>columns</code> \u2705 <code>datetime</code> \u2705 <code>cleaning</code> \u2705 <code>delta</code> \u2b1c <code>duplication</code> \u2b1c <code>schema</code> \u2b1c"}]}