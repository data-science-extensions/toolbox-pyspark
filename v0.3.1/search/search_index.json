{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"code/","title":"Modules","text":""},{"location":"code/#overview","title":"Overview","text":"<p>There are 12 modules used in this package, which covers 41 functions</p>"},{"location":"code/#module-descriptions","title":"Module Descriptions","text":"Module Description io The <code>io</code> module is used for reading and writing tables to/from directories. checks The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe. types The <code>types</code> module is used to get, check, and change a datafames column data types."},{"location":"code/#functions-by-module","title":"Functions by Module","text":"Module Function <code>io</code> <code>read_from_path()</code> <code>write_to_path()</code> <code>transfer_table()</code> <code>checks</code> <code>column_exists()</code> <code>columns_exists()</code> <code>is_vaid_spark_type()</code> <code>table_exists()</code> <code>types</code> <code>get_column_types()</code> <code>cast_column_to_type()</code> <code>cast_columns_to_type()</code> <code>map_cast_columns_to_type()</code>"},{"location":"code/#testing","title":"Testing","text":"<p>This package is fully tested against:</p> <ol> <li>Unit tests</li> <li>Lint tests</li> <li>MyPy tests</li> <li>Build tests</li> </ol>"},{"location":"code/#latest-code-coverage","title":"Latest Code Coverage","text":""},{"location":"code/checks/","title":"Checks","text":""},{"location":"code/checks/#toolbox_pyspark.checks","title":"toolbox_pyspark.checks","text":"<p>Summary</p> <p>The <code>checks</code> module is used to check and validate various attributed about a given <code>pyspark</code> dataframe.</p>"},{"location":"code/checks/#toolbox_pyspark.checks.column_exists","title":"column_exists","text":"<pre><code>column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>column.upper()</code>. Default: <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if exists or <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import column_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example1: Column Exists<pre><code>&gt;&gt;&gt; result = column_exists(df, \"a\")\n&gt;&gt;&gt; print(result)\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Column exists.</p> <p>Example 2: Column Missing<pre><code>&gt;&gt;&gt; result = column_exists(df, \"c\")\n&gt;&gt;&gt; print(result)\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py column.upper()`.&lt;br&gt;\n            Default: `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            `#!py True` if exists or `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import column_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example1: Column Exists\"}\n        &gt;&gt;&gt; result = column_exists(df, \"a\")\n        &gt;&gt;&gt; print(result)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Column exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Column Missing\"}\n        &gt;&gt;&gt; result = column_exists(df, \"c\")\n        &gt;&gt;&gt; print(result)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_exists(dataframe, [column], match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.columns_exists","title":"columns_exists","text":"<pre><code>columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str_list, str_tuple, str_set],\n    match_case: bool = False,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check</p> required <code>columns</code> <code>Union[str_list, str_tuple, str_set]</code> <p>The columns to check</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>[col.upper() for col in columns]</code>. Default: <code>False</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all columns exist or <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import columns_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: Columns exist<pre><code>&gt;&gt;&gt; columns_exists(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: All columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; columns_exists(df, [\"b\", \"d\"])\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: One column is missing.</p> <p>Example 3: All columns missing<pre><code>&gt;&gt;&gt; columns_exists(df, [\"c\", \"d\"])\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: All columns are missing.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str_list, str_tuple, str_set],\n    match_case: bool = False,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check\n        columns (Union[str_list, str_tuple, str_set]):\n            The columns to check\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py [col.upper() for col in columns]`.&lt;br&gt;\n            Default: `#!py False`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            `#!py True` if all columns exist or `#!py False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import columns_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Columns exist\"}\n        &gt;&gt;&gt; columns_exists(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: All columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; columns_exists(df, [\"b\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: One column is missing.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: All columns missing\"}\n        &gt;&gt;&gt; columns_exists(df, [\"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: All columns are missing.\"\n        &lt;/div&gt;\n    \"\"\"\n    return _columns_exists(dataframe, columns, match_case).result\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_column_exists","title":"assert_column_exists","text":"<pre><code>assert_column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>column</code> exists as a valid column within <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check.</p> required <code>column</code> <code>str</code> <p>The column to check.</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>column.upper()</code>. Default: <code>True</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1,2,3,4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_column_exists(df, \"a\")\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Column exists.</p> <p>Example 2: Error raised<pre><code>&gt;&gt;&gt; assert_column_exists(df, \"c\")\n</code></pre> Terminal<pre><code>Attribute Error: Column 'c' does not exist in 'dataframe'.\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_column_exists(\n    dataframe: psDataFrame,\n    column: str,\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py column` exists as a valid column within `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check.\n        column (str):\n            The column to check.\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py column.upper()`.&lt;br&gt;\n            Default: `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If `#!py column` does not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_column_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1,2,3,4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_column_exists(df, \"a\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Column exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Error raised\"}\n        &gt;&gt;&gt; assert_column_exists(df, \"c\")\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Column 'c' does not exist in 'dataframe'.\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    if not column_exists(dataframe, column, match_case):\n        raise AttributeError(\n            f\"Column '{column}' does not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}.\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.assert_columns_exists","title":"assert_columns_exists","text":"<pre><code>assert_columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str_list, str_tuple, str_set],\n    match_case: bool = False,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether all of the values in <code>columns</code> exist in <code>dataframe.columns</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to check</p> required <code>columns</code> <code>Union[str_list, str_tuple, str_set]</code> <p>The columns to check</p> required <code>match_case</code> <code>bool</code> <p>Whether or not to match the string case for the columns. If <code>False</code>, will default to: <code>[col.upper() for col in columns]</code>. Default: <code>True</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If the <code>columns</code> do not exist within <code>dataframe.columns</code>.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n</code></pre> <p>Example 1: No error<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"a\", \"b\"])\n</code></pre> Terminal<pre><code>None\n</code></pre> <p>Conclusion: Columns exist.</p> <p>Example 2: One column missing<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"c\"] do not exist in 'dataframe'.\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Column 'c' does not exist.</p> <p>Example 3: Multiple columns missing<pre><code>&gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\", \"d\"])\n</code></pre> Terminal<pre><code>Attribute Error: Columns [\"c\", \"d\"] do not exist in 'dataframe'.\nTry one of: [\"a\", \"b\"].\n</code></pre> <p>Conclusion: Columns 'c' and 'd' does not exist.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef assert_columns_exists(\n    dataframe: psDataFrame,\n    columns: Union[str_list, str_tuple, str_set],\n    match_case: bool = False,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether all of the values in `#!py columns` exist in `#!py dataframe.columns`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to check\n        columns (Union[str_list, str_tuple, str_set]):\n            The columns to check\n        match_case (bool, optional):\n            Whether or not to match the string case for the columns.&lt;br&gt;\n            If `#!py False`, will default to: `#!py [col.upper() for col in columns]`.&lt;br&gt;\n            Default: `#!py True`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If the `#!py columns` do not exist within `#!py dataframe.columns`.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.checks import assert_columns_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: No error\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"a\", \"b\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        None\n        ```\n        !!! success \"Conclusion: Columns exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: One column missing\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"c\"] do not exist in 'dataframe'.\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Column 'c' does not exist.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Multiple columns missing\"}\n        &gt;&gt;&gt; assert_columns_exists(df, [\"b\", \"c\", \"d\"])\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        Attribute Error: Columns [\"c\", \"d\"] do not exist in 'dataframe'.\n        Try one of: [\"a\", \"b\"].\n        ```\n        !!! failure \"Conclusion: Columns 'c' and 'd' does not exist.\"\n        &lt;/div&gt;\n    \"\"\"\n    (exist, missing_cols) = _columns_exists(dataframe, columns, match_case)\n    if not exist:\n        raise AttributeError(\n            f\"Columns {missing_cols} do not exist in 'dataframe'.\\n\"\n            f\"Try one of: {dataframe.columns}\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.is_vaid_spark_type","title":"is_vaid_spark_type","text":"<pre><code>is_vaid_spark_type(datatype: str) -&gt; None\n</code></pre> <p>Summary</p> <p>Check whether a given <code>datatype</code> is a correct and valid <code>pyspark</code> data type.</p> <p>Parameters:</p> Name Type Description Default <code>datatype</code> <code>str</code> <p>The name of the data type to check.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If the given <code>datatype</code> is not a valid <code>pyspark</code> data type.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned. Either an <code>AttributeError</code> exception is raised, or nothing.</p> Examples Set up<pre><code>&gt;&gt;&gt; from toolbox_pyspark.checks import is_vaid_spark_type\n</code></pre> <p>Loop through all valid types<pre><code>&gt;&gt;&gt; type_names = [\"string\", \"char\", \"varchar\", \"binary\", \"boolean\", \"decimal\", \"float\", \"double\", \"byte\", \"short\", \"integer\", \"long\", \"date\", \"timestamp\", \"timestamp_ntz\", \"void\"]\n&gt;&gt;&gt; for type_name in type_names:\n...     is_vaid_spark_type(type_name)\n</code></pre>  Nothing is returned each time. Because they're all valid. <p>Conclusion: They're all valid.</p> <p>Check some invalid types<pre><code>&gt;&gt;&gt; type_names = [\"np.ndarray\", \"pd.DataFrame\", \"dict\"]\n&gt;&gt;&gt; for type_name in type_names:\n...     is_vaid_spark_type(type_name)\n</code></pre> Terminal<pre><code>AttributeError: DataType 'np.ndarray' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> Terminal<pre><code>AttributeError: DataType 'pd.DataFrame' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> Terminal<pre><code>AttributeError: DataType 'dict' is not valid.\nMust be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n</code></pre> <p>Conclusion: All of these types are invalid.</p> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef is_vaid_spark_type(datatype: str) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Check whether a given `#!py datatype` is a correct and valid `#!py pyspark` data type.\n\n    Params:\n        datatype (str):\n            The name of the data type to check.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If the given `#!py datatype` is not a valid `#!py pyspark` data type.\n\n    Returns:\n        (type(None)):\n            Nothing is returned. Either an `#!py AttributeError` exception is raised, or nothing.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; from toolbox_pyspark.checks import is_vaid_spark_type\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Loop through all valid types\"}\n        &gt;&gt;&gt; type_names = [\"string\", \"char\", \"varchar\", \"binary\", \"boolean\", \"decimal\", \"float\", \"double\", \"byte\", \"short\", \"integer\", \"long\", \"date\", \"timestamp\", \"timestamp_ntz\", \"void\"]\n        &gt;&gt;&gt; for type_name in type_names:\n        ...     is_vaid_spark_type(type_name)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        Nothing is returned each time. Because they're all valid.\n        !!! success \"Conclusion: They're all valid.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Check some invalid types\"}\n        &gt;&gt;&gt; type_names = [\"np.ndarray\", \"pd.DataFrame\", \"dict\"]\n        &gt;&gt;&gt; for type_name in type_names:\n        ...     is_vaid_spark_type(type_name)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'np.ndarray' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'pd.DataFrame' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: DataType 'dict' is not valid.\n        Must be one of: [\"binary\", \"bool\", \"boolean\", \"byte\", \"char\", \"date\", \"decimal\", \"double\", \"float\", \"int\", \"integer\", \"long\", \"short\", \"str\", \"string\", \"timestamp\", \"timestamp_ntz\", \"varchar\", \"void\"]\n        ```\n        !!! failure \"Conclusion: All of these types are invalid.\"\n        &lt;/div&gt;\n    \"\"\"\n    if datatype not in VALID_PYSPARK_TYPE_NAMES:\n        raise AttributeError(\n            f\"DataType '{datatype}' is not valid.\\n\"\n            f\"Must be one of: {VALID_PYSPARK_TYPE_NAMES}\"\n        )\n</code></pre>"},{"location":"code/checks/#toolbox_pyspark.checks.table_exists","title":"table_exists","text":"<pre><code>table_exists(\n    name: str,\n    path: str,\n    data_format: str,\n    spark_session: SparkSession,\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Will try to read <code>table</code> from <code>path</code> using <code>format</code>, and if successful will return <code>True</code> otherwise <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to check exists.</p> required <code>path</code> <code>str</code> <p>The directory where the table should be existing.</p> required <code>data_format</code> <code>str</code> <p>The format of the table to try checking.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The <code>spark</code> session to use for the importing.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Returns <code>True</code> if the table exists, <code>False</code> otherwise.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Constants\n&gt;&gt;&gt; write_name = \"test_df\"\n&gt;&gt;&gt; write_path = f\"./test\"\n&gt;&gt;&gt; write_format = \"parquet\"\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write data\n&gt;&gt;&gt; write_to_path(df, f\"{write_name}.{write_format}\", write_path)\n</code></pre> <p>Example 1: Table exists<pre><code>&gt;&gt;&gt; table_exists(\"test_df.parquet\", \"./test\", \"parquet\", spark)\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Table exists.</p> <p>Example 2: Table does not exist<pre><code>&gt;&gt;&gt; table_exists(\"bad_table_name.parquet\", \"./test\", \"parquet\", spark)\n</code></pre> Terminal<pre><code>False\n</code></pre> <p>Conclusion: Table does not exist.</p> See Also <ul> <li><code>toolbox_pyspark.io.read_from_path()</code></li> </ul> Source code in <code>src/toolbox_pyspark/checks.py</code> <pre><code>@typechecked\ndef table_exists(\n    name: str,\n    path: str,\n    data_format: str,\n    spark_session: SparkSession,\n) -&gt; bool:\n    \"\"\"\n    !!! note \"Summary\"\n        Will try to read `#!py table` from `#!py path` using `#!py format`, and if successful will return `#!py True` otherwise `#!py False`.\n\n    Params:\n        name (str):\n            The name of the table to check exists.\n        path (str):\n            The directory where the table should be existing.\n        data_format (str):\n            The format of the table to try checking.\n        spark_session (SparkSession):\n            The `#!py spark` session to use for the importing.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (bool):\n            Returns `#!py True` if the table exists, `False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Constants\n        &gt;&gt;&gt; write_name = \"test_df\"\n        &gt;&gt;&gt; write_path = f\"./test\"\n        &gt;&gt;&gt; write_format = \"parquet\"\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Write data\n        &gt;&gt;&gt; write_to_path(df, f\"{write_name}.{write_format}\", write_path)\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Table exists\"}\n        &gt;&gt;&gt; table_exists(\"test_df.parquet\", \"./test\", \"parquet\", spark)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Table exists.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Table does not exist\"}\n        &gt;&gt;&gt; table_exists(\"bad_table_name.parquet\", \"./test\", \"parquet\", spark)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        False\n        ```\n        !!! failure \"Conclusion: Table does not exist.\"\n        &lt;/div&gt;\n\n    ???+ tip \"See Also\"\n        - [`toolbox_pyspark.io.read_from_path()`][toolbox_pyspark.io.read_from_path]\n    \"\"\"\n    try:\n        _ = read_from_path(\n            name=name,\n            path=path,\n            data_format=data_format,\n            spark_session=spark_session,\n        )\n    except Exception:\n        return False\n    return True\n</code></pre>"},{"location":"code/io/","title":"IO","text":""},{"location":"code/io/#toolbox_pyspark.io","title":"toolbox_pyspark.io","text":"<p>Summary</p> <p>The <code>io</code> module is used for reading and writing tables to/from directories.</p>"},{"location":"code/io/#toolbox_pyspark.io.read_from_path","title":"read_from_path","text":"<pre><code>read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Read an object from a given <code>path</code> in to memory as a <code>pyspark</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to read in.</p> required <code>path</code> <code>str</code> <p>The path from which it will be read.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The Spark session to use for the reading.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format of the object at location <code>path</code>. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>read_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. Like, for example:</p> <ul> <li>If the object is a CSV, you may want to define that it has a header row: <code>{\"header\": \"true\"}</code>.</li> <li>If the object is a Delta table, you may want to query a specific version: <code>{versionOf\": \"0\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The loaded dataframe.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         \"c\": [1, 1, 1, 1],\n...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...     }\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Write data\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> Terminal<pre><code>[\"table.csv\", \"table.parquet\"]\n</code></pre> </p> <p>Example 1: Read CSV<pre><code>&gt;&gt;&gt; df_csv = read_from_path(\n...     name=\"table.csv\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"csv\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_csv.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully read CSV.</p> <p>Example 2: Read Parquet<pre><code>&gt;&gt;&gt; df_parquet = read_from_path(\n...     name=\"table.parquet\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"parquet\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_parquet.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> <p>Conclusion: Successfully read Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Read an object from a given `path` in to memory as a `pyspark` dataframe.\n\n    Params:\n        name (str):\n            The name of the table to read in.\n        path (str):\n            The path from which it will be read.\n        spark_session (SparkSession):\n            The Spark session to use for the reading.\n        data_format (Optional[str], optional):\n            The format of the object at location `path`.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        read_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            Like, for example:&lt;br&gt;\n\n            - If the object is a CSV, you may want to define that it has a header row: `#!py {\"header\": \"true\"}`.\n            - If the object is a Delta table, you may want to query a specific version: `#!py {versionOf\": \"0\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The loaded dataframe.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4],\n        ...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         \"c\": [1, 1, 1, 1],\n        ...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Write data\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"table.csv\", \"table.parquet\"]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Read CSV\"}\n        &gt;&gt;&gt; df_csv = read_from_path(\n        ...     name=\"table.csv\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"csv\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df_csv.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully read CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Read Parquet\"}\n        &gt;&gt;&gt; df_parquet = read_from_path(\n        ...     name=\"table.parquet\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"parquet\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; df_parquet.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        !!! success \"Conclusion: Successfully read Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    data_format: str = data_format or \"parquet\"\n    reader: DataFrameReader = spark_session.read.format(data_format)\n    if read_options:\n        reader.options(**read_options)\n    return reader.load(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.write_to_path","title":"write_to_path","text":"<pre><code>write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>For a given <code>table</code>, write it out to a specified <code>path</code> with name <code>name</code> and format <code>format</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame</code> <p>The table to be written. Must be a valid <code>pyspark</code> DataFrame (<code>pyspark.sql.DataFrame</code>).</p> required <code>name</code> <code>str</code> <p>The name of the table where it will be written.</p> required <code>path</code> <code>str</code> <p>The path location for where to save the table.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format that the <code>table</code> will be written to. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>write_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. Like, for example:</p> <ul> <li>If you are writing to a Delta object, and wanted to overwrite the schema: <code>{\"overwriteSchema\": \"true\"}</code>.</li> <li>If you\"re writing to a CSV file, and wanted to specify the header row: <code>{\"header\": \"true\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Note <p>You know that this function is successful if the table exists at the specified location, and there are no errors thrown.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; df.show()\n</code></pre> Terminal<pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Example 1: Write to CSV<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     mode=\"overwrite\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully written to CSV.</p> <p>Example 2: Write to Parquet<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     mode=\"overwrite\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully written to Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `table`, write it out to a specified `path` with name `name` and format `format`.\n\n    Params:\n        table (psDataFrame):\n            The table to be written. Must be a valid `pyspark` DataFrame ([`pyspark.sql.DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)).\n        name (str):\n            The name of the table where it will be written.\n        path (str):\n            The path location for where to save the table.\n        data_format (Optional[str], optional):\n            The format that the `table` will be written to.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        write_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            Like, for example:\n\n            - If you are writing to a Delta object, and wanted to overwrite the schema: `#!py {\"overwriteSchema\": \"true\"}`.\n            - If you\"re writing to a CSV file, and wanted to specify the header row: `#!py {\"header\": \"true\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n        partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ tip \"Note\"\n        You know that this function is successful if the table exists at the specified location, and there are no errors thrown.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Write to CSV\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     mode=\"overwrite\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully written to CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Write to Parquet\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     mode=\"overwrite\",\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully written to Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    write_options: str_dict = write_options or dict()\n    data_format: str = data_format or \"parquet\"\n    writer: DataFrameWriter = table.write.mode(mode).format(data_format)\n    if write_options:\n        writer.options(**write_options)\n    if partition_cols is not None:\n        partition_cols = (\n            [partition_cols] if is_type(partition_cols, str) else partition_cols\n        )\n        writer = writer.partitionBy(list(partition_cols))\n    writer.save(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.transfer_table","title":"transfer_table","text":"<pre><code>transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[\n        str_collection\n    ] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Copy a table from one location to another.</p> Details <p>This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.</p> <p>Parameters:</p> Name Type Description Default <code>spark_session</code> <code>SparkSession</code> <p>The spark session to use for the transfer. Necessary in order to instantiate the reading process.</p> required <code>from_table_path</code> <code>str</code> <p>The path from which the table will be read.</p> required <code>from_table_name</code> <code>str</code> <p>The name of the table to be read.</p> required <code>from_table_format</code> <code>str</code> <p>The format of the data at the reading location.</p> required <code>to_table_path</code> <code>str</code> <p>The location where to save the table to.</p> required <code>to_table_name</code> <code>str</code> <p>The name of the table where it will be saved.</p> required <code>to_table_format</code> <code>str</code> <p>The format of the saved table.</p> required <code>from_table_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>to_table_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Note <p>You know that this function is successful if the table exists at the specified location, and there are no errors thrown.</p> Examples Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [1, 2, 3, 4],\n...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...         \"c\": [1, 1, 1 1],\n...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...     }\n... )\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> Terminal<pre><code>[\"table.csv\", \"table.parquet\"]\n</code></pre> </p> <p>Example 1: Transfer CSV<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.csv\",\n...     to_table_format=\"csv\",\n...     from_table_options={\"header\": \"true\"},\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./other\",\n...     data_format=\"csv\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred CSV to CSV.</p> <p>Example 2: Transfer Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.parquet\",\n...     from_table_format=\"parquet\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred Parquet to Parquet.</p> <p>Example 3: Transfer CSV to Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> Terminal<pre><code>True\n</code></pre> <p>Conclusion: Successfully transferred CSV to Parquet.</p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Copy a table from one location to another.\n\n    ???+ abstract \"Details\"\n        This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.\n\n    Params:\n        spark_session (SparkSession):\n            The spark session to use for the transfer. Necessary in order to instantiate the reading process.\n        from_table_path (str):\n            The path from which the table will be read.\n        from_table_name (str):\n            The name of the table to be read.\n        from_table_format (str):\n            The format of the data at the reading location.\n        to_table_path (str):\n            The location where to save the table to.\n        to_table_name (str):\n            The name of the table where it will be saved.\n        to_table_format (str):\n            The format of the saved table.\n        from_table_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        to_table_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ tip \"Note\"\n        You know that this function is successful if the table exists at the specified location, and there are no errors thrown.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [1, 2, 3, 4],\n        ...         \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...         \"c\": [1, 1, 1 1],\n        ...         \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        [\"table.csv\", \"table.parquet\"]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Transfer CSV\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.csv\",\n        ...     to_table_format=\"csv\",\n        ...     from_table_options={\"header\": \"true\"},\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./other\",\n        ...     data_format=\"csv\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred CSV to CSV.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Transfer Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.parquet\",\n        ...     from_table_format=\"parquet\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred Parquet to Parquet.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Transfer CSV to Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.sh .shell title=\"Terminal\"}\n        True\n        ```\n        !!! success \"Conclusion: Successfully transferred CSV to Parquet.\"\n        &lt;/div&gt;\n    \"\"\"\n    from_table_options: str_dict = from_table_options or dict()\n    to_table_options: str_dict = to_table_options or dict()\n    from_table: psDataFrame = read_from_path(\n        name=from_table_name,\n        path=from_table_path,\n        spark_session=spark_session,\n        data_format=from_table_format,\n        read_options=from_table_options,\n    )\n    write_to_path(\n        table=from_table,\n        name=to_table_name,\n        path=to_table_path,\n        data_format=to_table_format,\n        mode=to_table_mode,\n        write_options=to_table_options,\n        partition_cols=to_table_partition_cols,\n    )\n</code></pre>"},{"location":"code/types/","title":"Types","text":""},{"location":"code/types/#toolbox_pyspark.types","title":"toolbox_pyspark.types","text":"<p>Summary</p> <p>The <code>types</code> module is used to get, check, and change a datafames column data types.</p>"},{"location":"code/types/#toolbox_pyspark.types.get_column_types","title":"get_column_types","text":"<pre><code>get_column_types(\n    dataframe: psDataFrame, output_type: str = \"psDataFrame\"\n) -&gt; Union[psDataFrame, pdDataFrame]\n</code></pre> <p>Summary</p> <p>This is a convenient function to return the data types from a given table as either a <code>pyspark.sql.DataFrame</code> or <code>pandas.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be checked.</p> required <code>output_type</code> <code>str</code> <p>How should the data be returned? As <code>pdDataFrame</code> or <code>psDataFrame</code>.</p> <p>For <code>pandas</code>, use one of:</p> <pre><code>[\n    \"pandas\", \"pandas.DataFrame\",\n    \"pd.df\",  \"pd.DataFrame\",\n    \"pddf\",   \"pdDataFrame\",\n    \"pd\",     \"pdDF\",\n]\n</code></pre> <p>For <code>pyspark</code> use one of:</p> <pre><code>[\n    \"pyspark\", \"spark.DataFrame\",\n    \"spark\",   \"pyspark.DataFrame\",\n    \"ps.df\",   \"ps.DataFrame\",\n    \"psdf\",    \"psDataFrame\",\n    \"ps\",      \"psDF\",\n]\n</code></pre> <p>Any other options are invalid. Defaults to <code>\"psDataFrame\"</code>.</p> <code>'psDataFrame'</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If the given value parsed to <code>output_type</code> is not one of the given valid types.</p> <p>Returns:</p> Type Description <code>Union[DataFrame, DataFrame]</code> <p>The DataFrame where each row represents a column on the original <code>dataframe</code> object, and which has two columns:</p> <ol> <li>The column name from <code>dataframe</code>; and</li> <li>The data type for that column in <code>dataframe</code>.</li> </ol> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; print(df.dtypes)\n</code></pre> Terminal<pre><code>[\n    (\"a\", \"bigint\"),\n    (\"b\", \"string\"),\n    (\"c\", \"bigint\"),\n    (\"d\", \"string\"),\n]\n</code></pre> </p> <p>Example 1: Return PySpark<pre><code>&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully print PySpark output.</p> <p>Example 2: Return Pandas<pre><code>&gt;&gt;&gt; print(get_column_types(df, \"pd\"))\n</code></pre> Terminal<pre><code>   col_name  col_type\n0         a    bigint\n1         b    string\n2         c    bigint\n3         d    string\n</code></pre> <p>Conclusion: Successfully print Pandas output.</p> <p>Example 3: Invalid output<pre><code>&gt;&gt;&gt; print(get_column_types(df, \"foo\"))\n</code></pre> Terminal<pre><code>AttributeError: Invalid value for `output_type`: 'foo'.\nMust be one of: [\"pandas.DataFrame\", \"pandas\", \"pd.DataFrame\", \"pd.df\", \"pddf\", \"pdDataFrame\", \"pdDF\", \"pd\", \"spark.DataFrame\", \"pyspark.DataFrame\", \"pyspark\", \"spark\", \"ps.DataFrame\", \"ps.df\", \"psdf\", \"psDataFrame\", \"psDF\", \"ps\"]\n</code></pre> <p>Conclusion: Invalid input.</p> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef get_column_types(\n    dataframe: psDataFrame,\n    output_type: str = \"psDataFrame\",\n) -&gt; Union[psDataFrame, pdDataFrame]:\n    \"\"\"\n    !!! note \"Summary\"\n        This is a convenient function to return the data types from a given table as either a `#!py pyspark.sql.DataFrame` or `#!py pandas.DataFrame`.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be checked.\n\n        output_type (str, optional):\n            How should the data be returned? As `#!py pdDataFrame` or `#!py psDataFrame`.\n\n            For `#!py pandas`, use one of:\n\n            ```{.py .python}\n            [\n                \"pandas\", \"pandas.DataFrame\",\n                \"pd.df\",  \"pd.DataFrame\",\n                \"pddf\",   \"pdDataFrame\",\n                \"pd\",     \"pdDF\",\n            ]\n            ```\n\n            &lt;/div&gt;\n\n            For `#!py pyspark` use one of:\n\n            ```{.py .python}\n            [\n                \"pyspark\", \"spark.DataFrame\",\n                \"spark\",   \"pyspark.DataFrame\",\n                \"ps.df\",   \"ps.DataFrame\",\n                \"psdf\",    \"psDataFrame\",\n                \"ps\",      \"psDF\",\n            ]\n            ```\n\n            Any other options are invalid.&lt;br&gt;\n            Defaults to `#!py \"psDataFrame\"`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If the given value parsed to `#!py output_type` is not one of the given valid types.\n\n    Returns:\n        (Union[psDataFrame, pdDataFrame]):\n            The DataFrame where each row represents a column on the original `#!py dataframe` object, and which has two columns:\n\n            1. The column name from `#!py dataframe`; and\n            2. The data type for that column in `#!py dataframe`.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; print(df.dtypes)\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        [\n            (\"a\", \"bigint\"),\n            (\"b\", \"string\"),\n            (\"c\", \"bigint\"),\n            (\"d\", \"string\"),\n        ]\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Return PySpark\"}\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully print PySpark output.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Return Pandas\"}\n        &gt;&gt;&gt; print(get_column_types(df, \"pd\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n           col_name  col_type\n        0         a    bigint\n        1         b    string\n        2         c    bigint\n        3         d    string\n        ```\n        !!! success \"Conclusion: Successfully print Pandas output.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid output\"}\n        &gt;&gt;&gt; print(get_column_types(df, \"foo\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Invalid value for `output_type`: 'foo'.\n        Must be one of: [\"pandas.DataFrame\", \"pandas\", \"pd.DataFrame\", \"pd.df\", \"pddf\", \"pdDataFrame\", \"pdDF\", \"pd\", \"spark.DataFrame\", \"pyspark.DataFrame\", \"pyspark\", \"spark\", \"ps.DataFrame\", \"ps.df\", \"psdf\", \"psDataFrame\", \"psDF\", \"ps\"]\n        ```\n        !!! failure \"Conclusion: Invalid input.\"\n        &lt;/div&gt;\n    \"\"\"\n    if output_type not in VALID_DATAFRAME_NAMES:\n        raise AttributeError(\n            f\"Invalid value for `output_type`: '{output_type}'.\\n\"\n            f\"Must be one of: {VALID_DATAFRAME_NAMES}\"\n        )\n    output = pd.DataFrame(dataframe.dtypes, columns=[\"col_name\", \"col_type\"])\n    if output_type in VALID_PYSPARK_DATAFRAME_NAMES:\n        return dataframe.sparkSession.createDataFrame(output)\n    else:\n        return output\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.cast_column_to_type","title":"cast_column_to_type","text":"<pre><code>cast_column_to_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>This is a convenience function for casting a single column on a given table to another data type.</p> Details <p>At it's core, it will call the function like this:</p> <pre><code>dataframe = dataframe.withColumn(column, F.col(column).cast(datatype))\n</code></pre> <p>The reason for wrapping it up in this function is for validation of a columns existence and convenient re-declaration of the same.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>column</code> <code>str</code> <p>The column to be updated.</p> required <code>datatype</code> <code>Union[str, type, DataType]</code> <p>The datatype to be cast to. Must be a valid <code>pyspark</code> DataType.</p> <p>Use one of the following: <pre><code>[\n    \"string\",  \"char\",\n    \"varchar\", \"binary\",\n    \"boolean\", \"decimal\",\n    \"float\",   \"double\",\n    \"byte\",    \"short\",\n    \"integer\", \"long\",\n    \"date\",    \"timestamp\",\n    \"void\",    \"timestamp_ntz\",\n]\n</code></pre></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <code>AttributeError</code> <p>If <code>column</code> does not exist within <code>dataframe.columns</code>.</p> <code>ParseException</code> <p>If the given <code>datatype</code> is not a valid PySpark DataType.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 1: Valid casting<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, 'a', 'string')\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> <p>Conclusion: Successfully cast column to type.</p> <p>Example 2: Invalid column<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, 'x', 'string')\n</code></pre> Terminal<pre><code>AttributeError: Column 'x' does not exist in DataFrame.\nTry one of: ['a', 'b', 'c', 'd'].\n</code></pre> <p>Conclusion: Column <code>x</code> does not exist as a valid column.</p> <p>Example 3: Invalid datatype<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, 'b', 'foo')\n</code></pre> Terminal<pre><code>ParseException: DataType 'foo' is not supported.\n</code></pre> <p>Conclusion: Datatype <code>foo</code> is not valid.</p> See Also <ul> <li><code>assert_column_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef cast_column_to_type(\n    dataframe: psDataFrame,\n    column: str,\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        This is a convenience function for casting a single column on a given table to another data type.\n\n    ???+ abstract \"Details\"\n\n        At it's core, it will call the function like this:\n\n        ```{.py .python linenums=\"1\"}\n        dataframe = dataframe.withColumn(column, F.col(column).cast(datatype))\n        ```\n\n        The reason for wrapping it up in this function is for validation of a columns existence and convenient re-declaration of the same.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        column (str):\n            The column to be updated.\n        datatype (Union[str, type, T.DataType]):\n            The datatype to be cast to.\n            Must be a valid `#!py pyspark` DataType.\n\n            Use one of the following:\n            ```{.py .python}\n            [\n                \"string\",  \"char\",\n                \"varchar\", \"binary\",\n                \"boolean\", \"decimal\",\n                \"float\",   \"double\",\n                \"byte\",    \"short\",\n                \"integer\", \"long\",\n                \"date\",    \"timestamp\",\n                \"void\",    \"timestamp_ntz\",\n            ]\n            ```\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n        AttributeError:\n            If `#!py column` does not exist within `#!py dataframe.columns`.\n        ParseException:\n            If the given `#!py datatype` is not a valid PySpark DataType.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Valid casting\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, 'a', 'string')\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        !!! success \"Conclusion: Successfully cast column to type.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Invalid column\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, 'x', 'string')\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Column 'x' does not exist in DataFrame.\n        Try one of: ['a', 'b', 'c', 'd'].\n        ```\n        !!! failure \"Conclusion: Column `x` does not exist as a valid column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid datatype\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, 'b', 'foo')\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ParseException: DataType 'foo' is not supported.\n        ```\n        !!! failure \"Conclusion: Datatype `foo` is not valid.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_column_exists()`][toolbox_pyspark.checks.column_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n    assert_column_exists(dataframe, column)\n    datatype = _validate_pyspark_datatype(datatype=datatype)\n    return dataframe.withColumn(column, F.col(column).cast(datatype))  # type:ignore\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.cast_columns_to_type","title":"cast_columns_to_type","text":"<pre><code>cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_list],\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Cast multiple columns to a given type.</p> Details <p>An extension of <code>cast_column_to_type()</code> to allow casting of multiple columns simultaneously.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to be updated.</p> required <code>columns</code> <code>Union[str, str_list]</code> <p>The list of columns to be updated. They all must be valid columns existing on <code>DataFrame</code>.</p> required <code>datatype</code> <code>Union[str, type, DataType]</code> <p>The datatype to be cast to. Must be a valid PySpark DataType.</p> <p>Use one of the following:     <pre><code>[\n    \"string\",  \"char\",\n    \"varchar\", \"binary\",\n    \"boolean\", \"decimal\",\n    \"float\",   \"double\",\n    \"byte\",    \"short\",\n    \"integer\", \"long\",\n    \"date\",    \"timestamp\",\n    \"void\",    \"timestamp_ntz\",\n]\n</code></pre></p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The updated DataFrame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 1: Basic usage<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, ['a'], 'string')\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | bigint   |\n| d        | bigint   |\n+----------+----------+\n</code></pre> </p> <p>Example 2: Multiple columns<pre><code>&gt;&gt;&gt; df = cast_column_to_type(df, ['c', 'd'], 'string')\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Example 3: Invalid column<pre><code>&gt;&gt;&gt; df = cast_columns_to_type(df, ['x', 'y'], 'string')\n</code></pre> Terminal<pre><code>AttributeError: Columns ['x', 'y'] do not exist in DataFrame.\nTry one of: ['a', 'b', 'c', 'd'].\n</code></pre> <p>Conclusion: Columns [<code>x</code>] does not exist as a valid column.</p> <p>Example 4: Invalid datatype<pre><code>&gt;&gt;&gt; df = cast_columns_to_type(df, ['a', 'b'], 'foo')\n</code></pre> Terminal<pre><code>ParseException: DataType 'foo' is not supported.\n</code></pre> <p>Conclusion: Datatype <code>foo</code> is not valid.</p> See Also <ul> <li><code>assert_columns_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns: Union[str, str_list],\n    datatype: Union[str, type, T.DataType],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Cast multiple columns to a given type.\n\n    ???+ abstract \"Details\"\n        An extension of [`#!py cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type] to allow casting of multiple columns simultaneously.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to be updated.\n        columns (Union[str, str_list]):\n            The list of columns to be updated. They all must be valid columns existing on `#!py DataFrame`.\n        datatype (Union[str, type, T.DataType]):\n            The datatype to be cast to.\n            Must be a valid PySpark DataType.\n\n            Use one of the following:\n                ```{.py .python}\n                [\n                    \"string\",  \"char\",\n                    \"varchar\", \"binary\",\n                    \"boolean\", \"decimal\",\n                    \"float\",   \"double\",\n                    \"byte\",    \"short\",\n                    \"integer\", \"long\",\n                    \"date\",    \"timestamp\",\n                    \"void\",    \"timestamp_ntz\",\n                ]\n                ```\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The updated DataFrame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 1: Basic usage\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, ['a'], 'string')\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | bigint   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 2: Multiple columns\"}\n        &gt;&gt;&gt; df = cast_column_to_type(df, ['c', 'd'], 'string')\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 3: Invalid column\"}\n        &gt;&gt;&gt; df = cast_columns_to_type(df, ['x', 'y'], 'string')\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        AttributeError: Columns ['x', 'y'] do not exist in DataFrame.\n        Try one of: ['a', 'b', 'c', 'd'].\n        ```\n        !!! failure \"Conclusion: Columns [`x`] does not exist as a valid column.\"\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Example 4: Invalid datatype\"}\n        &gt;&gt;&gt; df = cast_columns_to_type(df, ['a', 'b'], 'foo')\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        ParseException: DataType 'foo' is not supported.\n        ```\n        !!! failure \"Conclusion: Datatype `foo` is not valid.\"\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n    columns = [columns] if isinstance(columns, str) else columns\n    assert_columns_exists(dataframe, columns)\n    datatype = _validate_pyspark_datatype(datatype=datatype)\n    return dataframe.withColumns({col: F.col(col).cast(datatype) for col in columns})\n</code></pre>"},{"location":"code/types/#toolbox_pyspark.types.map_cast_columns_to_type","title":"map_cast_columns_to_type","text":"<pre><code>map_cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns_type_mapping: dict[\n        Union[str, type, T.DataType],\n        Union[str, str_list, str_tuple],\n    ],\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Take a dictionary mapping of where the keys is the type and the values are the column(s), and apply that to the given dataframe.</p> Details <p>Applies <code>cast_columns_to_type()</code> and <code>cast_column_to_type()</code> under the hood.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The DataFrame to transform.</p> required <code>columns_type_mapping</code> <code>Dict[Union[str, type, DataType], Union[str, str_list, str_tuple]]</code> <p>The mapping of the columns to manipulate. The format must be: <code>{type: columns}</code>. Where the keys are the relevant type to cast to, and the values are the column(s) for casting.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transformed data frame.</p> Examples <p>Set up<pre><code>&gt;&gt;&gt; # Imports\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Instantiate Spark\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create data\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame(\n...         {\n...             \"a\": [1, 2, 3, 4],\n...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n...             \"c\": [1, 1, 1, 1],\n...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n...         }\n...     )\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Basic usage<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"str\": [\"a\", \"c\"]})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> <p>Multiple types<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"int\": [\"a\", \"c\"], \"str\": [\"b\"], \"float\": \"d\"})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | bigint   |\n| b        | string   |\n| c        | bigint   |\n| d        | float    |\n+----------+----------+\n</code></pre> </p> <p>All to single type<pre><code>&gt;&gt;&gt; df = map_cast_columns_to_type(df, {str: [col for col in df.columns]})\n&gt;&gt;&gt; get_column_types(df).show()\n</code></pre> Terminal<pre><code>+----------+----------+\n| col_name | col_type |\n+----------+----------+\n| a        | string   |\n| b        | string   |\n| c        | string   |\n| d        | string   |\n+----------+----------+\n</code></pre> </p> See Also <ul> <li><code>cast_column_to_type()</code></li> <li><code>cast_columns_to_type()</code></li> <li><code>assert_columns_exists()</code></li> <li><code>is_vaid_spark_type()</code></li> <li><code>get_column_types()</code></li> </ul> Source code in <code>src/toolbox_pyspark/types.py</code> <pre><code>@typechecked\ndef map_cast_columns_to_type(\n    dataframe: psDataFrame,\n    columns_type_mapping: dict[\n        Union[str, type, T.DataType],\n        Union[str, str_list, str_tuple],\n    ],\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Take a dictionary mapping of where the keys is the type and the values are the column(s), and apply that to the given dataframe.\n\n    ???+ abstract \"Details\"\n        Applies [`#!py cast_columns_to_type()`][toolbox_pyspark.types.cast_columns_to_type] and [`#!py cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type] under the hood.\n\n    Params:\n        dataframe (psDataFrame):\n            The DataFrame to transform.\n        columns_type_mapping (Dict[ Union[str, type, T.DataType], Union[str, str_list, str_tuple], ]):\n            The mapping of the columns to manipulate.&lt;br&gt;\n            The format must be: `#!py {type: columns}`.&lt;br&gt;\n            Where the keys are the relevant type to cast to, and the values are the column(s) for casting.\n\n    Returns:\n        (psDataFrame):\n            The transformed data frame.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; # Imports\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.types import cast_column_to_type, get_column_types\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Instantiate Spark\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Create data\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"a\": [1, 2, 3, 4],\n        ...             \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        ...             \"c\": [1, 1, 1, 1],\n        ...             \"d\": [\"2\", \"2\", \"2\", \"2\"],\n        ...         }\n        ...     )\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"str\": [\"a\", \"c\"]})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Multiple types\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {\"int\": [\"a\", \"c\"], \"str\": [\"b\"], \"float\": \"d\"})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | bigint   |\n        | b        | string   |\n        | c        | bigint   |\n        | d        | float    |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"All to single type\"}\n        &gt;&gt;&gt; df = map_cast_columns_to_type(df, {str: [col for col in df.columns]})\n        &gt;&gt;&gt; get_column_types(df).show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text title=\"Terminal\"}\n        +----------+----------+\n        | col_name | col_type |\n        +----------+----------+\n        | a        | string   |\n        | b        | string   |\n        | c        | string   |\n        | d        | string   |\n        +----------+----------+\n        ```\n        &lt;/div&gt;\n\n    ??? tip \"See Also\"\n        - [`cast_column_to_type()`][toolbox_pyspark.types.cast_column_to_type]\n        - [`cast_columns_to_type()`][toolbox_pyspark.types.cast_columns_to_type]\n        - [`assert_columns_exists()`][toolbox_pyspark.checks.assert_columns_exists]\n        - [`is_vaid_spark_type()`][toolbox_pyspark.checks.is_vaid_spark_type]\n        - [`get_column_types()`][toolbox_pyspark.types.get_column_types]\n    \"\"\"\n\n    # Ensure all keys are `str`\n    keys = (*columns_type_mapping.keys(),)\n    for key in keys:\n        if isinstance(key, type):\n            if key.__name__ in keys:\n                columns_type_mapping[key.__name__] = list(\n                    columns_type_mapping[key.__name__]\n                ) + list(columns_type_mapping.pop(key))\n            else:\n                columns_type_mapping[key.__name__] = columns_type_mapping.pop(key)\n\n    # Reverse keys and values\n    reversed_mapping = dict_reverse_keys_and_values(dictionary=columns_type_mapping)\n\n    # Validate\n    assert_columns_exists(dataframe, reversed_mapping.keys())\n\n    # Apply mapping to dataframe\n    try:\n        dataframe = dataframe.withColumns(\n            {\n                col: F.col(col).cast(_validate_pyspark_datatype(typ))\n                for col, typ in reversed_mapping.items()\n            }\n        )\n    except Exception as e:  # pragma: no cover\n        raise RuntimeError(f\"Raised {e.__class__.__name__}: {e}\") from e\n\n    # Return\n    return dataframe\n</code></pre>"},{"location":"usage/overview/","title":"Overview<code>toolbox-pyspark</code>","text":""},{"location":"usage/overview/#introduction","title":"Introduction","text":"<p>The purpose of this package is to provide some helper files/functions/classes for generic PySpark processes.</p>"},{"location":"usage/overview/#key-urls","title":"Key URLs","text":"<p>For reference, these URL's are used:</p> Type Source URL Git Repo GitHub https://github.com/data-science-extensions/toolbox-pyspark Python Package PyPI https://pypi.org/project/toolbox-pyspark Package Docs Pages https://data-science-extensions.com/toolbox-pyspark/"},{"location":"usage/overview/#installation","title":"Installation","text":"<p>You can install and use this package multiple ways by using <code>pip</code>, <code>pipenv</code>, or <code>poetry</code>.</p>"},{"location":"usage/overview/#using-pip","title":"Using <code>pip</code>:","text":"<ol> <li> <p>In your terminal, run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install --requirement=requirements.txt\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-pipenv","title":"Using <code>pipenv</code>:","text":"<ol> <li> <p>Install using environment variables:</p> <p>In your <code>Pipfile</code> file, add:</p> <pre><code>[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = false\nname = \"pypi\"\n\n[packages]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install pipenv\npython3 -m pipenv install --verbose --skip-lock --categories=root index=pypi toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m run pipenv install --verbose --skip-lock --requirements=requirements.txt\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>python3 -m pipenv install --verbose --skip-lock toolbox-pyspark\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-poetry","title":"Using <code>poetry</code>:","text":"<ol> <li> <p>In your <code>pyproject.toml</code> file, add:</p> <pre><code>[tool.poetry.dependencies]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>poetry add toolbox-pyspark\npoetry install\npoetry sync\n</code></pre> </li> </ol>"},{"location":"usage/overview/#contribution","title":"Contribution","text":"<p>Contribution is always welcome.</p> <ol> <li> <p>First, either fork or branch the main repo.</p> </li> <li> <p>Clone your forked/branched repo.</p> </li> <li> <p>Build your environment:</p> <ol> <li> <p>With <code>pipenv</code> on Windows:</p> <pre><code>if (-not (Test-Path .venv)) {mkdir .venv}\npython -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>pipenv</code> on Linux:</p> <pre><code>mkdir .venv\npython3 -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Windows:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install poetry\npython -m poetry init\npython -m poetry add $(cat requirements/root.txt)\npython -m poetry add --group=dev $(cat requirements/dev.txt)\npython -m poetry add --group=test $(cat requirements/test.txt)\npython -m poetry add --group=docs $(cat requirements/docs.txt)\npython -m poetry install\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Linux:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install poetry\npython3 -m poetry init\npython3 -m poetry add $(cat requirements/root.txt)\npython3 -m poetry add --group=dev $(cat requirements/dev.txt)\npython3 -m poetry add --group=test $(cat requirements/test.txt)\npython3 -m poetry add --group=docs $(cat requirements/docs.txt)\npython3 -m poetry install\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> </ol> </li> <li> <p>Start contributing.</p> </li> <li> <p>When you're happy with the changes, raise a Pull Request to merge with the main branch again.</p> </li> </ol>"},{"location":"usage/overview/#build-and-test","title":"Build and Test","text":"<p>To ensure that the package is working as expected, please ensure that:</p> <ol> <li>You write your code as per PEP8 requirements.</li> <li>You write a UnitTest for each function/feature you include.</li> <li>The CodeCoverage is 100%.</li> <li>All UnitTests are passing.</li> <li>MyPy is passing 100%.</li> </ol>"},{"location":"usage/overview/#testing","title":"Testing","text":"<ul> <li> <p>Run them all together</p> <pre><code>poetry run make check\n</code></pre> </li> <li> <p>Or run them individually:</p> <ul> <li> <p>Black <pre><code>poetry run make check-black\n</code></pre></p> </li> <li> <p>PyTests:     <pre><code>poetry run make ckeck-pytest\n</code></pre></p> </li> <li> <p>MyPy:     <pre><code>poetry run make check-mypy\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"usage/roadmap/","title":"Roadmap","text":"Module Completed Issue Milestone <code>io</code> \u2705 <code>checks</code> \u2705 <code>types</code> \u2705 <code>cleaning</code> \u2b1c <code>columns</code> \u2b1c <code>datetime</code> \u2b1c <code>delta</code> \u2b1c <code>dimensions</code> \u2b1c <code>duplication</code> \u2b1c <code>keys</code> \u2b1c <code>scale</code> \u2b1c <code>schema</code> \u2b1c"}]}