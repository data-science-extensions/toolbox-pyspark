{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"code/","title":"Modules","text":""},{"location":"code/#overview","title":"Overview","text":"<p>There are 12 modules used in this package, which covers 41 functions</p>"},{"location":"code/#module-descriptions","title":"Module Descriptions","text":"Module Description io The <code>io</code> module is used for reading and writing tables to/from directories."},{"location":"code/#functions-by-module","title":"Functions by Module","text":"Module Function io read_from_path() write_to_path() transfer_table()"},{"location":"code/#testing","title":"Testing","text":"<p>This package is fully tested against:</p> <ol> <li>Unit tests</li> <li>Lint tests</li> <li>MyPy tests</li> <li>Build tests</li> </ol>"},{"location":"code/#latest-code-coverage","title":"Latest Code Coverage","text":""},{"location":"code/io/","title":"IO","text":""},{"location":"code/io/#toolbox_pyspark.io","title":"toolbox_pyspark.io","text":"<p>Summary</p> <p>The <code>io</code> module is used for reading and writing tables to/from directories.</p>"},{"location":"code/io/#toolbox_pyspark.io.read_from_path","title":"read_from_path","text":"<pre><code>read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame\n</code></pre> <p>Summary</p> <p>Read an object from a given <code>path</code> in to memory as a <code>pyspark</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table to read in.</p> required <code>path</code> <code>str</code> <p>The path from which it will be read.</p> required <code>spark_session</code> <code>SparkSession</code> <p>The Spark session to use for the reading.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format of the object at location <code>path</code>. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>read_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. Like, for example:</p> <ul> <li>If the object is a CSV, you may want to define that it has a header row: <code>{\"header\": \"true\"}</code>.</li> <li>If the object is a Delta table, you may want to query a specific version: <code>{versionOf\": \"0\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The loaded dataframe.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'a': [1,2,3,4],\n...     'b': ['a','b','c','d'],\n...     'c': [1,1,1,1],\n...     'd': ['2','2','2','2'],\n... })\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> <pre><code>['table.csv', 'table.parquet']\n</code></pre> </p> <p>Read CSV<pre><code>&gt;&gt;&gt; df_csv = read_from_path(\n...     name=\"table.csv\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"csv\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt; df_csv.show()\n</code></pre> <pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Read Parquet<pre><code>&gt;&gt;&gt; df_parquet = read_from_path(\n...     name=\"table.csv\",\n...     path=\"./test\",\n...     spark_session=spark,\n...     data_format=\"parquet\",\n... )\n&gt;&gt;&gt; df_parquet.show()\n</code></pre> <pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef read_from_path(\n    name: str,\n    path: str,\n    spark_session: SparkSession,\n    data_format: Optional[str] = \"delta\",\n    read_options: Optional[str_dict] = None,\n) -&gt; psDataFrame:\n    \"\"\"\n    !!! note \"Summary\"\n        Read an object from a given `path` in to memory as a `pyspark` dataframe.\n\n    Params:\n        name (str):\n            The name of the table to read in.\n        path (str):\n            The path from which it will be read.\n        spark_session (SparkSession):\n            The Spark session to use for the reading.\n        data_format (Optional[str], optional):\n            The format of the object at location `path`.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        read_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            Like, for example:&lt;br&gt;\n\n            - If the object is a CSV, you may want to define that it has a header row: `#!py {\"header\": \"true\"}`.\n            - If the object is a Delta table, you may want to query a specific version: `#!py {versionOf\": \"0\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (psDataFrame):\n            The loaded dataframe.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import read_from_path\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'a': [1,2,3,4],\n        ...     'b': ['a','b','c','d'],\n        ...     'c': [1,1,1,1],\n        ...     'd': ['2','2','2','2'],\n        ... })\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        ['table.csv', 'table.parquet']\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Read CSV\"}\n        &gt;&gt;&gt; df_csv = read_from_path(\n        ...     name=\"table.csv\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"csv\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt; df_csv.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Read Parquet\"}\n        &gt;&gt;&gt; df_parquet = read_from_path(\n        ...     name=\"table.csv\",\n        ...     path=\"./test\",\n        ...     spark_session=spark,\n        ...     data_format=\"parquet\",\n        ... )\n        &gt;&gt;&gt; df_parquet.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n    \"\"\"\n    data_format: str = data_format or \"parquet\"\n    reader: DataFrameReader = spark_session.read.format(data_format)\n    if read_options:\n        reader.options(**read_options)\n    return reader.load(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.write_to_path","title":"write_to_path","text":"<pre><code>write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>For a given <code>table</code>, write it out to a specified <code>path</code> with name <code>name</code> and format <code>format</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame</code> <p>The table to be written. Must be a valid <code>pyspark</code> DataFrame (<code>pyspark.sql.DataFrame</code>).</p> required <code>name</code> <code>str</code> <p>The name of the table where it will be written.</p> required <code>path</code> <code>str</code> <p>The path location for where to save the table.</p> required <code>data_format</code> <code>Optional[str]</code> <p>The format that the <code>table</code> will be written to. Defaults to <code>\"delta\"</code>.</p> <code>'delta'</code> <code>mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>write_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. Like, for example:</p> <ul> <li>If you are writing to a Delta object, and wanted to overwrite the schema: <code>{\"overwriteSchema\": \"true\"}</code>.</li> <li>If you're writing to a CSV file, and wanted to specify the header row: <code>{\"header\": \"true\"}</code>.</li> </ul> <p>For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     pd.DataFrame({\n...         'a': [1,2,3,4],\n...         'b': ['a','b','c','d'],\n...         'c': [1,1,1,1],\n...         'd': ['2','2','2','2'],\n...     })\n... )\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; df.show()\n</code></pre> <pre><code>+---+---+---+---+\n| a | b | c | d |\n+---+---+---+---+\n| 1 | a | 1 | 2 |\n| 2 | b | 1 | 2 |\n| 3 | c | 1 | 2 |\n| 4 | d | 1 | 2 |\n+---+---+---+---+\n</code></pre> </p> <p>Write to CSV<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     mode=\"overwrite\",\n...     options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./test\",\n...     data_format=\"csv\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> <pre><code>True\n</code></pre> </p> <p>Write to Parquet<pre><code>&gt;&gt;&gt; write_to_path(\n...     table=df,\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     mode=\"overwrite\",\n... )\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./test\",\n...     data_format=\"parquet\",\n...     spark_session=df.sparkSession,\n... )\n</code></pre> <pre><code>True\n</code></pre> </p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef write_to_path(\n    table: psDataFrame,\n    name: str,\n    path: str,\n    data_format: Optional[str] = \"delta\",\n    mode: Optional[str] = None,\n    write_options: Optional[str_dict] = None,\n    partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        For a given `table`, write it out to a specified `path` with name `name` and format `format`.\n\n    Params:\n        table (psDataFrame):\n            The table to be written. Must be a valid `pyspark` DataFrame ([`pyspark.sql.DataFrame`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)).\n        name (str):\n            The name of the table where it will be written.\n        path (str):\n            The path location for where to save the table.\n        data_format (Optional[str], optional):\n            The format that the `table` will be written to.&lt;br&gt;\n            Defaults to `#!py \"delta\"`.\n        mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        write_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            Like, for example:\n\n            - If you are writing to a Delta object, and wanted to overwrite the schema: `#!py {\"overwriteSchema\": \"true\"}`.\n            - If you're writing to a CSV file, and wanted to specify the header row: `#!py {\"header\": \"true\"}`.\n\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#!py dict()`.\n        partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import write_to_path\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     pd.DataFrame({\n        ...         'a': [1,2,3,4],\n        ...         'b': ['a','b','c','d'],\n        ...         'c': [1,1,1,1],\n        ...         'd': ['2','2','2','2'],\n        ...     })\n        ... )\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; df.show()\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.txt .text}\n        +---+---+---+---+\n        | a | b | c | d |\n        +---+---+---+---+\n        | 1 | a | 1 | 2 |\n        | 2 | b | 1 | 2 |\n        | 3 | c | 1 | 2 |\n        | 4 | d | 1 | 2 |\n        +---+---+---+---+\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Write to CSV\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     mode=\"overwrite\",\n        ...     options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./test\",\n        ...     data_format=\"csv\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        True\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Write to Parquet\"}\n        &gt;&gt;&gt; write_to_path(\n        ...     table=df,\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     mode=\"overwrite\",\n        ... )\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./test\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=df.sparkSession,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        True\n        ```\n        &lt;/div&gt;\n    \"\"\"\n    write_options: str_dict = write_options or dict()\n    data_format: str = data_format or \"parquet\"\n    writer: DataFrameWriter = table.write.mode(mode).format(data_format)\n    if write_options:\n        writer.options(**write_options)\n    if partition_cols is not None:\n        partition_cols = (\n            [partition_cols] if is_type(partition_cols, str) else partition_cols\n        )\n        writer = writer.partitionBy(list(partition_cols))\n    writer.save(f\"{path}{'/' if not path.endswith('/') else ''}{name}\")\n</code></pre>"},{"location":"code/io/#toolbox_pyspark.io.transfer_table","title":"transfer_table","text":"<pre><code>transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[\n        str_collection\n    ] = None,\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Copy a table from one location to another.</p> Details <p>This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.</p> <p>Parameters:</p> Name Type Description Default <code>spark_session</code> <code>SparkSession</code> <p>The spark session to use for the transfer. Necessary in order to instantiate the reading process.</p> required <code>from_table_path</code> <code>str</code> <p>The path from which the table will be read.</p> required <code>from_table_name</code> <code>str</code> <p>The name of the table to be read.</p> required <code>from_table_format</code> <code>str</code> <p>The format of the data at the reading location.</p> required <code>to_table_path</code> <code>str</code> <p>The location where to save the table to.</p> required <code>to_table_name</code> <code>str</code> <p>The name of the table where it will be saved.</p> required <code>to_table_format</code> <code>str</code> <p>The format of the saved table.</p> required <code>from_table_options</code> <code>Dict[str, str]</code> <p>Any additional obtions to parse to the Spark reader. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameReader.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_mode</code> <code>Optional[str]</code> <p>The behaviour for when the data already exists. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.mode</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>to_table_options</code> <code>Dict[str, str]</code> <p>Any additional settings to parse to the writer class. For more info, check the <code>pyspark</code> docs: <code>pyspark.sql.DataFrameWriter.options</code>. Defaults to <code>dict()</code>.</p> <code>None</code> <code>to_table_partition_cols</code> <code>Optional[Union[str_collection, str]]</code> <p>The column(s) that the table should partition by. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs parsed to the parameters of this function are not the correct type. Uses the <code>@typeguard.typechecked</code> decorator.</p> <p>Returns:</p> Type Description <code>type(None)</code> <p>Nothing is returned.</p> Examples Set up<pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n&gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'a': [1,2,3,4],\n...     'b': ['a','b','c','d'],\n...     'c': [1,1,1,1],\n...     'd': ['2','2','2','2'],\n... })\n&gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n&gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n</code></pre> <p>Check<pre><code>&gt;&gt;&gt; import os\n&gt;&gt;&gt; print(os.listdir(\"./test\"))\n</code></pre> <pre><code>['table.csv', 'table.parquet']\n</code></pre> </p> <p>Move CSV<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.csv\",\n...     from_table_format=\"csv\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.csv\",\n...     to_table_format=\"csv\",\n...     from_table_options={\"header\": \"true\"},\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"header\": \"true\"},\n... )\n&gt;&gt;&gt; table_exists(\n...     name=\"df.csv\",\n...     path=\"./other\",\n...     data_format=\"csv\",\n...     spark_session=spark,\n... )\n</code></pre> <pre><code>True\n</code></pre> </p> <p>Move Parquet<pre><code>&gt;&gt;&gt; transfer_table(\n...     spark_session=spark,\n...     from_table_path=\"./test\",\n...     from_table_name=\"table.parquet\",\n...     from_table_format=\"parquet\",\n...     to_table_path=\"./other\",\n...     to_table_name=\"table.parquet\",\n...     to_table_format=\"parquet\",\n...     to_table_mode=\"overwrite\",\n...     to_table_options={\"overwriteSchema\": \"true\"},\n... )\n&gt;&gt;&gt; table_exists(\n...     name=\"df.parquet\",\n...     path=\"./other\",\n...     data_format=\"parquet\",\n...     spark_session=spark,\n... )\n</code></pre> <pre><code>True\n</code></pre> </p> Source code in <code>src/toolbox_pyspark/io.py</code> <pre><code>@typechecked\ndef transfer_table(\n    spark_session: SparkSession,\n    from_table_path: str,\n    from_table_name: str,\n    from_table_format: str,\n    to_table_path: str,\n    to_table_name: str,\n    to_table_format: str,\n    from_table_options: Optional[str_dict] = None,\n    to_table_mode: Optional[str] = None,\n    to_table_options: Optional[str_dict] = None,\n    to_table_partition_cols: Optional[str_collection] = None,\n) -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        Copy a table from one location to another.\n\n    ???+ abstract \"Details\"\n        This is a blind transfer. There is no validation, no alteration, no adjustments made at all. Simply read directly from one location and move immediately to another location straight away.\n\n    Params:\n        spark_session (SparkSession):\n            The spark session to use for the transfer. Necessary in order to instantiate the reading process.\n        from_table_path (str):\n            The path from which the table will be read.\n        from_table_name (str):\n            The name of the table to be read.\n        from_table_format (str):\n            The format of the data at the reading location.\n        to_table_path (str):\n            The location where to save the table to.\n        to_table_name (str):\n            The name of the table where it will be saved.\n        to_table_format (str):\n            The format of the saved table.\n        from_table_options (Dict[str, str], optional):\n            Any additional obtions to parse to the Spark reader.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameReader.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_mode (Optional[str], optional):\n            The behaviour for when the data already exists.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.mode`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html).&lt;br&gt;\n            Defaults to `#!py None`.\n        to_table_options (Dict[str, str], optional):\n            Any additional settings to parse to the writer class.&lt;br&gt;\n            For more info, check the `pyspark` docs: [`pyspark.sql.DataFrameWriter.options`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html).&lt;br&gt;\n            Defaults to `#! dict()`.\n        to_table_partition_cols (Optional[Union[str_collection, str]], optional):\n            The column(s) that the table should partition by.&lt;br&gt;\n            Defaults to `#!py None`.\n\n    Raises:\n        TypeError:\n            If any of the inputs parsed to the parameters of this function are not the correct type. Uses the [`@typeguard.typechecked`](https://typeguard.readthedocs.io/en/stable/api.html#typeguard.typechecked) decorator.\n\n    Returns:\n        (type(None)):\n            Nothing is returned.\n\n    ???+ example \"Examples\"\n\n        ```{.py .python linenums=\"1\" title=\"Set up\"}\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from toolbox_pyspark.io import transfer_table\n        &gt;&gt;&gt; from toolbox_pyspark.checks import table_exists\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'a': [1,2,3,4],\n        ...     'b': ['a','b','c','d'],\n        ...     'c': [1,1,1,1],\n        ...     'd': ['2','2','2','2'],\n        ... })\n        &gt;&gt;&gt; df.to_csv(\"./test/table.csv\")\n        &gt;&gt;&gt; df.to_parquet(\"./test/table.parquet\")\n        ```\n\n        ```{.py .python linenums=\"1\" title=\"Check\"}\n        &gt;&gt;&gt; import os\n        &gt;&gt;&gt; print(os.listdir(\"./test\"))\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        ['table.csv', 'table.parquet']\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Move CSV\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.csv\",\n        ...     from_table_format=\"csv\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.csv\",\n        ...     to_table_format=\"csv\",\n        ...     from_table_options={\"header\": \"true\"},\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"header\": \"true\"},\n        ... )\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.csv\",\n        ...     path=\"./other\",\n        ...     data_format=\"csv\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        True\n        ```\n        &lt;/div&gt;\n\n        ```{.py .python linenums=\"1\" title=\"Move Parquet\"}\n        &gt;&gt;&gt; transfer_table(\n        ...     spark_session=spark,\n        ...     from_table_path=\"./test\",\n        ...     from_table_name=\"table.parquet\",\n        ...     from_table_format=\"parquet\",\n        ...     to_table_path=\"./other\",\n        ...     to_table_name=\"table.parquet\",\n        ...     to_table_format=\"parquet\",\n        ...     to_table_mode=\"overwrite\",\n        ...     to_table_options={\"overwriteSchema\": \"true\"},\n        ... )\n        &gt;&gt;&gt; table_exists(\n        ...     name=\"df.parquet\",\n        ...     path=\"./other\",\n        ...     data_format=\"parquet\",\n        ...     spark_session=spark,\n        ... )\n        ```\n        &lt;div class=\"result\" markdown&gt;\n        ```{.py .python}\n        True\n        ```\n        &lt;/div&gt;\n    \"\"\"\n    from_table_options: str_dict = from_table_options or dict()\n    to_table_options: str_dict = to_table_options or dict()\n    from_table: psDataFrame = read_from_path(\n        name=from_table_name,\n        path=from_table_path,\n        spark_session=spark_session,\n        data_format=from_table_format,\n        read_options=from_table_options,\n    )\n    write_to_path(\n        table=from_table,\n        name=to_table_name,\n        path=to_table_path,\n        data_format=to_table_format,\n        mode=to_table_mode,\n        write_options=to_table_options,\n        partition_cols=to_table_partition_cols,\n    )\n</code></pre>"},{"location":"usage/overview/","title":"Overview<code>toolbox-pyspark</code>","text":""},{"location":"usage/overview/#introduction","title":"Introduction","text":"<p>The purpose of this package is to provide some helper files/functions/classes for generic PySpark processes.</p>"},{"location":"usage/overview/#key-urls","title":"Key URLs","text":"<p>For reference, these URL's are used:</p> Type Source URL Git Repo GitHub https://github.com/data-science-extensions/toolbox-pyspark Python Package PyPI https://pypi.org/project/toolbox-pyspark Package Docs Pages https://data-science-extensions.com/toolbox-pyspark/"},{"location":"usage/overview/#installation","title":"Installation","text":"<p>You can install and use this package multiple ways by using <code>pip</code>, <code>pipenv</code>, or <code>poetry</code>.</p>"},{"location":"usage/overview/#using-pip","title":"Using <code>pip</code>:","text":"<ol> <li> <p>In your terminal, run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install --requirement=requirements.txt\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-pipenv","title":"Using <code>pipenv</code>:","text":"<ol> <li> <p>Install using environment variables:</p> <p>In your <code>Pipfile</code> file, add:</p> <pre><code>[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = false\nname = \"pypi\"\n\n[packages]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>python3 -m pip install pipenv\npython3 -m pipenv install --verbose --skip-lock --categories=root index=pypi toolbox-pyspark\n</code></pre> </li> <li> <p>Or, in your <code>requirements.txt</code> file, add:</p> <pre><code>toolbox-pyspark\n</code></pre> <p>Then run:</p> <pre><code>python3 -m run pipenv install --verbose --skip-lock --requirements=requirements.txt\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>python3 -m pipenv install --verbose --skip-lock toolbox-pyspark\n</code></pre> </li> </ol>"},{"location":"usage/overview/#using-poetry","title":"Using <code>poetry</code>:","text":"<ol> <li> <p>In your <code>pyproject.toml</code> file, add:</p> <pre><code>[tool.poetry.dependencies]\ntoolbox-pyspark = \"*\"\n</code></pre> <p>Then run:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Or just run this:</p> <pre><code>poetry add toolbox-pyspark\npoetry install\npoetry sync\n</code></pre> </li> </ol>"},{"location":"usage/overview/#contribution","title":"Contribution","text":"<p>Contribution is always welcome.</p> <ol> <li> <p>First, either fork or branch the main repo.</p> </li> <li> <p>Clone your forked/branched repo.</p> </li> <li> <p>Build your environment:</p> <ol> <li> <p>With <code>pipenv</code> on Windows:</p> <pre><code>if (-not (Test-Path .venv)) {mkdir .venv}\npython -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>pipenv</code> on Linux:</p> <pre><code>mkdir .venv\npython3 -m pipenv install --requirements requirements.txt --requirements requirements-dev.txt --skip-lock\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Windows:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install poetry\npython -m poetry init\npython -m poetry add $(cat requirements/root.txt)\npython -m poetry add --group=dev $(cat requirements/dev.txt)\npython -m poetry add --group=test $(cat requirements/test.txt)\npython -m poetry add --group=docs $(cat requirements/docs.txt)\npython -m poetry install\npython -m poetry run pre-commit install\npython -m poetry shell\n</code></pre> </li> <li> <p>With <code>poetry</code> on Linux:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install poetry\npython3 -m poetry init\npython3 -m poetry add $(cat requirements/root.txt)\npython3 -m poetry add --group=dev $(cat requirements/dev.txt)\npython3 -m poetry add --group=test $(cat requirements/test.txt)\npython3 -m poetry add --group=docs $(cat requirements/docs.txt)\npython3 -m poetry install\npython3 -m poetry run pre-commit install\npython3 -m poetry shell\n</code></pre> </li> </ol> </li> <li> <p>Start contributing.</p> </li> <li> <p>When you're happy with the changes, raise a Pull Request to merge with the main branch again.</p> </li> </ol>"},{"location":"usage/overview/#build-and-test","title":"Build and Test","text":"<p>To ensure that the package is working as expected, please ensure that:</p> <ol> <li>You write your code as per PEP8 requirements.</li> <li>You write a UnitTest for each function/feature you include.</li> <li>The CodeCoverage is 100%.</li> <li>All UnitTests are passing.</li> <li>MyPy is passing 100%.</li> </ol>"},{"location":"usage/overview/#testing","title":"Testing","text":"<ul> <li> <p>Run them all together</p> <pre><code>poetry run make check\n</code></pre> </li> <li> <p>Or run them individually:</p> <ul> <li> <p>Black <pre><code>poetry run make check-black\n</code></pre></p> </li> <li> <p>PyTests:     <pre><code>poetry run make ckeck-pytest\n</code></pre></p> </li> <li> <p>MyPy:     <pre><code>poetry run make check-mypy\n</code></pre></p> </li> </ul> </li> </ul>"}]}